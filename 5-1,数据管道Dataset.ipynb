{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-1,数据管道Dataset\n",
    "\n",
    "如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。\n",
    "\n",
    "但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。\n",
    "\n",
    "使用 tf.data API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，构建数据管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。\n",
    "\n",
    "其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。\n",
    "\n",
    "通过tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecords文件，读取后再解析成tf.Example。\n",
    "\n",
    "但tfrecords文件的优点是压缩后文件较小，便于网络传播，加载速度较快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1,从Numpy array构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Numpy array构建数据管道\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "ds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"],iris[\"target\"]))\n",
    "for features,label in ds1.take(5):\n",
    "    print(features,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2,从 Pandas DataFrame构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 Pandas DataFrame构建数据管道\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets \n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "dfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\n",
    "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n",
    "\n",
    "for features,label in ds2.take(3):\n",
    "    print(features,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3,从Python generator构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Python generator构建数据管道\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 定义一个从文件中读取图片的generator\n",
    "image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
    "                    \"./data/cifar2/test/\",\n",
    "                    target_size=(32, 32),\n",
    "                    batch_size=20,\n",
    "                    class_mode='binary')\n",
    "\n",
    "classdict = image_generator.class_indices\n",
    "print(classdict)\n",
    "\n",
    "def generator():\n",
    "    for features,label in image_generator:\n",
    "        yield (features,label)\n",
    "\n",
    "ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds3.unbatch().take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-cifar2预览.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4,从csv文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv文件构建数据管道\n",
    "ds4 = tf.data.experimental.make_csv_dataset(\n",
    "      file_pattern = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"],\n",
    "      batch_size=3, \n",
    "      label_name=\"Survived\",\n",
    "      na_value=\"\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)\n",
    "\n",
    "for data,label in ds4.take(2):\n",
    "    print(data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([540,  58, 764], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 1], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Frolicher, Miss. Hedwig Margaritha', b'Novel, Mr. Mansouer',\n",
    "       b'Carter, Mrs. William Ernest (Lucile Polk)'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'female', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([22. , 28.5, 36. ], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 2], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'13568', b'2697', b'113760'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 49.5   ,   7.2292, 120.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'B39', b'', b'B96 B98'], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'C', b'S'], dtype=object)>)]) tf.Tensor([1 0 1], shape=(3,), dtype=int32)\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([845,  66, 390], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 2], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Culumovic, Mr. Jeso', b'Moubarek, Master. Gerios',\n",
    "       b'Lehmann, Miss. Bertha'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'male', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([17.,  0., 17.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'315090', b'2661', b'SC 1748'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 8.6625, 15.2458, 12.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'S', b'C', b'C'], dtype=object)>)]) tf.Tensor([0 1 1], shape=(3,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5,从文本文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件构建数据管道\n",
    "\n",
    "ds5 = tf.data.TextLineDataset(\n",
    "    filenames = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"]\n",
    "    ).skip(1) #略去第一行header\n",
    "\n",
    "for line in ds5.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'687,0,3,\"Panula, Mr. Jaako Arnold\",male,14.0,4,1,3101295,39.6875,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6,从文件路径构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6 = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "for file in ds6.take(5):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/1263.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/2837.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/4264.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/4241.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/192.jpg', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "for i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n",
    "    plt.figure(i)\n",
    "    plt.imshow((img/255.0).numpy())\n",
    "    plt.title(\"label = %d\"%label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7,从tfrecords文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# inpath：原始数据路径 outpath:TFRecord文件输出路径\n",
    "def create_tfrecords(inpath,outpath): \n",
    "    writer = tf.io.TFRecordWriter(outpath)\n",
    "    dirs = os.listdir(inpath)\n",
    "    for index, name in enumerate(dirs):\n",
    "        class_path = inpath +\"/\"+ name+\"/\"\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = class_path + img_name\n",
    "            img = tf.io.read_file(img_path)\n",
    "            #img = tf.image.decode_image(img)\n",
    "            #img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩\n",
    "            example = tf.train.Example(\n",
    "               features=tf.train.Features(feature={\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n",
    "               }))\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "create_tfrecords(\"./data/cifar2/test/\",\"./data/cifar2_test.tfrecords/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def parse_example(proto):\n",
    "    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n",
    "                   'label': tf.io.FixedLenFeature([], tf.int64)} \n",
    "    example = tf.io.parse_single_example(proto, description)\n",
    "    img = tf.image.decode_jpeg(example[\"img_raw\"])   #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img, (32,32))\n",
    "    label = example[\"label\"]\n",
    "    return(img,label)\n",
    "\n",
    "ds7 = tf.data.TFRecordDataset(\"./data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds7.take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow((img/255.0).numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，应用数据转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。\n",
    "\n",
    "Dataset包含了非常丰富的数据转换功能。\n",
    "\n",
    "* map: 将转换函数映射到数据集每一个元素。\n",
    "\n",
    "* flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n",
    "\n",
    "* interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n",
    "\n",
    "* filter: 过滤掉某些元素。\n",
    "\n",
    "* zip: 将两个长度相同的Dataset横向铰合。\n",
    "\n",
    "* concatenate: 将两个Dataset纵向连接。\n",
    "\n",
    "* reduce: 执行归并操作。\n",
    "\n",
    "* batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。\n",
    "\n",
    "* padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。\n",
    "\n",
    "* window :构建滑动窗口，返回Dataset of Dataset.\n",
    "\n",
    "* shuffle: 数据顺序洗牌。\n",
    "\n",
    "* repeat: 重复数据若干次，不带参数时，重复无数次。\n",
    "\n",
    "* shard: 采样，从某个位置开始隔固定距离采样一个元素。\n",
    "\n",
    "* take: 采样，从开始位置取前几个元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map:将转换函数映射到数据集每一个元素\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_map = ds.map(lambda x:tf.strings.split(x,\" \"))\n",
    "for x in ds_map:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_flatmap = ds.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_flatmap:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_interleave:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter:过滤掉某些元素。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "#找出含有字母a或B的元素\n",
    "ds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, \".*[a|B].*\"))\n",
    "for x in ds_filter:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip:将两个长度相同的Dataset横向铰合。\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds3 = tf.data.Dataset.range(6,9)\n",
    "ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))\n",
    "for x,y,z in ds_zip:\n",
    "    print(x.numpy(),y.numpy(),z.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 3 6\n",
    "1 4 7\n",
    "2 5 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condatenate:将两个Dataset纵向连接。\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds_concat = tf.data.Dataset.concatenate(ds1,ds2)\n",
    "for x in ds_concat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce:执行归并操作。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5.0])\n",
    "result = ds.reduce(0.0,lambda x,y:tf.add(x,y))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 \n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_batch = ds.batch(4)\n",
    "for x in ds_batch:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
    "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。\n",
    "\n",
    "elements = [[1, 2],[3, 4, 5],[6, 7],[8]]\n",
    "ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n",
    "\n",
    "ds_padded_batch = ds.padded_batch(2,padded_shapes = [4,])\n",
    "for x in ds_padded_batch:\n",
    "    print(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[1 2 0 0]\n",
    " [3 4 5 0]], shape=(2, 4), dtype=int32)\n",
    "tf.Tensor(\n",
    "[[6 7 0 0]\n",
    " [8 0 0 0]], shape=(2, 4), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window:构建滑动窗口，返回Dataset of Dataset.\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "#window返回的是Dataset of Dataset,可以用flat_map压平\n",
    "ds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3,drop_remainder=True)) \n",
    "for x in ds_window:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
    "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
    "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
    "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
    "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n",
    "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle:数据顺序洗牌。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shuffle = ds.shuffle(buffer_size = 5)\n",
    "for x in ds_shuffle:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(6, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(11, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(9, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "tf.Tensor(8, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat:重复数据若干次，不带参数时，重复无数次。\n",
    "\n",
    "ds = tf.data.Dataset.range(3)\n",
    "ds_repeat = ds.repeat(3)\n",
    "for x in ds_repeat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shard:采样，从某个位置开始隔固定距离采样一个元素。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shard = ds.shard(3,index = 1)\n",
    "\n",
    "for x in ds_shard:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take:采样，从开始位置取前几个元素。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_take = ds.take(3)\n",
    "\n",
    "list(ds_take.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 1, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，提升管道性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练深度学习模型常常会非常耗时。\n",
    "\n",
    "模型训练的耗时主要来自于两个部分，一部分来自**数据准备**，另一部分来自**参数迭代**。\n",
    "\n",
    "参数迭代过程的耗时通常依赖于GPU来提升。\n",
    "\n",
    "而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。\n",
    "\n",
    "以下是一些构建高效数据管道的建议。\n",
    "\n",
    "* 1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n",
    "\n",
    "* 2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。\n",
    "\n",
    "* 3，使用 map 时设置num_parallel_calls 让数据转换过程多进程执行。\n",
    "\n",
    "* 4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\n",
    "\n",
    "* 5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 数据准备和参数迭代两个过程默认情况下是串行的。\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(10):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要1s\n",
    "    time.sleep(1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程预计耗时 10*2+10*1 = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for x in ds:\n",
    "    train_step()  \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n",
    "\n",
    "# 训练过程预计耗时 max(10*2,10*1) = 20s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training with prefetch...\"))\n",
    "\n",
    "# tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数\n",
    "for x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):\n",
    "    train_step()  \n",
    "    \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(4):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(8):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'181,0,3,\"Sage, Miss. Constance Gladys\",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'405,0,3,\"Oreskovic, Miss. Marija\",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'635,0,3,\"Skoog, Miss. Mabel\",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'701,1,1,\"Astor, Mrs. John Jacob (Madeleine Talmadge Force)\",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单进程转换\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start transformation...\"))\n",
    "\n",
    "ds_map = ds.map(load_image)\n",
    "for _ in ds_map:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多进程转换\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start parallel transformation...\"))\n",
    "\n",
    "ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "for _ in ds_map_parallel:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end parallel transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要0s\n",
    "    pass\n",
    "\n",
    "# 训练过程预计耗时 (5*2+5*0)*3 = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "\n",
    "# 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要0s\n",
    "    time.sleep(0) \n",
    "\n",
    "# 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先map后batch\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_map_batch = ds.map(lambda x:x**2).batch(20)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start scalar transformation...\"))\n",
    "for x in ds_map_batch:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end scalar transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先batch后map\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_batch_map = ds.batch(20).map(lambda x:x**2)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start vector transformation...\"))\n",
    "for x in ds_batch_map:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end vector transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
