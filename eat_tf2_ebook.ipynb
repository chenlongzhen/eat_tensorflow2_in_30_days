{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã€Š30å¤©åƒæ‰é‚£åª TensorFlow2.0 ã€‹å¼€ç¯‡è¾ ğŸ”¥ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š gitbookç”µå­ä¹¦åœ°å€ï¼š https://lyhue1991.github.io/eat_tensorflow2_in_30_days\n",
    "\n",
    "ğŸš€ githubé¡¹ç›®åœ°å€ï¼šhttps://github.com/lyhue1991/eat_tensorflow2_in_30_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒTensorFlow2 ğŸ è¿˜æ˜¯ PytorchğŸ”¥\n",
    "\n",
    "å…ˆè¯´ç»“è®º:\n",
    "\n",
    "**å¦‚æœæ˜¯å·¥ç¨‹å¸ˆï¼Œåº”è¯¥ä¼˜å…ˆé€‰TensorFlow2.**\n",
    "\n",
    "**å¦‚æœæ˜¯å­¦ç”Ÿæˆ–è€…ç ”ç©¶äººå‘˜ï¼Œåº”è¯¥ä¼˜å…ˆé€‰æ‹©Pytorch.**\n",
    "\n",
    "**å¦‚æœæ—¶é—´è¶³å¤Ÿï¼Œæœ€å¥½TensorFlow2å’ŒPytorchéƒ½è¦å­¦ä¹ æŒæ¡ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç†ç”±å¦‚ä¸‹ï¼š\n",
    "\n",
    "* 1ï¼Œ**åœ¨å·¥ä¸šç•Œæœ€é‡è¦çš„æ˜¯æ¨¡å‹è½åœ°ï¼Œç›®å‰å›½å†…çš„å¤§éƒ¨åˆ†äº’è”ç½‘ä¼ä¸šåªæ”¯æŒTensorFlowæ¨¡å‹çš„åœ¨çº¿éƒ¨ç½²ï¼Œä¸æ”¯æŒPytorchã€‚** å¹¶ä¸”å·¥ä¸šç•Œæ›´åŠ æ³¨é‡çš„æ˜¯æ¨¡å‹çš„é«˜å¯ç”¨æ€§ï¼Œè®¸å¤šæ—¶å€™ä½¿ç”¨çš„éƒ½æ˜¯æˆç†Ÿçš„æ¨¡å‹æ¶æ„ï¼Œè°ƒè¯•éœ€æ±‚å¹¶ä¸å¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2ï¼Œ**ç ”ç©¶äººå‘˜æœ€é‡è¦çš„æ˜¯å¿«é€Ÿè¿­ä»£å‘è¡¨æ–‡ç« ï¼Œéœ€è¦å°è¯•ä¸€äº›è¾ƒæ–°çš„æ¨¡å‹æ¶æ„ã€‚è€ŒPytorchåœ¨æ˜“ç”¨æ€§ä¸Šç›¸æ¯”TensorFlow2æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œæ›´åŠ æ–¹ä¾¿è°ƒè¯•ã€‚** å¹¶ä¸”åœ¨2019å¹´ä»¥æ¥åœ¨å­¦æœ¯ç•Œå é¢†äº†å¤§åŠå£æ±Ÿå±±ï¼Œèƒ½å¤Ÿæ‰¾åˆ°çš„ç›¸åº”æœ€æ–°ç ”ç©¶æˆæœæ›´å¤šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3ï¼ŒTensorFlow2å’ŒPytorchå®é™…ä¸Šæ•´ä½“é£æ ¼å·²ç»éå¸¸ç›¸ä¼¼äº†ï¼Œå­¦ä¼šäº†å…¶ä¸­ä¸€ä¸ªï¼Œå­¦ä¹ å¦å¤–ä¸€ä¸ªå°†æ¯”è¾ƒå®¹æ˜“ã€‚ä¸¤ç§æ¡†æ¶éƒ½æŒæ¡çš„è¯ï¼Œèƒ½å¤Ÿå‚è€ƒçš„å¼€æºæ¨¡å‹æ¡ˆä¾‹æ›´å¤šï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼ŒKerasğŸ å’Œ tf.keras ğŸ\n",
    "\n",
    "å…ˆè¯´ç»“è®ºï¼š\n",
    "\n",
    "**Kerasåº“åœ¨2.3.0ç‰ˆæœ¬åå°†ä¸å†æ›´æ–°ï¼Œç”¨æˆ·åº”è¯¥ä½¿ç”¨tf.kerasã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keraså¯ä»¥çœ‹æˆæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜é˜¶æ¥å£è§„èŒƒï¼Œå®ƒå¸®åŠ©ç”¨æˆ·ä»¥æ›´ç®€æ´çš„å½¢å¼å®šä¹‰å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ ç½‘ç»œã€‚\n",
    "\n",
    "ä½¿ç”¨pipå®‰è£…çš„Kerasåº“åŒæ—¶åœ¨tensorflow,theano,CNTKç­‰åç«¯åŸºç¡€ä¸Šè¿›è¡Œäº†è¿™ç§é«˜é˜¶æ¥å£è§„èŒƒçš„å®ç°ã€‚\n",
    "\n",
    "è€Œtf.kerasæ˜¯åœ¨TensorFlowä¸­ä»¥TensorFlowä½é˜¶APIä¸ºåŸºç¡€å®ç°çš„è¿™ç§é«˜é˜¶æ¥å£ï¼Œå®ƒæ˜¯Tensorflowçš„ä¸€ä¸ªå­æ¨¡å—ã€‚\n",
    "\n",
    "tf.kerasç»å¤§éƒ¨åˆ†åŠŸèƒ½å’Œå…¼å®¹å¤šç§åç«¯çš„Kerasåº“ç”¨æ³•å®Œå…¨ä¸€æ ·ï¼Œä½†å¹¶éå…¨éƒ¨ï¼Œå®ƒå’ŒTensorFlowä¹‹é—´çš„ç»“åˆæ›´ä¸ºç´§å¯†ã€‚\n",
    "\n",
    "éšç€è°·æ­Œå¯¹Kerasçš„æ”¶è´­ï¼ŒKerasåº“2.3.0ç‰ˆæœ¬åä¹Ÿå°†ä¸å†è¿›è¡Œæ›´æ–°ï¼Œç”¨æˆ·åº”å½“ä½¿ç”¨tf.kerasè€Œä¸æ˜¯ä½¿ç”¨pipå®‰è£…çš„Keras.\n",
    "\n",
    "### ä¸‰ï¼Œæœ¬ä¹¦ğŸ“–é¢å‘è¯»è€… ğŸ‘¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æœ¬ä¹¦å‡å®šè¯»è€…æœ‰ä¸€å®šçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œä½¿ç”¨è¿‡Kerasæˆ–è€…Tensorflow1.0æˆ–è€…Pytorchæ­å»ºè®­ç»ƒè¿‡æ¨¡å‹ã€‚**\n",
    "\n",
    "**å¯¹äºæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦ï¼Œå»ºè®®åœ¨å­¦ä¹ æœ¬ä¹¦æ—¶åŒæ­¥å‚è€ƒå­¦ä¹ ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦ã€‚**\n",
    "\n",
    "ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹è¿™æœ¬ä¹¦æ˜¯Kerasä¹‹çˆ¶Francois Cholletæ‰€è‘—ï¼Œè¯¥ä¹¦å‡å®šè¯»è€…æ— ä»»ä½•æœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œä»¥Kerasä¸ºå·¥å…·ï¼Œ\n",
    "\n",
    "ä½¿ç”¨ä¸°å¯Œçš„èŒƒä¾‹ç¤ºèŒƒæ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œè¯¥ä¹¦é€šä¿—æ˜“æ‡‚ï¼Œ**å…¨ä¹¦æ²¡æœ‰ä¸€ä¸ªæ•°å­¦å…¬å¼ï¼Œæ³¨é‡åŸ¹å…»è¯»è€…çš„æ·±åº¦å­¦ä¹ ç›´è§‰ã€‚**ã€‚\n",
    "\n",
    "è¯¥ä¹¦ç”µå­ç‰ˆä¸‹è½½é“¾æ¥ï¼šhttps://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA æå–ç ï¼šwtzo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œæœ¬ä¹¦å†™ä½œé£æ ¼ ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æœ¬ä¹¦æ˜¯ä¸€æœ¬å¯¹äººç±»ç”¨æˆ·æå…¶å‹å–„çš„TensorFlow2.0å…¥é—¨å·¥å…·ä¹¦ï¼Œä¸åˆ»æ„æ¶å¿ƒè¯»è€…æ˜¯æœ¬ä¹¦çš„åº•é™è¦æ±‚ï¼ŒDon't let me thinkæ˜¯æœ¬ä¹¦çš„æœ€é«˜è¿½æ±‚ã€‚**\n",
    "\n",
    "æœ¬ä¹¦ä¸»è¦æ˜¯åœ¨å‚è€ƒTensorFlowå®˜æ–¹æ–‡æ¡£å’Œå‡½æ•°docæ–‡æ¡£åŸºç¡€ä¸Šæ•´ç†å†™æˆçš„ã€‚\n",
    "\n",
    "ä½†æœ¬ä¹¦åœ¨ç¯‡ç« ç»“æ„å’ŒèŒƒä¾‹é€‰å–ä¸Šåšäº†å¤§é‡çš„ä¼˜åŒ–ã€‚\n",
    "\n",
    "ä¸åŒäºå®˜æ–¹æ–‡æ¡£æ··ä¹±çš„ç¯‡ç« ç»“æ„ï¼Œæ—¢æœ‰æ•™ç¨‹åˆæœ‰æŒ‡å—ï¼Œç¼ºå°‘æ•´ä½“çš„ç¼–æ’é€»è¾‘ã€‚\n",
    "\n",
    "æœ¬ä¹¦æŒ‰ç…§å†…å®¹éš¾æ˜“ç¨‹åº¦ã€è¯»è€…æ£€ç´¢ä¹ æƒ¯å’ŒTensorFlowè‡ªèº«çš„å±‚æ¬¡ç»“æ„è®¾è®¡å†…å®¹ï¼Œå¾ªåºæ¸è¿›ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œæ–¹ä¾¿æŒ‰ç…§åŠŸèƒ½æŸ¥æ‰¾ç›¸åº”èŒƒä¾‹ã€‚\n",
    "\n",
    "ä¸åŒäºå®˜æ–¹æ–‡æ¡£å†—é•¿çš„èŒƒä¾‹ä»£ç ï¼Œæœ¬ä¹¦åœ¨èŒƒä¾‹è®¾è®¡ä¸Šå°½å¯èƒ½ç®€çº¦åŒ–å’Œç»“æ„åŒ–ï¼Œå¢å¼ºèŒƒä¾‹æ˜“è¯»æ€§å’Œé€šç”¨æ€§ï¼Œå¤§éƒ¨åˆ†ä»£ç ç‰‡æ®µåœ¨å®è·µä¸­å¯å³å–å³ç”¨ã€‚\n",
    "\n",
    "**å¦‚æœè¯´é€šè¿‡å­¦ä¹ TensorFlowå®˜æ–¹æ–‡æ¡£æŒæ¡TensorFlow2.0çš„éš¾åº¦å¤§æ¦‚æ˜¯9çš„è¯ï¼Œé‚£ä¹ˆé€šè¿‡å­¦ä¹ æœ¬ä¹¦æŒæ¡TensorFlow2.0çš„éš¾åº¦åº”è¯¥å¤§æ¦‚æ˜¯3.**\n",
    "\n",
    "è°¨ä»¥ä¸‹å›¾å¯¹æ¯”ä¸€ä¸‹TensorFlowå®˜æ–¹æ•™ç¨‹ä¸æœ¬æ•™ç¨‹çš„å·®å¼‚ã€‚\n",
    "\n",
    "![](./data/30å¤©åƒæ‰é‚£ä¸ªTF2.0.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œæœ¬ä¹¦å­¦ä¹ æ–¹æ¡ˆ â°\n",
    "\n",
    "**1ï¼Œå­¦ä¹ è®¡åˆ’**\n",
    "\n",
    "æœ¬ä¹¦æ˜¯ä½œè€…åˆ©ç”¨å·¥ä½œä¹‹ä½™å’Œç–«æƒ…æ”¾å‡æœŸé—´å¤§æ¦‚2ä¸ªæœˆå†™æˆçš„ï¼Œå¤§éƒ¨åˆ†è¯»è€…åº”è¯¥åœ¨30å¤©å¯ä»¥å®Œå…¨å­¦ä¼šã€‚\n",
    "\n",
    "é¢„è®¡æ¯å¤©èŠ±è´¹çš„å­¦ä¹ æ—¶é—´åœ¨30åˆ†é’Ÿåˆ°2ä¸ªå°æ—¶ä¹‹é—´ã€‚\n",
    "\n",
    "å½“ç„¶ï¼Œæœ¬ä¹¦ä¹Ÿéå¸¸é€‚åˆä½œä¸ºTensorFlowçš„å·¥å…·æ‰‹å†Œåœ¨å·¥ç¨‹è½åœ°æ—¶ä½œä¸ºèŒƒä¾‹åº“å‚è€ƒã€‚\n",
    "\n",
    "**ç‚¹å‡»å­¦ä¹ å†…å®¹è“è‰²æ ‡é¢˜å³å¯è¿›å…¥è¯¥ç« èŠ‚ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|æ—¥æœŸ | å­¦ä¹ å†…å®¹                                                       | å†…å®¹éš¾åº¦   | é¢„è®¡å­¦ä¹ æ—¶é—´ | æ›´æ–°çŠ¶æ€|\n",
    "|----:|:--------------------------------------------------------------|-----------:|----------:|-----:|\n",
    "|&nbsp;|[**ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹**](./ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹.md)    |â­ï¸   |   0hour   |âœ…    |\n",
    "|day1 |  [1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸ |   1hour    |âœ…    |\n",
    "|day2 |[1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|day3 |  [1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|day4 |  [1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|&nbsp;    |[**äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ**](./äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ.md)  | â­ï¸  |  0hour |âœ…  |\n",
    "|day5 |  [2-1,å¼ é‡æ•°æ®ç»“æ„](./2-1,å¼ é‡æ•°æ®ç»“æ„.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|day6 |  [2-2,ä¸‰ç§è®¡ç®—å›¾](./2-2,ä¸‰ç§è®¡ç®—å›¾.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…    |\n",
    "|day7 |  [2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶](./2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|&nbsp; |[**ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„**](./ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„.md) |   â­ï¸  |  0hour   |âœ…  |\n",
    "|day8 |  [3-1,ä½é˜¶APIç¤ºèŒƒ](./3-1,ä½é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day9 |  [3-2,ä¸­é˜¶APIç¤ºèŒƒ](./3-2,ä¸­é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day10 |  [3-3,é«˜é˜¶APIç¤ºèŒƒ](./3-3,é«˜é˜¶APIç¤ºèŒƒ.md)  | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|&nbsp; |[**å››ã€TensorFlowçš„ä½é˜¶API**](./å››ã€TensorFlowçš„ä½é˜¶API.md) |â­ï¸    | 0hour|âœ…  |\n",
    "|day11|  [4-1,å¼ é‡çš„ç»“æ„æ“ä½œ](./4-1,å¼ é‡çš„ç»“æ„æ“ä½œ.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…   |\n",
    "|day12|  [4-2,å¼ é‡çš„æ•°å­¦è¿ç®—](./4-2,å¼ é‡çš„æ•°å­¦è¿ç®—.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day13|  [4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ](./4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ.md)| â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day14|  [4-4,AutoGraphçš„æœºåˆ¶åŸç†](./4-4,AutoGraphçš„æœºåˆ¶åŸç†.md)    | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |\n",
    "|day15|  [4-5,AutoGraphå’Œtf.Module](./4-5,AutoGraphå’Œtf.Module.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|&nbsp; |[**äº”ã€TensorFlowçš„ä¸­é˜¶API**](./äº”ã€TensorFlowçš„ä¸­é˜¶API.md) |  â­ï¸  | 0hour|âœ… |\n",
    "|day16|  [5-1,æ•°æ®ç®¡é“Dataset](./5-1,æ•°æ®ç®¡é“Dataset.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |\n",
    "|day17|  [5-2,ç‰¹å¾åˆ—feature_column](./5-2,ç‰¹å¾åˆ—feature_column.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day18|  [5-3,æ¿€æ´»å‡½æ•°activation](./5-3,æ¿€æ´»å‡½æ•°activation.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day19|  [5-4,æ¨¡å‹å±‚layers](./5-4,æ¨¡å‹å±‚layers.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day20|  [5-5,æŸå¤±å‡½æ•°losses](./5-5,æŸå¤±å‡½æ•°losses.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day21|  [5-6,è¯„ä¼°æŒ‡æ ‡metrics](./5-6,è¯„ä¼°æŒ‡æ ‡metrics.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|day22|  [5-7,ä¼˜åŒ–å™¨optimizers](./5-7,ä¼˜åŒ–å™¨optimizers.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day23|  [5-8,å›è°ƒå‡½æ•°callbacks](./5-8,å›è°ƒå‡½æ•°callbacks.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|&nbsp; |[**å…­ã€TensorFlowçš„é«˜é˜¶API**](./å…­ã€TensorFlowçš„é«˜é˜¶API.md)|    â­ï¸ | 0hour|âœ…  |\n",
    "|day24|  [6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•](./6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•.md)   | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ… |\n",
    "|day25|  [6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•](./6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|day26|  [6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹](./6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day27|  [6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹](./6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day28|  [6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹](./6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day29| [6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹](./6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸| 1hour |âœ…   |\n",
    "|day30| [6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹](./6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸|2hour|âœ…  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ï¼Œå­¦ä¹ ç¯å¢ƒ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ä¹¦å…¨éƒ¨æºç åœ¨jupyterä¸­ç¼–å†™æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®é€šè¿‡gitå…‹éš†åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨jupyterä¸­äº¤äº’å¼è¿è¡Œå­¦ä¹ ã€‚\n",
    "\n",
    "ä¸ºäº†ç›´æ¥èƒ½å¤Ÿåœ¨jupyterä¸­æ‰“å¼€markdownæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…jupytextï¼Œå°†markdownè½¬æ¢æˆipnbã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥å…³æ³¨å…¬ä¼—å·â€**Pythonä¸ç®—æ³•ä¹‹ç¾**â€œ ï¼Œåå°å›å¤å…³é”®å­—ï¼š**tf**ï¼Œè·å–æœ¬ä¹¦ipynbæºç çš„ä¸‹è½½é“¾æ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…‹éš†æœ¬ä¹¦æºç åˆ°æœ¬åœ°\n",
    "#!git clone https://github.com/lyhue1991/eat_tensorflow2_in_30_days\n",
    "\n",
    "#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…jupytextï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†æœ¬ä¹¦å„ç« èŠ‚markdownæ–‡ä»¶è§†ä½œipynbæ–‡ä»¶è¿è¡Œ\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext\n",
    "    \n",
    "#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…æœ€æ–°ç‰ˆæœ¬tensorflow æµ‹è¯•æœ¬ä¹¦ä¸­çš„ä»£ç \n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#æ³¨ï¼šæœ¬ä¹¦å…¨éƒ¨ä»£ç åœ¨tensorflow 2.1ç‰ˆæœ¬æµ‹è¯•é€šè¿‡\n",
    "tf.print(\"tensorflow version:\",tf.__version__)\n",
    "\n",
    "a = tf.constant(\"hello\")\n",
    "b = tf.constant(\"tensorflow2\")\n",
    "c = tf.strings.join([a,b],\" \")\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow version: 2.1.0\n",
    "hello tensorflow2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œé¼“åŠ±å’Œè”ç³»ä½œè€… ğŸˆğŸˆ\n",
    "\n",
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"**Pythonä¸ç®—æ³•ä¹‹ç¾**\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "å¦‚æœæœ‰æƒ³è¦è·å–æœ¬ä¹¦çš„jupyter notebookæºä»£ç çš„å°ä¼™ä¼´ï¼Œä¹Ÿå¯ä»¥å…³æ³¨å…¬ä¼—å·ï¼Œåœ¨åå°å›å¤å…³é”®å­—ï¼š**tf**ï¼Œè·å–æœ¬ä¹¦å…¨éƒ¨ä»£ç å’Œæ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°½ç®¡TensorFlowè®¾è®¡ä¸Šè¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥ç”¨äºè¿›è¡Œå„ç§å¤æ‚çš„æ•°å€¼è®¡ç®—ã€‚\n",
    "\n",
    "ä½†é€šå¸¸äººä»¬ä½¿ç”¨TensorFlowæ¥å®ç°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶å¸¸ç”¨äºå®ç°ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚\n",
    "\n",
    "ä»åŸç†ä¸Šè¯´å¯ä»¥ä½¿ç”¨å¼ é‡æ„å»ºè®¡ç®—å›¾æ¥å®šä¹‰ç¥ç»ç½‘ç»œï¼Œå¹¶é€šè¿‡è‡ªåŠ¨å¾®åˆ†æœºåˆ¶è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "ä½†ä¸ºç®€æ´èµ·è§ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨TensorFlowçš„é«˜å±‚æ¬¡kerasæ¥å£æ¥å®ç°ç¥ç»ç½‘ç»œç½‘æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨TensorFlowå®ç°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€èˆ¬æµç¨‹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1ï¼Œå‡†å¤‡æ•°æ®\n",
    "\n",
    "2ï¼Œå®šä¹‰æ¨¡å‹\n",
    "\n",
    "3ï¼Œè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "4ï¼Œè¯„ä¼°æ¨¡å‹\n",
    "\n",
    "5ï¼Œä½¿ç”¨æ¨¡å‹\n",
    "\n",
    "6ï¼Œä¿å­˜æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¯¹æ–°æ‰‹æ¥è¯´ï¼Œå…¶ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†å®é™…ä¸Šæ˜¯å‡†å¤‡æ•°æ®è¿‡ç¨‹ã€‚** \n",
    "\n",
    "æˆ‘ä»¬åœ¨å®è·µä¸­é€šå¸¸ä¼šé‡åˆ°çš„æ•°æ®ç±»å‹åŒ…æ‹¬ç»“æ„åŒ–æ•°æ®ï¼Œå›¾ç‰‡æ•°æ®ï¼Œæ–‡æœ¬æ•°æ®ï¼Œæ—¶é—´åºåˆ—æ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†åˆ†åˆ«ä»¥titanicç”Ÿå­˜é¢„æµ‹é—®é¢˜ï¼Œcifar2å›¾ç‰‡åˆ†ç±»é—®é¢˜ï¼Œimdbç”µå½±è¯„è®ºåˆ†ç±»é—®é¢˜ï¼Œå›½å†…æ–°å† ç–«æƒ…ç»“æŸæ—¶é—´é¢„æµ‹é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºåº”ç”¨tensorflowå¯¹è¿™å››ç±»æ•°æ®çš„å»ºæ¨¡æ–¹æ³•ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "# 1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanicæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ä¹˜å®¢ä¿¡æ¯é¢„æµ‹ä»–ä»¬åœ¨Titanicå·æ’å‡»å†°å±±æ²‰æ²¡åèƒ½å¦ç”Ÿå­˜ã€‚\n",
    "\n",
    "ç»“æ„åŒ–æ•°æ®ä¸€èˆ¬ä¼šä½¿ç”¨Pandasä¸­çš„DataFrameè¿›è¡Œé¢„å¤„ç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers\n",
    "\n",
    "dftrain_raw = pd.read_csv('./data/titanic/train.csv')\n",
    "dftest_raw = pd.read_csv('./data/titanic/test.csv')\n",
    "dftrain_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-æ•°æ®é›†å±•ç¤º.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å­—æ®µè¯´æ˜ï¼š\n",
    "\n",
    "* Survived:0ä»£è¡¨æ­»äº¡ï¼Œ1ä»£è¡¨å­˜æ´»ã€yæ ‡ç­¾ã€‘\n",
    "* Pclass:ä¹˜å®¢æ‰€æŒç¥¨ç±»ï¼Œæœ‰ä¸‰ç§å€¼(1,2,3) ã€è½¬æ¢æˆonehotç¼–ç ã€‘\n",
    "* Name:ä¹˜å®¢å§“å ã€èˆå»ã€‘\n",
    "* Sex:ä¹˜å®¢æ€§åˆ« ã€è½¬æ¢æˆboolç‰¹å¾ã€‘\n",
    "* Age:ä¹˜å®¢å¹´é¾„(æœ‰ç¼ºå¤±) ã€æ•°å€¼ç‰¹å¾ï¼Œæ·»åŠ â€œå¹´é¾„æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘\n",
    "* SibSp:ä¹˜å®¢å…„å¼Ÿå§å¦¹/é…å¶çš„ä¸ªæ•°(æ•´æ•°å€¼) ã€æ•°å€¼ç‰¹å¾ã€‘\n",
    "* Parch:ä¹˜å®¢çˆ¶æ¯/å­©å­çš„ä¸ªæ•°(æ•´æ•°å€¼)ã€æ•°å€¼ç‰¹å¾ã€‘\n",
    "* Ticket:ç¥¨å·(å­—ç¬¦ä¸²)ã€èˆå»ã€‘\n",
    "* Fare:ä¹˜å®¢æ‰€æŒç¥¨çš„ä»·æ ¼(æµ®ç‚¹æ•°ï¼Œ0-500ä¸ç­‰) ã€æ•°å€¼ç‰¹å¾ã€‘\n",
    "* Cabin:ä¹˜å®¢æ‰€åœ¨èˆ¹èˆ±(æœ‰ç¼ºå¤±) ã€æ·»åŠ â€œæ‰€åœ¨èˆ¹èˆ±æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘\n",
    "* Embarked:ä¹˜å®¢ç™»èˆ¹æ¸¯å£:Sã€Cã€Q(æœ‰ç¼ºå¤±)ã€è½¬æ¢æˆonehotç¼–ç ï¼Œå››ç»´åº¦ S,C,Q,nanã€‘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ©ç”¨Pandasçš„æ•°æ®å¯è§†åŒ–åŠŸèƒ½æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æEDAï¼ˆExploratory Data Analysisï¼‰ã€‚\n",
    "\n",
    "labelåˆ†å¸ƒæƒ…å†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw['Survived'].value_counts().plot(kind = 'bar',\n",
    "     figsize = (12,8),fontsize=15,rot = 0)\n",
    "ax.set_ylabel('Counts',fontsize = 15)\n",
    "ax.set_xlabel('Survived',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-Labelåˆ†å¸ƒ.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹´é¾„åˆ†å¸ƒæƒ…å†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw['Age'].plot(kind = 'hist',bins = 20,color= 'purple',\n",
    "                    figsize = (12,8),fontsize=15)\n",
    "\n",
    "ax.set_ylabel('Frequency',fontsize = 15)\n",
    "ax.set_xlabel('Age',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-å¹´é¾„åˆ†å¸ƒ.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹´é¾„å’Œlabelçš„ç›¸å…³æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw.query('Survived == 0')['Age'].plot(kind = 'density',\n",
    "                      figsize = (12,8),fontsize=15)\n",
    "dftrain_raw.query('Survived == 1')['Age'].plot(kind = 'density',\n",
    "                      figsize = (12,8),fontsize=15)\n",
    "ax.legend(['Survived==0','Survived==1'],fontsize = 12)\n",
    "ax.set_ylabel('Density',fontsize = 15)\n",
    "ax.set_xlabel('Age',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-å¹´é¾„ç›¸å…³æ€§.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä¸ºæ­£å¼çš„æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dfdata):\n",
    "\n",
    "    dfresult= pd.DataFrame()\n",
    "\n",
    "    #Pclass\n",
    "    dfPclass = pd.get_dummies(dfdata['Pclass'])\n",
    "    dfPclass.columns = ['Pclass_' +str(x) for x in dfPclass.columns ]\n",
    "    dfresult = pd.concat([dfresult,dfPclass],axis = 1)\n",
    "\n",
    "    #Sex\n",
    "    dfSex = pd.get_dummies(dfdata['Sex'])\n",
    "    dfresult = pd.concat([dfresult,dfSex],axis = 1)\n",
    "\n",
    "    #Age\n",
    "    dfresult['Age'] = dfdata['Age'].fillna(0)\n",
    "    dfresult['Age_null'] = pd.isna(dfdata['Age']).astype('int32')\n",
    "\n",
    "    #SibSp,Parch,Fare\n",
    "    dfresult['SibSp'] = dfdata['SibSp']\n",
    "    dfresult['Parch'] = dfdata['Parch']\n",
    "    dfresult['Fare'] = dfdata['Fare']\n",
    "\n",
    "    #Carbin\n",
    "    dfresult['Cabin_null'] =  pd.isna(dfdata['Cabin']).astype('int32')\n",
    "\n",
    "    #Embarked\n",
    "    dfEmbarked = pd.get_dummies(dfdata['Embarked'],dummy_na=True)\n",
    "    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]\n",
    "    dfresult = pd.concat([dfresult,dfEmbarked],axis = 1)\n",
    "\n",
    "    return(dfresult)\n",
    "\n",
    "x_train = preprocessing(dftrain_raw)\n",
    "y_train = dftrain_raw['Survived'].values\n",
    "\n",
    "x_test = preprocessing(dftest_raw)\n",
    "y_test = dftest_raw['Survived'].values\n",
    "\n",
    "print(\"x_train.shape =\", x_train.shape )\n",
    "print(\"x_test.shape =\", x_test.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x_train.shape = (712, 15)\n",
    "x_test.shape = (179, 15)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "æ­¤å¤„é€‰æ‹©ä½¿ç”¨æœ€ç®€å•çš„Sequentialï¼ŒæŒ‰å±‚é¡ºåºæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20,activation = 'relu',input_shape=(15,)))\n",
    "model.add(layers.Dense(10,activation = 'relu' ))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid' ))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 20)                320       \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                210       \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 541\n",
    "Trainable params: 541\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºŒåˆ†ç±»é—®é¢˜é€‰æ‹©äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°\n",
    "model.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['AUC'])\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    batch_size= 64,\n",
    "                    epochs= 30,\n",
    "                    validation_split=0.2 #åˆ†å‰²ä¸€éƒ¨åˆ†è®­ç»ƒæ•°æ®ç”¨äºéªŒè¯\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train on 569 samples, validate on 143 samples\n",
    "Epoch 1/30\n",
    "569/569 [==============================] - 1s 2ms/sample - loss: 3.5841 - AUC: 0.4079 - val_loss: 3.4429 - val_AUC: 0.4129\n",
    "Epoch 2/30\n",
    "569/569 [==============================] - 0s 102us/sample - loss: 2.6093 - AUC: 0.3967 - val_loss: 2.4886 - val_AUC: 0.4139\n",
    "Epoch 3/30\n",
    "569/569 [==============================] - 0s 68us/sample - loss: 1.8375 - AUC: 0.4003 - val_loss: 1.7383 - val_AUC: 0.4223\n",
    "Epoch 4/30\n",
    "569/569 [==============================] - 0s 83us/sample - loss: 1.2545 - AUC: 0.4390 - val_loss: 1.1936 - val_AUC: 0.4765\n",
    "Epoch 5/30\n",
    "569/569 [==============================] - ETA: 0s - loss: 1.4435 - AUC: 0.375 - 0s 90us/sample - loss: 0.9141 - AUC: 0.5192 - val_loss: 0.8274 - val_AUC: 0.5584\n",
    "Epoch 6/30\n",
    "569/569 [==============================] - 0s 110us/sample - loss: 0.7052 - AUC: 0.6290 - val_loss: 0.6596 - val_AUC: 0.6880\n",
    "Epoch 7/30\n",
    "569/569 [==============================] - 0s 90us/sample - loss: 0.6410 - AUC: 0.7086 - val_loss: 0.6519 - val_AUC: 0.6845\n",
    "Epoch 8/30\n",
    "569/569 [==============================] - 0s 93us/sample - loss: 0.6246 - AUC: 0.7080 - val_loss: 0.6480 - val_AUC: 0.6846\n",
    "Epoch 9/30\n",
    "569/569 [==============================] - 0s 73us/sample - loss: 0.6088 - AUC: 0.7113 - val_loss: 0.6497 - val_AUC: 0.6838\n",
    "Epoch 10/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.6051 - AUC: 0.7117 - val_loss: 0.6454 - val_AUC: 0.6873\n",
    "Epoch 11/30\n",
    "569/569 [==============================] - 0s 96us/sample - loss: 0.5972 - AUC: 0.7218 - val_loss: 0.6369 - val_AUC: 0.6888\n",
    "Epoch 12/30\n",
    "569/569 [==============================] - 0s 92us/sample - loss: 0.5918 - AUC: 0.7294 - val_loss: 0.6330 - val_AUC: 0.6908\n",
    "Epoch 13/30\n",
    "569/569 [==============================] - 0s 75us/sample - loss: 0.5864 - AUC: 0.7363 - val_loss: 0.6281 - val_AUC: 0.6948\n",
    "Epoch 14/30\n",
    "569/569 [==============================] - 0s 104us/sample - loss: 0.5832 - AUC: 0.7426 - val_loss: 0.6240 - val_AUC: 0.7030\n",
    "Epoch 15/30\n",
    "569/569 [==============================] - 0s 74us/sample - loss: 0.5777 - AUC: 0.7507 - val_loss: 0.6200 - val_AUC: 0.7066\n",
    "Epoch 16/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.5726 - AUC: 0.7569 - val_loss: 0.6155 - val_AUC: 0.7132\n",
    "Epoch 17/30\n",
    "569/569 [==============================] - 0s 99us/sample - loss: 0.5674 - AUC: 0.7643 - val_loss: 0.6070 - val_AUC: 0.7255\n",
    "Epoch 18/30\n",
    "569/569 [==============================] - 0s 97us/sample - loss: 0.5631 - AUC: 0.7721 - val_loss: 0.6061 - val_AUC: 0.7305\n",
    "Epoch 19/30\n",
    "569/569 [==============================] - 0s 73us/sample - loss: 0.5580 - AUC: 0.7792 - val_loss: 0.6027 - val_AUC: 0.7332\n",
    "Epoch 20/30\n",
    "569/569 [==============================] - 0s 85us/sample - loss: 0.5533 - AUC: 0.7861 - val_loss: 0.5997 - val_AUC: 0.7366\n",
    "Epoch 21/30\n",
    "569/569 [==============================] - 0s 87us/sample - loss: 0.5497 - AUC: 0.7926 - val_loss: 0.5961 - val_AUC: 0.7433\n",
    "Epoch 22/30\n",
    "569/569 [==============================] - 0s 101us/sample - loss: 0.5454 - AUC: 0.7987 - val_loss: 0.5943 - val_AUC: 0.7438\n",
    "Epoch 23/30\n",
    "569/569 [==============================] - 0s 100us/sample - loss: 0.5398 - AUC: 0.8057 - val_loss: 0.5926 - val_AUC: 0.7492\n",
    "Epoch 24/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.5328 - AUC: 0.8122 - val_loss: 0.5912 - val_AUC: 0.7493\n",
    "Epoch 25/30\n",
    "569/569 [==============================] - 0s 86us/sample - loss: 0.5283 - AUC: 0.8147 - val_loss: 0.5902 - val_AUC: 0.7509\n",
    "Epoch 26/30\n",
    "569/569 [==============================] - 0s 67us/sample - loss: 0.5246 - AUC: 0.8196 - val_loss: 0.5845 - val_AUC: 0.7552\n",
    "Epoch 27/30\n",
    "569/569 [==============================] - 0s 72us/sample - loss: 0.5205 - AUC: 0.8271 - val_loss: 0.5837 - val_AUC: 0.7584\n",
    "Epoch 28/30\n",
    "569/569 [==============================] - 0s 74us/sample - loss: 0.5144 - AUC: 0.8302 - val_loss: 0.5848 - val_AUC: 0.7561\n",
    "Epoch 29/30\n",
    "569/569 [==============================] - 0s 77us/sample - loss: 0.5099 - AUC: 0.8326 - val_loss: 0.5809 - val_AUC: 0.7583\n",
    "Epoch 30/30\n",
    "569/569 [==============================] - 0s 80us/sample - loss: 0.5071 - AUC: 0.8349 - val_loss: 0.5816 - val_AUC: 0.7605\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬é¦–å…ˆè¯„ä¼°ä¸€ä¸‹æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-Lossæ›²çº¿.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-AUCæ›²çº¿.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å†çœ‹ä¸€ä¸‹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x = x_test,y = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#é¢„æµ‹æ¦‚ç‡\n",
    "model.predict(x_test[0:10])\n",
    "#model(tf.constant(x_test[0:10].values,dtype = tf.float32)) #ç­‰ä»·å†™æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.26501188],\n",
    "       [0.40970832],\n",
    "       [0.44285864],\n",
    "       [0.78408605],\n",
    "       [0.47650957],\n",
    "       [0.43849158],\n",
    "       [0.27426785],\n",
    "       [0.5962582 ],\n",
    "       [0.59476686],\n",
    "       [0.17882936]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#é¢„æµ‹ç±»åˆ«\n",
    "model.predict_classes(x_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0],\n",
    "       [0],\n",
    "       [0],\n",
    "       [1],\n",
    "       [0],\n",
    "       [0],\n",
    "       [0],\n",
    "       [1],\n",
    "       [1],\n",
    "       [0]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨Kerasæ–¹å¼ä¿å­˜æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜ã€‚å‰è€…ä»…ä»…é€‚åˆä½¿ç”¨Pythonç¯å¢ƒæ¢å¤æ¨¡å‹ï¼Œåè€…åˆ™å¯ä»¥è·¨å¹³å°è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚\n",
    "\n",
    "æ¨èä½¿ç”¨åä¸€ç§æ–¹å¼è¿›è¡Œä¿å­˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ï¼ŒKerasæ–¹å¼ä¿å­˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹ç»“æ„åŠæƒé‡\n",
    "\n",
    "model.save('./data/keras_model.h5')  \n",
    "\n",
    "del model  #åˆ é™¤ç°æœ‰æ¨¡å‹\n",
    "\n",
    "# identical to the previous one\n",
    "model = models.load_model('./data/keras_model.h5')\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹ç»“æ„\n",
    "json_str = model.to_json()\n",
    "\n",
    "# æ¢å¤æ¨¡å‹ç»“æ„\n",
    "model_json = models.model_from_json(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜æ¨¡å‹æƒé‡\n",
    "model.save_weights('./data/keras_model_weight.h5')\n",
    "\n",
    "# æ¢å¤æ¨¡å‹ç»“æ„\n",
    "model_json = models.model_from_json(json_str)\n",
    "model_json.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "# åŠ è½½æƒé‡\n",
    "model_json.load_weights('./data/keras_model_weight.h5')\n",
    "model_json.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ï¼ŒTensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æƒé‡ï¼Œè¯¥æ–¹å¼ä»…ä»…ä¿å­˜æƒé‡å¼ é‡\n",
    "model.save_weights('./data/tf_model_weights.ckpt',save_format = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹ç»“æ„ä¸æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶,è¯¥æ–¹å¼ä¿å­˜çš„æ¨¡å‹å…·æœ‰è·¨å¹³å°æ€§ä¾¿äºéƒ¨ç½²\n",
    "\n",
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191365896656527, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar2æ•°æ®é›†ä¸ºcifar10æ•°æ®é›†çš„å­é›†ï¼ŒåªåŒ…æ‹¬å‰ä¸¤ç§ç±»åˆ«airplaneå’Œautomobileã€‚\n",
    "\n",
    "è®­ç»ƒé›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„5000å¼ ï¼Œæµ‹è¯•é›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„1000å¼ ã€‚\n",
    "\n",
    "cifar2ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å¯¹é£æœºairplaneå’ŒæœºåŠ¨è½¦automobileä¸¤ç§å›¾ç‰‡è¿›è¡Œåˆ†ç±»ã€‚\n",
    "\n",
    "æˆ‘ä»¬å‡†å¤‡çš„Cifar2æ•°æ®é›†çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚\n",
    "\n",
    "![](./data/cifar2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨tensorflowä¸­å‡†å¤‡å›¾ç‰‡æ•°æ®çš„å¸¸ç”¨æ–¹æ¡ˆæœ‰ä¸¤ç§ï¼Œç¬¬ä¸€ç§æ˜¯ä½¿ç”¨tf.kerasä¸­çš„ImageDataGeneratorå·¥å…·æ„å»ºå›¾ç‰‡æ•°æ®ç”Ÿæˆå™¨ã€‚\n",
    "\n",
    "ç¬¬äºŒç§æ˜¯ä½¿ç”¨tf.data.Datasetæ­é…tf.imageä¸­çš„ä¸€äº›å›¾ç‰‡å¤„ç†æ–¹æ³•æ„å»ºæ•°æ®ç®¡é“ã€‚\n",
    "\n",
    "ç¬¬ä¸€ç§æ–¹æ³•æ›´ä¸ºç®€å•ï¼Œå…¶ä½¿ç”¨èŒƒä¾‹å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç« ã€‚\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67466552\n",
    "\n",
    "ç¬¬äºŒç§æ–¹æ³•æ˜¯TensorFlowçš„åŸç”Ÿæ–¹æ³•ï¼Œæ›´åŠ çµæ´»ï¼Œä½¿ç”¨å¾—å½“çš„è¯ä¹Ÿå¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ­¤å¤„ä»‹ç»ç¬¬äºŒç§æ–¹æ³•ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import datasets,layers,models\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = tf.constant(1,tf.int8) if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") \\\n",
    "            else tf.constant(0,tf.int8)\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼\n",
    "    img = tf.image.resize(img,size)/255.0\n",
    "    return(img,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨å¹¶è¡ŒåŒ–é¢„å¤„ç†num_parallel_calls å’Œé¢„å­˜æ•°æ®prefetchæ¥æå‡æ€§èƒ½\n",
    "ds_train = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\") \\\n",
    "           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "           .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "           .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "\n",
    "ds_test = tf.data.Dataset.list_files(\"./data/cifar2/test/*/*.jpg\") \\\n",
    "           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "           .batch(BATCH_SIZE) \\\n",
    "           .prefetch(tf.data.experimental.AUTOTUNE)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "for i,(img,label) in enumerate(ds_train.unbatch().take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-å›¾ç‰‡é¢„è§ˆ.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ds_train.take(1):\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(100, 32, 32, 3) (100,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "æ­¤å¤„é€‰æ‹©ä½¿ç”¨å‡½æ•°å¼APIæ„å»ºæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #æ¸…ç©ºä¼šè¯\n",
    "\n",
    "inputs = layers.Input(shape=(32,32,3))\n",
    "x = layers.Conv2D(32,kernel_size=(3,3))(inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(64,kernel_size=(5,5))(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32,activation='relu')(x)\n",
    "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs = inputs,outputs = outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 5, 5, 64)          0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 1600)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 32)                51232     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 33        \n",
    "=================================================================\n",
    "Total params: 103,425\n",
    "Trainable params: 103,425\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "history = model.fit(ds_train,epochs= 10,validation_data=ds_test,\n",
    "                    callbacks = [tensorboard_callback],workers = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 100 steps, validate for 20 steps\n",
    "Epoch 1/10\n",
    "100/100 [==============================] - 16s 156ms/step - loss: 0.4830 - accuracy: 0.7697 - val_loss: 0.3396 - val_accuracy: 0.8475\n",
    "Epoch 2/10\n",
    "100/100 [==============================] - 14s 142ms/step - loss: 0.3437 - accuracy: 0.8469 - val_loss: 0.2997 - val_accuracy: 0.8680\n",
    "Epoch 3/10\n",
    "100/100 [==============================] - 13s 131ms/step - loss: 0.2871 - accuracy: 0.8777 - val_loss: 0.2390 - val_accuracy: 0.9015\n",
    "Epoch 4/10\n",
    "100/100 [==============================] - 12s 117ms/step - loss: 0.2410 - accuracy: 0.9040 - val_loss: 0.2005 - val_accuracy: 0.9195\n",
    "Epoch 5/10\n",
    "100/100 [==============================] - 13s 130ms/step - loss: 0.1992 - accuracy: 0.9213 - val_loss: 0.1949 - val_accuracy: 0.9180\n",
    "Epoch 6/10\n",
    "100/100 [==============================] - 14s 136ms/step - loss: 0.1737 - accuracy: 0.9323 - val_loss: 0.1723 - val_accuracy: 0.9275\n",
    "Epoch 7/10\n",
    "100/100 [==============================] - 14s 139ms/step - loss: 0.1531 - accuracy: 0.9412 - val_loss: 0.1670 - val_accuracy: 0.9310\n",
    "Epoch 8/10\n",
    "100/100 [==============================] - 13s 134ms/step - loss: 0.1299 - accuracy: 0.9525 - val_loss: 0.1553 - val_accuracy: 0.9340\n",
    "Epoch 9/10\n",
    "100/100 [==============================] - 14s 137ms/step - loss: 0.1158 - accuracy: 0.9556 - val_loss: 0.1581 - val_accuracy: 0.9340\n",
    "Epoch 10/10\n",
    "100/100 [==============================] - 14s 142ms/step - loss: 0.1006 - accuracy: 0.9617 - val_loss: 0.1614 - val_accuracy: 0.9345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ./data/keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹\n",
    "notebook.start(\"--logdir ./data/keras_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-tensorboard.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dfhistory = pd.DataFrame(history.history)\n",
    "dfhistory.index = range(1,len(dfhistory) + 1)\n",
    "dfhistory.index.name = 'epoch'\n",
    "\n",
    "dfhistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-dfhistory.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-Lossæ›²çº¿.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-Accuracyæ›²çº¿.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯ä»¥ä½¿ç”¨evaluateå¯¹æ•°æ®è¿›è¡Œè¯„ä¼°\n",
    "val_loss,val_accuracy = model.evaluate(ds_test,workers=4)\n",
    "print(val_loss,val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.16139143370091916 0.9345\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨model.predict(ds_test)è¿›è¡Œé¢„æµ‹ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥ä½¿ç”¨model.predict_on_batch(x_test)å¯¹ä¸€ä¸ªæ‰¹é‡è¿›è¡Œé¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[9.9996173e-01],\n",
    "       [9.5104784e-01],\n",
    "       [2.8648047e-04],\n",
    "       ...,\n",
    "       [1.1484033e-03],\n",
    "       [3.5589080e-02],\n",
    "       [9.8537153e-01]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ds_test.take(1):\n",
    "    print(model.predict_on_batch(x[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[3.8065155e-05]\n",
    " [8.8236779e-01]\n",
    " [9.1433197e-01]\n",
    " [9.9921846e-01]\n",
    " [6.4052093e-01]\n",
    " [4.9970779e-03]\n",
    " [2.6735585e-04]\n",
    " [9.9842811e-01]\n",
    " [7.9198682e-01]\n",
    " [7.4823302e-01]\n",
    " [8.7208226e-03]\n",
    " [9.3951421e-03]\n",
    " [9.9790359e-01]\n",
    " [9.9998581e-01]\n",
    " [2.1642199e-05]\n",
    " [1.7915063e-02]\n",
    " [2.5839690e-02]\n",
    " [9.7538447e-01]\n",
    " [9.7393811e-01]\n",
    " [9.7333014e-01]], shape=(20, 1), dtype=float32)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æƒé‡ï¼Œè¯¥æ–¹å¼ä»…ä»…ä¿å­˜æƒé‡å¼ é‡\n",
    "model.save_weights('./data/tf_model_weights.ckpt',save_format = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹ç»“æ„ä¸æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶,è¯¥æ–¹å¼ä¿å­˜çš„æ¨¡å‹å…·æœ‰è·¨å¹³å°æ€§ä¾¿äºéƒ¨ç½²\n",
    "\n",
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.16139124035835267, 0.9345]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "# 1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdbæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ç”µå½±è¯„è®ºçš„æ–‡æœ¬å†…å®¹é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚\n",
    "\n",
    "è®­ç»ƒé›†æœ‰20000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œæµ‹è¯•é›†æœ‰5000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œå…¶ä¸­æ­£é¢è¯„è®ºå’Œè´Ÿé¢è¯„è®ºéƒ½å„å ä¸€åŠã€‚\n",
    "\n",
    "æ–‡æœ¬æ•°æ®é¢„å¤„ç†è¾ƒä¸ºç¹çï¼ŒåŒ…æ‹¬ä¸­æ–‡åˆ‡è¯ï¼ˆæœ¬ç¤ºä¾‹ä¸æ¶‰åŠï¼‰ï¼Œæ„å»ºè¯å…¸ï¼Œç¼–ç è½¬æ¢ï¼Œåºåˆ—å¡«å……ï¼Œæ„å»ºæ•°æ®ç®¡é“ç­‰ç­‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨tensorflowä¸­å®Œæˆæ–‡æœ¬æ•°æ®é¢„å¤„ç†çš„å¸¸ç”¨æ–¹æ¡ˆæœ‰ä¸¤ç§ï¼Œç¬¬ä¸€ç§æ˜¯åˆ©ç”¨tf.keras.preprocessingä¸­çš„Tokenizerè¯å…¸æ„å»ºå·¥å…·å’Œtf.keras.utils.Sequenceæ„å»ºæ–‡æœ¬æ•°æ®ç”Ÿæˆå™¨ç®¡é“ã€‚\n",
    "\n",
    "ç¬¬äºŒç§æ˜¯ä½¿ç”¨tf.data.Datasetæ­é….keras.layers.experimental.preprocessing.TextVectorizationé¢„å¤„ç†å±‚ã€‚\n",
    "\n",
    "ç¬¬ä¸€ç§æ–¹æ³•è¾ƒä¸ºå¤æ‚ï¼Œå…¶ä½¿ç”¨èŒƒä¾‹å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç« ã€‚\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67697840\n",
    "\n",
    "ç¬¬äºŒç§æ–¹æ³•ä¸ºTensorFlowåŸç”Ÿæ–¹å¼ï¼Œç›¸å¯¹ä¹Ÿæ›´åŠ ç®€å•ä¸€äº›ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ­¤å¤„ä»‹ç»ç¬¬äºŒç§æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/ç”µå½±è¯„è®º.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path =  \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯\n",
    "MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "#æ„å»ºç®¡é“\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)\n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#æ„å»ºè¯å…¸\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #æœ‰ä¸€ä¸ªç•™ç»™å ä½ç¬¦\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "\n",
    "#å•è¯ç¼–ç \n",
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not', b'you', b'his', b'are', b'have', b'be', b'he', b'one', b'its', b'at', b'all', b'by', b'an', b'they', b'from', b'who', b'so', b'like', b'her', b'just', b'or', b'about', b'has', b'if', b'out', b'some', b'there', b'what', b'good', b'more', b'when', b'very', b'she', b'even', b'my', b'no', b'would', b'up', b'time', b'only', b'which', b'story', b'really', b'their', b'were', b'had', b'see', b'can', b'me', b'than', b'we', b'much', b'well', b'get', b'been', b'will', b'into', b'people', b'also', b'other', b'do', b'bad', b'because', b'great', b'first', b'how', b'him', b'most', b'dont', b'made', b'then', b'them', b'films', b'movies', b'way', b'make', b'could', b'too', b'any', b'after', b'characters']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "æ­¤å¤„é€‰æ‹©ä½¿ç”¨ç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºè‡ªå®šä¹‰æ¨¡å‹èŒƒä¾‹ï¼Œå®é™…ä¸Šåº”è¯¥ä¼˜å…ˆä½¿ç”¨Sequentialæˆ–è€…å‡½æ•°å¼API\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\n",
    "        self.pool = layers.MaxPool1D()\n",
    "        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(CnnModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n",
    "    \n",
    "model = CnnModel()\n",
    "model.build(input_shape =(None,MAX_LEN))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"cnn_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        multiple                  70000     \n",
    "_________________________________________________________________\n",
    "conv_1 (Conv1D)              multiple                  576       \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) multiple                  0         \n",
    "_________________________________________________________________\n",
    "conv_2 (Conv1D)              multiple                  4224      \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            multiple                  0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  6145      \n",
    "=================================================================\n",
    "Total params: 80,945\n",
    "Trainable params: 80,945\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features,training = False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "        \n",
    "        #æ­¤å¤„logsæ¨¡æ¿éœ€è¦æ ¹æ®metricå…·ä½“æƒ…å†µä¿®æ”¹\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' \n",
    "        \n",
    "        if epoch%1==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,epochs = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:54:08\n",
    "Epoch=1,Loss:0.442317516,Accuracy:0.7695,Valid Loss:0.323672801,Valid Accuracy:0.8614\n",
    "\n",
    "================================================================================13:54:20\n",
    "Epoch=2,Loss:0.245737702,Accuracy:0.90215,Valid Loss:0.356488883,Valid Accuracy:0.8554\n",
    "\n",
    "================================================================================13:54:32\n",
    "Epoch=3,Loss:0.17360799,Accuracy:0.93455,Valid Loss:0.361132562,Valid Accuracy:0.8674\n",
    "\n",
    "================================================================================13:54:44\n",
    "Epoch=4,Loss:0.113476314,Accuracy:0.95975,Valid Loss:0.483677238,Valid Accuracy:0.856\n",
    "\n",
    "================================================================================13:54:57\n",
    "Epoch=5,Loss:0.0698405355,Accuracy:0.9768,Valid Loss:0.607856631,Valid Accuracy:0.857\n",
    "\n",
    "================================================================================13:55:15\n",
    "Epoch=6,Loss:0.0366807655,Accuracy:0.98825,Valid Loss:0.745884955,Valid Accuracy:0.854\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒçš„æ¨¡å‹æ²¡æœ‰ç»è¿‡ç¼–è¯‘ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨model.evaluate(ds_valid)æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model,ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "         valid_step(model,features,labels)\n",
    "    logs = 'Valid Loss:{},Valid Accuracy:{}' \n",
    "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model,ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Valid Loss:0.745884418,Valid Accuracy:0.854\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•:\n",
    "\n",
    "* model.predict(ds_test)\n",
    "* model(x_test)\n",
    "* model.call(x_test)\n",
    "* model.predict_on_batch(x_test)\n",
    "\n",
    "æ¨èä¼˜å…ˆä½¿ç”¨model.predict(ds_test)æ–¹æ³•ï¼Œæ—¢å¯ä»¥å¯¹Datasetï¼Œä¹Ÿå¯ä»¥å¯¹Tensorä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.7864823 ],\n",
    "       [0.9999901 ],\n",
    "       [0.99944776],\n",
    "       ...,\n",
    "       [0.8498302 ],\n",
    "       [0.13382755],\n",
    "       [1.        ]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_test,_ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    #ä»¥ä¸‹æ–¹æ³•ç­‰ä»·ï¼š\n",
    "    #print(model.call(x_test))\n",
    "    #print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[7.8648227e-01]\n",
    " [9.9999011e-01]\n",
    " [9.9944776e-01]\n",
    " [3.7153201e-09]\n",
    " [9.4462049e-01]\n",
    " [2.3522753e-04]\n",
    " [1.2044354e-04]\n",
    " [9.3752089e-07]\n",
    " [9.9996352e-01]\n",
    " [9.3435925e-01]\n",
    " [9.8746723e-01]\n",
    " [9.9908626e-01]\n",
    " [4.1563155e-08]\n",
    " [4.1808244e-03]\n",
    " [8.0184749e-05]\n",
    " [8.3910513e-01]\n",
    " [3.5167937e-05]\n",
    " [7.2113985e-01]\n",
    " [4.5228912e-03]\n",
    " [9.9942589e-01]], shape=(20, 1), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.7864823 ],\n",
    "       [0.9999901 ],\n",
    "       [0.99944776],\n",
    "       ...,\n",
    "       [0.8498302 ],\n",
    "       [0.13382755],\n",
    "       [1.        ]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ä»å‘ç°è‡³ä»Šå·²ç»æŒç»­3ä¸ªå¤šæœˆäº†ï¼Œè¿™åœºèµ·æºäºåƒé‡å‘³çš„ç¾éš¾ç»™å¤§å®¶çš„ç”Ÿæ´»é€ æˆäº†è¯¸å¤šæ–¹é¢çš„å½±å“ã€‚\n",
    "\n",
    "æœ‰çš„åŒå­¦æ˜¯æ”¶å…¥ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯æ„Ÿæƒ…ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯å¿ƒç†ä¸Šçš„ï¼Œè¿˜æœ‰çš„åŒå­¦æ˜¯ä½“é‡ä¸Šçš„ã€‚\n",
    "\n",
    "é‚£ä¹ˆå›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ä½•æ—¶ç»“æŸå‘¢ï¼Ÿä»€ä¹ˆæ—¶å€™æˆ‘ä»¬æ‰å¯ä»¥é‡è·è‡ªç”±å‘¢ï¼Ÿ\n",
    "\n",
    "æœ¬ç¯‡æ–‡ç« å°†åˆ©ç”¨TensorFlow2.0å»ºç«‹æ—¶é—´åºåˆ—RNNæ¨¡å‹ï¼Œå¯¹å›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ç»“æŸæ—¶é—´è¿›è¡Œé¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/ç–«æƒ…å‰åå¯¹æ¯”.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬æ–‡çš„æ•°æ®é›†å–è‡ªtushareï¼Œè·å–è¯¥æ•°æ®é›†çš„æ–¹æ³•å‚è€ƒäº†ä»¥ä¸‹æ–‡ç« ã€‚\n",
    "\n",
    "ã€Šhttps://zhuanlan.zhihu.com/p/109556102ã€‹\n",
    "\n",
    "![](./data/1-4-æ–°å¢äººæ•°.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers,losses,metrics,callbacks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "df = pd.read_csv(\"./data/covid-19.csv\",sep = \"\\t\")\n",
    "df.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-ç´¯ç§¯æ›²çº¿.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.set_index(\"date\")\n",
    "dfdiff = dfdata.diff(periods=1).dropna()\n",
    "dfdiff = dfdiff.reset_index(\"date\")\n",
    "\n",
    "dfdiff.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60)\n",
    "dfdiff = dfdiff.drop(\"date\",axis = 1).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-æ–°å¢æ›²çº¿.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç”¨æŸæ—¥å‰8å¤©çª—å£æ•°æ®ä½œä¸ºè¾“å…¥é¢„æµ‹è¯¥æ—¥æ•°æ®\n",
    "WINDOW_SIZE = 8\n",
    "\n",
    "def batch_dataset(dataset):\n",
    "    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=True)\n",
    "    return dataset_batched\n",
    "\n",
    "ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \\\n",
    "   .window(WINDOW_SIZE,shift=1).flat_map(batch_dataset)\n",
    "\n",
    "ds_label = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))\n",
    "\n",
    "#æ•°æ®è¾ƒå°ï¼Œå¯ä»¥å°†å…¨éƒ¨è®­ç»ƒæ•°æ®æ”¾å…¥åˆ°ä¸€ä¸ªbatchä¸­ï¼Œæå‡æ€§èƒ½\n",
    "ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(38).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "æ­¤å¤„é€‰æ‹©ä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è€ƒè™‘åˆ°æ–°å¢ç¡®è¯Šï¼Œæ–°å¢æ²»æ„ˆï¼Œæ–°å¢æ­»äº¡äººæ•°æ•°æ®ä¸å¯èƒ½å°äº0ï¼Œè®¾è®¡å¦‚ä¸‹ç»“æ„\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Block, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x_input,x):\n",
    "        x_out = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "        return x_out\n",
    "    \n",
    "    def get_config(self):  \n",
    "        config = super(Block, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "x_input = layers.Input(shape = (None,3),dtype = tf.float32)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x_input)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,input_shape=(None,3))(x)\n",
    "x = layers.Dense(3)(x)\n",
    "\n",
    "#è€ƒè™‘åˆ°æ–°å¢ç¡®è¯Šï¼Œæ–°å¢æ²»æ„ˆï¼Œæ–°å¢æ­»äº¡äººæ•°æ•°æ®ä¸å¯èƒ½å°äº0ï¼Œè®¾è®¡å¦‚ä¸‹ç»“æ„\n",
    "#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "x = Block()(x_input,x)\n",
    "model = models.Model(inputs = [x_input],outputs = [x])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, None, 3)]         0         \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_3 (LSTM)                (None, 3)                 84        \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 3)                 12        \n",
    "_________________________________________________________________\n",
    "block (Block)                (None, 3)                 0         \n",
    "=================================================================\n",
    "Total params: 348\n",
    "Trainable params: 348\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚\n",
    "\n",
    "æ³¨ï¼šå¾ªç¯ç¥ç»ç½‘ç»œè°ƒè¯•è¾ƒä¸ºå›°éš¾ï¼Œéœ€è¦è®¾ç½®å¤šä¸ªä¸åŒçš„å­¦ä¹ ç‡å¤šæ¬¡å°è¯•ï¼Œä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè€ƒè™‘å¹³æ–¹å·®å’Œé¢„æµ‹ç›®æ ‡çš„æ¯”å€¼\n",
    "class MSPE(losses.Loss):\n",
    "    def call(self,y_true,y_pred):\n",
    "        err_percent = (y_true - y_pred)**2/(tf.maximum(y_true**2,1e-7))\n",
    "        mean_err_percent = tf.reduce_mean(err_percent)\n",
    "        return mean_err_percent\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MSPE, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer,loss=MSPE(name = \"MSPE\"))\n",
    "\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "#å¦‚æœlossåœ¨100ä¸ªepochåæ²¡æœ‰æå‡ï¼Œå­¦ä¹ ç‡å‡åŠã€‚\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor = 0.5, patience = 100)\n",
    "#å½“lossåœ¨200ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™æå‰ç»ˆæ­¢è®­ç»ƒã€‚\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience= 200)\n",
    "callbacks_list = [tb_callback,lr_callback,stop_callback]\n",
    "\n",
    "history = model.fit(ds_train,epochs=500,callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 371/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.1184\n",
    "Epoch 372/500\n",
    "1/1 [==============================] - 0s 64ms/step - loss: 0.1177\n",
    "Epoch 373/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1169\n",
    "Epoch 374/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.1161\n",
    "Epoch 375/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1154\n",
    "Epoch 376/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1147\n",
    "Epoch 377/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.1140\n",
    "Epoch 378/500\n",
    "1/1 [==============================] - 0s 93ms/step - loss: 0.1133\n",
    "Epoch 379/500\n",
    "1/1 [==============================] - 0s 85ms/step - loss: 0.1126\n",
    "Epoch 380/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.1119\n",
    "Epoch 381/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1113\n",
    "Epoch 382/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.1107\n",
    "Epoch 383/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1100\n",
    "Epoch 384/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1094\n",
    "Epoch 385/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.1088\n",
    "Epoch 386/500\n",
    "1/1 [==============================] - 0s 74ms/step - loss: 0.1082\n",
    "Epoch 387/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1077\n",
    "Epoch 388/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1071\n",
    "Epoch 389/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1066\n",
    "Epoch 390/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1060\n",
    "Epoch 391/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.1055\n",
    "Epoch 392/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1050\n",
    "Epoch 393/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.1045\n",
    "Epoch 394/500\n",
    "1/1 [==============================] - 0s 65ms/step - loss: 0.1040\n",
    "Epoch 395/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.1035\n",
    "Epoch 396/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1031\n",
    "Epoch 397/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.1026\n",
    "Epoch 398/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1022\n",
    "Epoch 399/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.1017\n",
    "Epoch 400/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.1013\n",
    "Epoch 401/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.1009\n",
    "Epoch 402/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.1005\n",
    "Epoch 403/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1001\n",
    "Epoch 404/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0997\n",
    "Epoch 405/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0993\n",
    "Epoch 406/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0990\n",
    "Epoch 407/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0986\n",
    "Epoch 408/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.0982\n",
    "Epoch 409/500\n",
    "1/1 [==============================] - 0s 67ms/step - loss: 0.0979\n",
    "Epoch 410/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0976\n",
    "Epoch 411/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0972\n",
    "Epoch 412/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0969\n",
    "Epoch 413/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0966\n",
    "Epoch 414/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0963\n",
    "Epoch 415/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0960\n",
    "Epoch 416/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0957\n",
    "Epoch 417/500\n",
    "1/1 [==============================] - 0s 69ms/step - loss: 0.0954\n",
    "Epoch 418/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0951\n",
    "Epoch 419/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0948\n",
    "Epoch 420/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0946\n",
    "Epoch 421/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0943\n",
    "Epoch 422/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0941\n",
    "Epoch 423/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0938\n",
    "Epoch 424/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0936\n",
    "Epoch 425/500\n",
    "1/1 [==============================] - 0s 100ms/step - loss: 0.0933\n",
    "Epoch 426/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0931\n",
    "Epoch 427/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0929\n",
    "Epoch 428/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0926\n",
    "Epoch 429/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0924\n",
    "Epoch 430/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0922\n",
    "Epoch 431/500\n",
    "1/1 [==============================] - 0s 75ms/step - loss: 0.0920\n",
    "Epoch 432/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0918\n",
    "Epoch 433/500\n",
    "1/1 [==============================] - 0s 77ms/step - loss: 0.0916\n",
    "Epoch 434/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0914\n",
    "Epoch 435/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0912\n",
    "Epoch 436/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0911\n",
    "Epoch 437/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0909\n",
    "Epoch 438/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0907\n",
    "Epoch 439/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0905\n",
    "Epoch 440/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0904\n",
    "Epoch 441/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0902\n",
    "Epoch 442/500\n",
    "1/1 [==============================] - 0s 73ms/step - loss: 0.0901\n",
    "Epoch 443/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0899\n",
    "Epoch 444/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0898\n",
    "Epoch 445/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0896\n",
    "Epoch 446/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.0895\n",
    "Epoch 447/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0893\n",
    "Epoch 448/500\n",
    "1/1 [==============================] - 0s 64ms/step - loss: 0.0892\n",
    "Epoch 449/500\n",
    "1/1 [==============================] - 0s 70ms/step - loss: 0.0891\n",
    "Epoch 450/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0889\n",
    "Epoch 451/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0888\n",
    "Epoch 452/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0887\n",
    "Epoch 453/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0886\n",
    "Epoch 454/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0885\n",
    "Epoch 455/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0883\n",
    "Epoch 456/500\n",
    "1/1 [==============================] - 0s 71ms/step - loss: 0.0882\n",
    "Epoch 457/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0881\n",
    "Epoch 458/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0880\n",
    "Epoch 459/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0879\n",
    "Epoch 460/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0878\n",
    "Epoch 461/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0878\n",
    "Epoch 462/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0879\n",
    "Epoch 463/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0879\n",
    "Epoch 464/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0888\n",
    "Epoch 465/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0875\n",
    "Epoch 466/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0873\n",
    "Epoch 467/500\n",
    "1/1 [==============================] - 0s 49ms/step - loss: 0.0872\n",
    "Epoch 468/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0872\n",
    "Epoch 469/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0871\n",
    "Epoch 470/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0871\n",
    "Epoch 471/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0870\n",
    "Epoch 472/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0871\n",
    "Epoch 473/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0869\n",
    "Epoch 474/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.0870\n",
    "Epoch 475/500\n",
    "1/1 [==============================] - 0s 47ms/step - loss: 0.0868\n",
    "Epoch 476/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0868\n",
    "Epoch 477/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0866\n",
    "Epoch 478/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0867\n",
    "Epoch 479/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0865\n",
    "Epoch 480/500\n",
    "1/1 [==============================] - 0s 65ms/step - loss: 0.0866\n",
    "Epoch 481/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0864\n",
    "Epoch 482/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0865\n",
    "Epoch 483/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0863\n",
    "Epoch 484/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0864\n",
    "Epoch 485/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0862\n",
    "Epoch 486/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0863\n",
    "Epoch 487/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.0861\n",
    "Epoch 488/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0862\n",
    "Epoch 489/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0860\n",
    "Epoch 490/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0861\n",
    "Epoch 491/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0859\n",
    "Epoch 492/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0860\n",
    "Epoch 493/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0859\n",
    "Epoch 494/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0860\n",
    "Epoch 495/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0858\n",
    "Epoch 496/500\n",
    "1/1 [==============================] - 0s 69ms/step - loss: 0.0859\n",
    "Epoch 497/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.0857\n",
    "Epoch 498/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0858\n",
    "Epoch 499/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0857\n",
    "Epoch 500/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0858\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯„ä¼°æ¨¡å‹ä¸€èˆ¬è¦è®¾ç½®éªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ï¼Œç”±äºæ­¤ä¾‹æ•°æ®è¾ƒå°‘ï¼Œæˆ‘ä»¬ä»…ä»…å¯è§†åŒ–æŸå¤±å‡½æ•°åœ¨è®­ç»ƒé›†ä¸Šçš„è¿­ä»£æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.title('Training '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-æŸå¤±å‡½æ•°æ›²çº¿.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç–«æƒ…ç»“æŸæ—¶é—´ï¼Œå³ æ–°å¢ç¡®è¯Šç—…ä¾‹ä¸º0 çš„æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨dfresultè®°å½•ç°æœ‰æ•°æ®ä»¥åŠæ­¤åé¢„æµ‹çš„ç–«æƒ…æ•°æ®\n",
    "dfresult = dfdiff[[\"confirmed_num\",\"cured_num\",\"dead_num\"]].copy()\n",
    "dfresult.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-æ—¥æœŸ3æœˆ10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#é¢„æµ‹æ­¤å100å¤©çš„æ–°å¢èµ°åŠ¿,å°†å…¶ç»“æœæ·»åŠ åˆ°dfresultä¸­\n",
    "for i in range(100):\n",
    "    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[-38:,:],axis = 0)))\n",
    "\n",
    "    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),\n",
    "                columns = dfresult.columns)\n",
    "    dfresult = dfresult.append(dfpredict,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"confirmed_num==0\").head()\n",
    "\n",
    "# ç¬¬55å¤©å¼€å§‹æ–°å¢ç¡®è¯Šé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯10å¤©åï¼Œå³é¢„è®¡3æœˆ20æ—¥æ–°å¢ç¡®è¯Šé™ä¸º0\n",
    "# æ³¨ï¼šè¯¥é¢„æµ‹åä¹è§‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-é¢„æµ‹ç¡®è¯Š.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"cured_num==0\").head()\n",
    "\n",
    "# ç¬¬164å¤©å¼€å§‹æ–°å¢æ²»æ„ˆé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚4ä¸ªæœˆåï¼Œå³7æœˆ10æ—¥å·¦å³å…¨éƒ¨æ²»æ„ˆã€‚\n",
    "# æ³¨: è¯¥é¢„æµ‹åæ‚²è§‚ï¼Œå¹¶ä¸”å­˜åœ¨é—®é¢˜ï¼Œå¦‚æœå°†æ¯å¤©æ–°å¢æ²»æ„ˆäººæ•°åŠ èµ·æ¥ï¼Œå°†è¶…è¿‡ç´¯è®¡ç¡®è¯Šäººæ•°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-é¢„æµ‹æ²»æ„ˆ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"dead_num==0\").head()\n",
    "\n",
    "# ç¬¬60å¤©å¼€å§‹ï¼Œæ–°å¢æ­»äº¡é™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚15å¤©åï¼Œå³20200325\n",
    "# è¯¥é¢„æµ‹è¾ƒä¸ºåˆç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-é¢„æµ‹æ­»äº¡.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel',compile=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_loaded.compile(optimizer=optimizer,loss=MSPE(name = \"MSPE\"))\n",
    "model_loaded.predict(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "TensorFlowâ„¢ æ˜¯ä¸€ä¸ªé‡‡ç”¨ **æ•°æ®æµå›¾**ï¼ˆdata flow graphsï¼‰ï¼Œç”¨äºæ•°å€¼è®¡ç®—çš„å¼€æºè½¯ä»¶åº“ã€‚èŠ‚ç‚¹ï¼ˆNodesï¼‰åœ¨å›¾ä¸­è¡¨ç¤ºæ•°å­¦æ“ä½œï¼Œå›¾ä¸­çš„çº¿ï¼ˆedgesï¼‰åˆ™è¡¨ç¤ºåœ¨èŠ‚ç‚¹é—´ç›¸äº’è”ç³»çš„å¤šç»´æ•°æ®æ•°ç»„ï¼Œå³å¼ é‡ï¼ˆtensorï¼‰ã€‚å®ƒçµæ´»çš„æ¶æ„è®©ä½ å¯ä»¥**åœ¨å¤šç§å¹³å°ä¸Šå±•å¼€è®¡ç®—**ï¼Œä¾‹å¦‚å°å¼è®¡ç®—æœºä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªCPUï¼ˆæˆ–GPUï¼‰ï¼ŒæœåŠ¡å™¨ï¼Œç§»åŠ¨è®¾å¤‡ç­‰ç­‰ã€‚TensorFlow æœ€åˆç”±Googleå¤§è„‘å°ç»„ï¼ˆéš¶å±äºGoogleæœºå™¨æ™ºèƒ½ç ”ç©¶æœºæ„ï¼‰çš„ç ”ç©¶å‘˜å’Œå·¥ç¨‹å¸ˆä»¬å¼€å‘å‡ºæ¥ï¼Œ**ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œ**æ–¹é¢çš„ç ”ç©¶ï¼Œä½†è¿™ä¸ªç³»ç»Ÿçš„é€šç”¨æ€§ä½¿å…¶ä¹Ÿå¯**å¹¿æ³›ç”¨äºå…¶ä»–è®¡ç®—é¢†åŸŸ**ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowçš„ä¸»è¦ä¼˜ç‚¹ï¼š\n",
    "\n",
    "* çµæ´»æ€§ï¼šæ”¯æŒåº•å±‚æ•°å€¼è®¡ç®—ï¼ŒC++è‡ªå®šä¹‰æ“ä½œç¬¦\n",
    "\n",
    "* å¯ç§»æ¤æ€§ï¼šä»æœåŠ¡å™¨åˆ°PCåˆ°æ‰‹æœºï¼Œä»CPUåˆ°GPUåˆ°TPU\n",
    "\n",
    "* åˆ†å¸ƒå¼è®¡ç®—ï¼šåˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—ï¼Œå¯æŒ‡å®šæ“ä½œç¬¦å¯¹åº”è®¡ç®—è®¾å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿—è¯è¯´ï¼Œä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·ï¼ŒTensorFlowè¿™åº§å¤§å¦ä¹Ÿæœ‰å®ƒçš„åœ°åŸºã€‚\n",
    "\n",
    "Tensorflowåº•å±‚æœ€æ ¸å¿ƒçš„æ¦‚å¿µæ˜¯å¼ é‡ï¼Œè®¡ç®—å›¾ä»¥åŠè‡ªåŠ¨å¾®åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1,å¼ é‡æ•°æ®ç»“æ„\n",
    "\n",
    "ç¨‹åº = æ•°æ®ç»“æ„+ç®—æ³•ã€‚\n",
    "\n",
    "TensorFlowç¨‹åº = å¼ é‡æ•°æ®ç»“æ„ + è®¡ç®—å›¾ç®—æ³•è¯­è¨€\n",
    "\n",
    "å¼ é‡å’Œè®¡ç®—å›¾æ˜¯ TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µã€‚\n",
    "\n",
    "Tensorflowçš„åŸºæœ¬æ•°æ®ç»“æ„æ˜¯å¼ é‡Tensorã€‚å¼ é‡å³å¤šç»´æ•°ç»„ã€‚Tensorflowçš„å¼ é‡å’Œnumpyä¸­çš„arrayå¾ˆç±»ä¼¼ã€‚\n",
    "\n",
    "ä»è¡Œä¸ºç‰¹æ€§æ¥çœ‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„å¼ é‡ï¼Œå¸¸é‡constantå’Œå˜é‡Variable.\n",
    "\n",
    "å¸¸é‡çš„å€¼åœ¨è®¡ç®—å›¾ä¸­ä¸å¯ä»¥è¢«é‡æ–°èµ‹å€¼ï¼Œå˜é‡å¯ä»¥åœ¨è®¡ç®—å›¾ä¸­ç”¨assignç­‰ç®—å­é‡æ–°èµ‹å€¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå¸¸é‡å¼ é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼ é‡çš„æ•°æ®ç±»å‹å’Œnumpy.arrayåŸºæœ¬ä¸€ä¸€å¯¹åº”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "i = tf.constant(1) # tf.int32 ç±»å‹å¸¸é‡\n",
    "l = tf.constant(1,dtype = tf.int64) # tf.int64 ç±»å‹å¸¸é‡\n",
    "f = tf.constant(1.23) #tf.float32 ç±»å‹å¸¸é‡\n",
    "d = tf.constant(3.14,dtype = tf.double) # tf.double ç±»å‹å¸¸é‡\n",
    "s = tf.constant(\"hello world\") # tf.stringç±»å‹å¸¸é‡\n",
    "b = tf.constant(True) #tf.boolç±»å‹å¸¸é‡\n",
    "\n",
    "\n",
    "print(tf.int64 == np.int64) \n",
    "print(tf.bool == np.bool)\n",
    "print(tf.double == np.float64)\n",
    "print(tf.string == np.unicode) # tf.stringç±»å‹å’Œnp.unicodeç±»å‹ä¸ç­‰ä»·\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "True\n",
    "True\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸åŒç±»å‹çš„æ•°æ®å¯ä»¥ç”¨ä¸åŒç»´åº¦(rank)çš„å¼ é‡æ¥è¡¨ç¤ºã€‚\n",
    "\n",
    "æ ‡é‡ä¸º0ç»´å¼ é‡ï¼Œå‘é‡ä¸º1ç»´å¼ é‡ï¼ŒçŸ©é˜µä¸º2ç»´å¼ é‡ã€‚\n",
    "\n",
    "å½©è‰²å›¾åƒæœ‰rgbä¸‰ä¸ªé€šé“ï¼Œå¯ä»¥è¡¨ç¤ºä¸º3ç»´å¼ é‡ã€‚\n",
    "\n",
    "è§†é¢‘è¿˜æœ‰æ—¶é—´ç»´ï¼Œå¯ä»¥è¡¨ç¤ºä¸º4ç»´å¼ é‡ã€‚\n",
    "\n",
    "å¯ä»¥ç®€å•åœ°æ€»ç»“ä¸ºï¼šæœ‰å‡ å±‚ä¸­æ‹¬å·ï¼Œå°±æ˜¯å¤šå°‘ç»´çš„å¼ é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = tf.constant(True)  #æ ‡é‡ï¼Œ0ç»´å¼ é‡\n",
    "\n",
    "print(tf.rank(scalar))\n",
    "print(scalar.numpy().ndim)  # tf.rankçš„ä½œç”¨å’Œnumpyçš„ndimæ–¹æ³•ç›¸åŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([1.0,2.0,3.0,4.0]) #å‘é‡ï¼Œ1ç»´å¼ é‡\n",
    "\n",
    "print(tf.rank(vector))\n",
    "print(np.ndim(vector.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.constant([[1.0,2.0],[3.0,4.0]]) #çŸ©é˜µ, 2ç»´å¼ é‡\n",
    "\n",
    "print(tf.rank(matrix).numpy())\n",
    "print(np.ndim(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor3 = tf.constant([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])  # 3ç»´å¼ é‡\n",
    "print(tensor3)\n",
    "print(tf.rank(tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[[1. 2.]\n",
    "  [3. 4.]]\n",
    "\n",
    " [[5. 6.]\n",
    "  [7. 8.]]], shape=(2, 2, 2), dtype=float32)\n",
    "tf.Tensor(3, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor4 = tf.constant([[[[1.0,1.0],[2.0,2.0]],[[3.0,3.0],[4.0,4.0]]],\n",
    "                        [[[5.0,5.0],[6.0,6.0]],[[7.0,7.0],[8.0,8.0]]]])  # 4ç»´å¼ é‡\n",
    "print(tensor4)\n",
    "print(tf.rank(tensor4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[[[1. 1.]\n",
    "   [2. 2.]]\n",
    "\n",
    "  [[3. 3.]\n",
    "   [4. 4.]]]\n",
    "\n",
    "\n",
    " [[[5. 5.]\n",
    "   [6. 6.]]\n",
    "\n",
    "  [[7. 7.]\n",
    "   [8. 8.]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ç”¨tf.castæ”¹å˜å¼ é‡çš„æ•°æ®ç±»å‹ã€‚\n",
    "\n",
    "å¯ä»¥ç”¨numpyæ–¹æ³•å°†tensorflowä¸­çš„å¼ é‡è½¬åŒ–æˆnumpyä¸­çš„å¼ é‡ã€‚\n",
    "\n",
    "å¯ä»¥ç”¨shapeæ–¹æ³•æŸ¥çœ‹å¼ é‡çš„å°ºå¯¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.constant([123,456],dtype = tf.int32)\n",
    "f = tf.cast(h,tf.float32)\n",
    "print(h.dtype, f.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<dtype: 'int32'> <dtype: 'float32'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "print(y.numpy()) #è½¬æ¢æˆnp.array\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1. 2.]\n",
    " [3. 4.]]\n",
    "(2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = tf.constant(u\"ä½ å¥½ ä¸–ç•Œ\")\n",
    "print(u.numpy())  \n",
    "print(u.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd \\xe4\\xb8\\x96\\xe7\\x95\\x8c'\n",
    "ä½ å¥½ ä¸–ç•Œ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå˜é‡å¼ é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹ä¸­éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ä¸€èˆ¬è¢«è®¾ç½®æˆå˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸é‡å€¼ä¸å¯ä»¥æ”¹å˜ï¼Œå¸¸é‡çš„é‡æ–°èµ‹å€¼ç›¸å½“äºåˆ›é€ æ–°çš„å†…å­˜ç©ºé—´\n",
    "c = tf.constant([1.0,2.0])\n",
    "print(c)\n",
    "print(id(c))\n",
    "c = c + tf.constant([1.0,1.0])\n",
    "print(c)\n",
    "print(id(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
    "5276289568\n",
    "tf.Tensor([2. 3.], shape=(2,), dtype=float32)\n",
    "5276290240\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# å˜é‡çš„å€¼å¯ä»¥æ”¹å˜ï¼Œå¯ä»¥é€šè¿‡assign, assign_addç­‰æ–¹æ³•ç»™å˜é‡é‡æ–°èµ‹å€¼\n",
    "v = tf.Variable([1.0,2.0],name = \"v\")\n",
    "print(v)\n",
    "print(id(v))\n",
    "v.assign_add([1.0,1.0])\n",
    "print(v)\n",
    "print(id(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
    "5276259888\n",
    "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n",
    "5276259888\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2,ä¸‰ç§è®¡ç®—å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutograph.\n",
    "\n",
    "åœ¨TensorFlow1.0æ—¶ä»£ï¼Œé‡‡ç”¨çš„æ˜¯é™æ€è®¡ç®—å›¾ï¼Œéœ€è¦å…ˆä½¿ç”¨TensorFlowçš„å„ç§ç®—å­åˆ›å»ºè®¡ç®—å›¾ï¼Œç„¶åå†å¼€å¯ä¸€ä¸ªä¼šè¯Sessionï¼Œæ˜¾å¼æ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "è€Œåœ¨TensorFlow2.0æ—¶ä»£ï¼Œé‡‡ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾ï¼Œå³æ¯ä½¿ç”¨ä¸€ä¸ªç®—å­åï¼Œè¯¥ç®—å­ä¼šè¢«åŠ¨æ€åŠ å…¥åˆ°éšå«çš„é»˜è®¤è®¡ç®—å›¾ä¸­ç«‹å³æ‰§è¡Œå¾—åˆ°ç»“æœï¼Œè€Œæ— éœ€å¼€å¯Sessionã€‚\n",
    "\n",
    "ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾å³Eager Excutionçš„å¥½å¤„æ˜¯æ–¹ä¾¿è°ƒè¯•ç¨‹åºï¼Œå®ƒä¼šè®©TensorFlowä»£ç çš„è¡¨ç°å’ŒPythonåŸç”Ÿä»£ç çš„è¡¨ç°ä¸€æ ·ï¼Œå†™èµ·æ¥å°±åƒå†™numpyä¸€æ ·ï¼Œå„ç§æ—¥å¿—æ‰“å°ï¼Œæ§åˆ¶æµå…¨éƒ¨éƒ½æ˜¯å¯ä»¥ä½¿ç”¨çš„ã€‚\n",
    "\n",
    "ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾çš„ç¼ºç‚¹æ˜¯è¿è¡Œæ•ˆç‡ç›¸å¯¹ä¼šä½ä¸€äº›ã€‚å› ä¸ºä½¿ç”¨åŠ¨æ€å›¾ä¼šæœ‰è®¸å¤šæ¬¡Pythonè¿›ç¨‹å’ŒTensorFlowçš„C++è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ã€‚è€Œé™æ€è®¡ç®—å›¾æ„å»ºå®Œæˆä¹‹åå‡ ä¹å…¨éƒ¨åœ¨TensorFlowå†…æ ¸ä¸Šä½¿ç”¨C++ä»£ç æ‰§è¡Œï¼Œæ•ˆç‡æ›´é«˜ã€‚æ­¤å¤–é™æ€å›¾ä¼šå¯¹è®¡ç®—æ­¥éª¤è¿›è¡Œä¸€å®šçš„ä¼˜åŒ–ï¼Œå‰ªå»å’Œç»“æœæ— å…³çš„è®¡ç®—æ­¥éª¤ã€‚\n",
    "\n",
    "å¦‚æœéœ€è¦åœ¨TensorFlow2.0ä¸­ä½¿ç”¨é™æ€å›¾ï¼Œå¯ä»¥ä½¿ç”¨@tf.functionè£…é¥°å™¨å°†æ™®é€šPythonå‡½æ•°è½¬æ¢æˆå¯¹åº”çš„TensorFlowè®¡ç®—å›¾æ„å»ºä»£ç ã€‚è¿è¡Œè¯¥å‡½æ•°å°±ç›¸å½“äºåœ¨TensorFlow1.0ä¸­ç”¨Sessionæ‰§è¡Œä»£ç ã€‚ä½¿ç”¨tf.functionæ„å»ºé™æ€å›¾çš„æ–¹å¼å«åš Autograph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œè®¡ç®—å›¾ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¡ç®—å›¾ç”±èŠ‚ç‚¹(nodes)å’Œçº¿(edges)ç»„æˆã€‚\n",
    "\n",
    "èŠ‚ç‚¹è¡¨ç¤ºæ“ä½œç¬¦Operatorï¼Œæˆ–è€…ç§°ä¹‹ä¸ºç®—å­ï¼Œçº¿è¡¨ç¤ºè®¡ç®—é—´çš„ä¾èµ–ã€‚\n",
    "\n",
    "å®çº¿è¡¨ç¤ºæœ‰æ•°æ®ä¼ é€’ä¾èµ–ï¼Œä¼ é€’çš„æ•°æ®å³å¼ é‡ã€‚\n",
    "\n",
    "è™šçº¿é€šå¸¸å¯ä»¥è¡¨ç¤ºæ§åˆ¶ä¾èµ–ï¼Œå³æ‰§è¡Œå…ˆåé¡ºåºã€‚\n",
    "\n",
    "![](./data/strjoin_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œé™æ€è®¡ç®—å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨é™æ€è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 1.0é™æ€è®¡ç®—å›¾èŒƒä¾‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#å®šä¹‰è®¡ç®—å›¾\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #placeholderä¸ºå ä½ç¬¦ï¼Œæ‰§è¡Œä¼šè¯æ—¶å€™æŒ‡å®šå¡«å……å¯¹è±¡\n",
    "    x = tf.placeholder(name='x', shape=[], dtype=tf.string)  \n",
    "    y = tf.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.string_join([x,y],name = 'join',separator=' ')\n",
    "\n",
    "#æ‰§è¡Œè®¡ç®—å›¾\n",
    "with tf.Session(graph = g) as sess:\n",
    "    print(sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"}))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow2.0 æ€€æ—§ç‰ˆé™æ€è®¡ç®—å›¾**\n",
    "\n",
    "TensorFlow2.0ä¸ºäº†ç¡®ä¿å¯¹è€ç‰ˆæœ¬tensorflowé¡¹ç›®çš„å…¼å®¹æ€§ï¼Œåœ¨tf.compat.v1å­æ¨¡å—ä¸­ä¿ç•™äº†å¯¹TensorFlow1.0é‚£ç§é™æ€è®¡ç®—å›¾æ„å»ºé£æ ¼çš„æ”¯æŒã€‚\n",
    "\n",
    "å¯ç§°ä¹‹ä¸ºæ€€æ—§ç‰ˆé™æ€è®¡ç®—å›¾ï¼Œå·²ç»ä¸æ¨èä½¿ç”¨äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.compat.v1.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)\n",
    "    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.strings.join([x,y],name = \"join\",separator = \" \")\n",
    "\n",
    "with tf.compat.v1.Session(graph = g) as sess:\n",
    "    # fetchesçš„ç»“æœéå¸¸åƒä¸€ä¸ªå‡½æ•°çš„è¿”å›å€¼ï¼Œè€Œfeed_dictä¸­çš„å ä½ç¬¦ç›¸å½“äºå‡½æ•°çš„å‚æ•°åºåˆ—ã€‚\n",
    "    result = sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "b'hello world'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼ŒåŠ¨æ€è®¡ç®—å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨TensorFlow2.0ä¸­ï¼Œä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutograph.\n",
    "\n",
    "åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨é™æ€è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "åŠ¨æ€è®¡ç®—å›¾å·²ç»ä¸åŒºåˆ†è®¡ç®—å›¾çš„å®šä¹‰å’Œæ‰§è¡Œäº†ï¼Œè€Œæ˜¯å®šä¹‰åç«‹å³æ‰§è¡Œã€‚å› æ­¤ç§°ä¹‹ä¸º Eager Excution.\n",
    "\n",
    "Eagerè¿™ä¸ªè‹±æ–‡å•è¯çš„åŸæ„æ˜¯\"è¿«ä¸åŠå¾…çš„\"ï¼Œä¹Ÿå°±æ˜¯ç«‹å³æ‰§è¡Œçš„æ„æ€ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ¨æ€è®¡ç®—å›¾åœ¨æ¯ä¸ªç®—å­å¤„éƒ½è¿›è¡Œæ„å»ºï¼Œæ„å»ºåç«‹å³æ‰§è¡Œ\n",
    "\n",
    "x = tf.constant(\"hello\")\n",
    "y = tf.constant(\"world\")\n",
    "z = tf.strings.join([x,y],separator=\" \")\n",
    "\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯ä»¥å°†åŠ¨æ€è®¡ç®—å›¾ä»£ç çš„è¾“å…¥å’Œè¾“å‡ºå…³ç³»å°è£…æˆå‡½æ•°\n",
    "\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼ŒTensorFlow2.0çš„Autograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ¨æ€è®¡ç®—å›¾è¿è¡Œæ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚\n",
    "\n",
    "å¯ä»¥ç”¨@tf.functionè£…é¥°å™¨å°†æ™®é€šPythonå‡½æ•°è½¬æ¢æˆå’ŒTensorFlow1.0å¯¹åº”çš„é™æ€è®¡ç®—å›¾æ„å»ºä»£ç ã€‚\n",
    "\n",
    "åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "åœ¨TensorFlow2.0ä¸­ï¼Œå¦‚æœé‡‡ç”¨Autographçš„æ–¹å¼ä½¿ç”¨è®¡ç®—å›¾ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾å˜æˆäº†å®šä¹‰å‡½æ•°ï¼Œç¬¬äºŒæ­¥æ‰§è¡Œè®¡ç®—å›¾å˜æˆäº†è°ƒç”¨å‡½æ•°ã€‚\n",
    "\n",
    "ä¸éœ€è¦ä½¿ç”¨ä¼šè¯äº†ï¼Œä¸€äº›éƒ½åƒåŸå§‹çš„Pythonè¯­æ³•ä¸€æ ·è‡ªç„¶ã€‚\n",
    "\n",
    "å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šå…ˆç”¨åŠ¨æ€è®¡ç®—å›¾è°ƒè¯•ä»£ç ï¼Œç„¶ååœ¨éœ€è¦æé«˜æ€§èƒ½çš„çš„åœ°æ–¹åˆ©ç”¨@tf.functionåˆ‡æ¢æˆAutographè·å¾—æ›´é«˜çš„æ•ˆç‡ã€‚\n",
    "\n",
    "å½“ç„¶ï¼Œ@tf.functionçš„ä½¿ç”¨éœ€è¦éµå¾ªä¸€å®šçš„è§„èŒƒï¼Œæˆ‘ä»¬åé¢ç« èŠ‚å°†é‡ç‚¹ä»‹ç»ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ä½¿ç”¨autographæ„å»ºé™æ€å›¾\n",
    "\n",
    "@tf.function\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# åˆ›å»ºæ—¥å¿—\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/autograph/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "#å¼€å¯autographè·Ÿè¸ª\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "#æ‰§è¡Œautograph\n",
    "result = strjoin(\"hello\",\"world\")\n",
    "\n",
    "#å°†è®¡ç®—å›¾ä¿¡æ¯å†™å…¥æ—¥å¿—\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"autograph\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯åŠ¨ tensorboardåœ¨jupyterä¸­çš„é­”æ³•å‘½ä»¤\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯åŠ¨tensorboard\n",
    "%tensorboard --logdir ./data/autograph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/2-2-tensorboardè®¡ç®—å›¾.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¥ç»ç½‘ç»œé€šå¸¸ä¾èµ–åå‘ä¼ æ’­æ±‚æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œæ±‚æ¢¯åº¦è¿‡ç¨‹é€šå¸¸æ˜¯ä¸€ä»¶éå¸¸å¤æ‚è€Œå®¹æ˜“å‡ºé”™çš„äº‹æƒ…ã€‚\n",
    "\n",
    "è€Œæ·±åº¦å­¦ä¹ æ¡†æ¶å¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨åœ°å®Œæˆè¿™ç§æ±‚æ¢¯åº¦è¿ç®—ã€‚\n",
    "\n",
    "Tensorflowä¸€èˆ¬ä½¿ç”¨æ¢¯åº¦ç£å¸¦tf.GradientTapeæ¥è®°å½•æ­£å‘è¿ç®—è¿‡ç¨‹ï¼Œç„¶ååæ’­ç£å¸¦è‡ªåŠ¨å¾—åˆ°æ¢¯åº¦å€¼ã€‚\n",
    "\n",
    "è¿™ç§åˆ©ç”¨tf.GradientTapeæ±‚å¾®åˆ†çš„æ–¹æ³•å«åšTensorflowçš„è‡ªåŠ¨å¾®åˆ†æœºåˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œåˆ©ç”¨æ¢¯åº¦ç£å¸¦æ±‚å¯¼æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "# f(x) = a*x**2 + b*x + cçš„å¯¼æ•°\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx = tape.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(-2.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹å¸¸é‡å¼ é‡ä¹Ÿå¯ä»¥æ±‚å¯¼ï¼Œéœ€è¦å¢åŠ watch\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a,b,c])\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])\n",
    "print(dy_da)\n",
    "print(dy_dc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0.0, shape=(), dtype=float32)\n",
    "tf.Tensor(1.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯ä»¥æ±‚äºŒé˜¶å¯¼æ•°\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:   \n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape1.gradient(y,x)   \n",
    "dy2_dx2 = tape2.gradient(dy_dx,x)\n",
    "\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(2.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯ä»¥åœ¨autographä¸­ä½¿ç”¨\n",
    "\n",
    "@tf.function\n",
    "def f(x):   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    # è‡ªå˜é‡è½¬æ¢æˆtf.float32\n",
    "    x = tf.cast(x,tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = a*tf.pow(x,2)+b*x+c\n",
    "    dy_dx = tape.gradient(y,x) \n",
    "    \n",
    "    return((dy_dx,y))\n",
    "\n",
    "tf.print(f(tf.constant(0.0)))\n",
    "tf.print(f(tf.constant(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(-2, 1)\n",
    "(0, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œåˆ©ç”¨æ¢¯åº¦ç£å¸¦å’Œä¼˜åŒ–å™¨æ±‚æœ€å°å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼\n",
    "# ä½¿ç”¨optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape.gradient(y,x)\n",
    "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "    \n",
    "tf.print(\"y =\",y,\"; x =\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = 0 ; x = 0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼\n",
    "# ä½¿ç”¨optimizer.minimize\n",
    "# optimizer.minimizeç›¸å½“äºå…ˆç”¨tapeæ±‚gradient,å†apply_gradient\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "\n",
    "#æ³¨æ„f()æ— å‚æ•°\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "for _ in range(1000):\n",
    "    optimizer.minimize(f,[x])   \n",
    "    \n",
    "tf.print(\"y =\",f(),\"; x =\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = 0 ; x = 0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨autographä¸­å®Œæˆæœ€å°å€¼æ±‚è§£\n",
    "# ä½¿ç”¨optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    for _ in tf.range(1000): #æ³¨æ„autographæ—¶ä½¿ç”¨tf.range(1000)è€Œä¸æ˜¯range(1000)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(minimizef())\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨autographä¸­å®Œæˆæœ€å°å€¼æ±‚è§£\n",
    "# ä½¿ç”¨optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "@tf.function\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    return(f())\n",
    "\n",
    "\n",
    "tf.print(train(1000))\n",
    "tf.print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç« æˆ‘ä»¬ä»‹ç»TensorFlowä¸­5ä¸ªä¸åŒçš„å±‚æ¬¡ç»“æ„ï¼šå³ç¡¬ä»¶å±‚ï¼Œå†…æ ¸å±‚ï¼Œä½é˜¶APIï¼Œä¸­é˜¶APIï¼Œé«˜é˜¶APIã€‚å¹¶ä»¥çº¿æ€§å›å½’ä¸ºä¾‹ï¼Œç›´è§‚å¯¹æ¯”å±•ç¤ºåœ¨ä¸åŒå±‚çº§å®ç°æ¨¡å‹çš„ç‰¹ç‚¹ã€‚\n",
    "\n",
    "TensorFlowçš„å±‚æ¬¡ç»“æ„ä»ä½åˆ°é«˜å¯ä»¥åˆ†æˆå¦‚ä¸‹äº”å±‚ã€‚\n",
    "\n",
    "æœ€åº•å±‚ä¸ºç¡¬ä»¶å±‚ï¼ŒTensorFlowæ”¯æŒCPUã€GPUæˆ–TPUåŠ å…¥è®¡ç®—èµ„æºæ± ã€‚\n",
    "\n",
    "ç¬¬äºŒå±‚ä¸ºC++å®ç°çš„å†…æ ¸ï¼Œkernelå¯ä»¥è·¨å¹³å°åˆ†å¸ƒè¿è¡Œã€‚\n",
    "\n",
    "ç¬¬ä¸‰å±‚ä¸ºPythonå®ç°çš„æ“ä½œç¬¦ï¼Œæä¾›äº†å°è£…C++å†…æ ¸çš„ä½çº§APIæŒ‡ä»¤ï¼Œä¸»è¦åŒ…æ‹¬å„ç§å¼ é‡æ“ä½œç®—å­ã€è®¡ç®—å›¾ã€è‡ªåŠ¨å¾®åˆ†.\n",
    "å¦‚tf.Variable,tf.constant,tf.function,tf.GradientTape,tf.nn.softmax...\n",
    "å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬ä¸‰å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚\n",
    "\n",
    "ç¬¬å››å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹ç»„ä»¶ï¼Œå¯¹ä½çº§APIè¿›è¡Œäº†å‡½æ•°å°è£…ï¼Œä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ï¼Œç‰¹å¾åˆ—ç­‰ç­‰ã€‚\n",
    "å¦‚tf.keras.layers,tf.keras.losses,tf.keras.metrics,tf.keras.optimizers,tf.data.DataSet,tf.feature_column...\n",
    "å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬å››å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚\n",
    "\n",
    "ç¬¬äº”å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹æˆå“ï¼Œä¸€èˆ¬ä¸ºæŒ‰ç…§OOPæ–¹å¼å°è£…çš„é«˜çº§APIï¼Œä¸»è¦ä¸ºtf.keras.modelsæä¾›çš„æ¨¡å‹çš„ç±»æ¥å£ã€‚\n",
    "å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬äº”å±‚APIå°±æ˜¯æ¨¡å‹æœ¬èº«ï¼Œå³ã€æ¨¡å‹ä¹‹å±‹ã€‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/tensorflow_structure.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1,ä½é˜¶APIç¤ºèŒƒ\n",
    "\n",
    "ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„ä½é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚\n",
    "\n",
    "ä½é˜¶APIä¸»è¦åŒ…æ‹¬å¼ é‡æ“ä½œï¼Œè®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ ·æœ¬æ•°é‡\n",
    "n = 400\n",
    "\n",
    "# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨åŠ¨æ€å›¾è°ƒè¯•\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #æ­£å‘ä¼ æ’­æ±‚æŸå¤±\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "            \n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-1-è¾“å‡º01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ä½¿ç”¨autographæœºåˆ¶è½¬æ¢æˆé™æ€å›¾åŠ é€Ÿ\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "@tf.function\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #æ­£å‘ä¼ æ’­æ±‚æŸå¤±\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-1-è¾“å‡º02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-2,ä¸­é˜¶APIç¤ºèŒƒ\n",
    "\n",
    "ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„ä¸­é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚\n",
    "\n",
    "TensorFlowçš„ä¸­é˜¶APIä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ï¼Œç‰¹å¾åˆ—ç­‰ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,losses,metrics,optimizers\n",
    "\n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ ·æœ¬æ•°é‡\n",
    "n = 800\n",
    "\n",
    "# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨\n",
    "\n",
    "#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“\n",
    "ds = tf.data.Dataset.from_tensor_slices((X,Y)) \\\n",
    "     .shuffle(buffer_size = 1000).batch(100) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "\n",
    "#å®šä¹‰ä¼˜åŒ–å™¨\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = layers.Dense(units = 1)\n",
    "linear.build(input_shape = (2,)) \n",
    "\n",
    "@tf.function\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        L = tf.constant(0.0) #ä½¿ç”¨Lè®°å½•losså€¼\n",
    "        for X_batch,Y_batch in ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                Y_hat = linear(X_batch)\n",
    "                loss = losses.mean_squared_error(tf.reshape(Y_hat,[-1]),tf.reshape(Y_batch,[-1]))\n",
    "            grads = tape.gradient(loss,linear.variables)\n",
    "            optimizer.apply_gradients(zip(grads,linear.variables))\n",
    "            L = loss\n",
    "        \n",
    "        if(epoch%100==0):\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\"loss =\",L)\n",
    "            tf.print(\"w =\",linear.kernel)\n",
    "            tf.print(\"b =\",linear.bias)\n",
    "            tf.print(\"\")\n",
    "\n",
    "train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-2-è¾“å‡º01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-3,é«˜é˜¶APIç¤ºèŒƒ\n",
    "\n",
    "ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„é«˜é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚\n",
    "\n",
    "TensorFlowçš„é«˜é˜¶APIä¸»è¦ä¸ºtf.keras.modelsæä¾›çš„æ¨¡å‹çš„ç±»æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "æ­¤å¤„åˆ†åˆ«æ¼”ç¤ºä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ä»¥åŠç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ã€é¢å‘æ–°æ‰‹ã€‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "#æ ·æœ¬æ•°é‡\n",
    "n = 800\n",
    "\n",
    "# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "linear = models.Sequential()\n",
    "linear.add(layers.Dense(1,input_shape =(2,)))\n",
    "linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-åºåˆ—ç»“æ„.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "linear.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 20,epochs = 200)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[0].kernel)\n",
    "tf.print(\"b = \",linear.layers[0].bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-å†…ç½®è®­ç»ƒ.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€é¢å‘ä¸“å®¶ã€‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers,losses,metrics\n",
    "\n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ ·æœ¬æ•°é‡\n",
    "n = 800\n",
    "\n",
    "# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X[0:n*3//4,:],Y[0:n*3//4,:])) \\\n",
    "     .shuffle(buffer_size = 1000).batch(20) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
    "     .cache()\n",
    "\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X[n*3//4:,:],Y[n*3//4:,:])) \\\n",
    "     .shuffle(buffer_size = 1000).batch(20) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
    "     .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class MyModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.dense1 = layers.Dense(1)   \n",
    "        super(MyModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        y = self.dense1(x)\n",
    "        return(y)\n",
    "\n",
    "model = MyModel()\n",
    "model.build(input_shape =(None,2))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-æ¨¡å‹ç»“æ„.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯(ä¸“å®¶æ•™ç¨‹)\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam()\n",
    "loss_func = losses.MeanSquaredError()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_metric = tf.keras.metrics.MeanAbsoluteError(name='train_mae')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_metric = tf.keras.metrics.MeanAbsoluteError(name='valid_mae')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},MAE:{},Valid Loss:{},Valid MAE:{}'\n",
    "        \n",
    "        if  epoch%100 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"w=\",model.layers[0].kernel)\n",
    "            tf.print(\"b=\",model.layers[0].bias)\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_valid,400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-è‡ªå®šä¹‰è®­ç»ƒ.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å››ã€TensorFlowçš„ä½é˜¶API\n",
    "\n",
    "TensorFlowçš„ä½é˜¶APIä¸»è¦åŒ…æ‹¬å¼ é‡æ“ä½œï¼Œè®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†ã€‚\n",
    "\n",
    "å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä½é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚\n",
    "\n",
    "åœ¨ä½é˜¶APIå±‚æ¬¡ä¸Šï¼Œå¯ä»¥æŠŠTensorFlowå½“åšä¸€ä¸ªå¢å¼ºç‰ˆçš„numpyæ¥ä½¿ç”¨ã€‚\n",
    "\n",
    "TensorFlowæä¾›çš„æ–¹æ³•æ¯”numpyæ›´å…¨é¢ï¼Œè¿ç®—é€Ÿåº¦æ›´å¿«ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼Œè¿˜å¯ä»¥ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿã€‚\n",
    "\n",
    "å‰é¢å‡ ç« æˆ‘ä»¬å¯¹ä½é˜¶APIå·²ç»æœ‰äº†ä¸€ä¸ªæ•´ä½“çš„è®¤è¯†ï¼Œæœ¬ç« æˆ‘ä»¬å°†é‡ç‚¹è¯¦ç»†ä»‹ç»å¼ é‡æ“ä½œå’ŒAutographè®¡ç®—å›¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚\n",
    "\n",
    "å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚\n",
    "\n",
    "å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚\n",
    "\n",
    "Autographè®¡ç®—å›¾æˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨Autographçš„è§„èŒƒå»ºè®®ï¼ŒAutographçš„æœºåˆ¶åŸç†ï¼ŒAutographå’Œtf.Module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-1,å¼ é‡çš„ç»“æ„æ“ä½œ\n",
    "\n",
    "å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚\n",
    "\n",
    "å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚\n",
    "\n",
    "å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚\n",
    "\n",
    "æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„ç»“æ„æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œåˆ›å»ºå¼ é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼ é‡åˆ›å»ºçš„è®¸å¤šæ–¹æ³•å’Œnumpyä¸­åˆ›å»ºarrayçš„æ–¹æ³•å¾ˆåƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3],dtype = tf.float32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.range(1,10,delta = 2)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 3 5 7 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.linspace(0.0,2*3.14,100)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.zeros([3,3])\n",
    "tf.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[0 0 0]\n",
    " [0 0 0]\n",
    " [0 0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones([3,3])\n",
    "b = tf.zeros_like(a,dtype= tf.float32)\n",
    "tf.print(a)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 1 1]\n",
    " [1 1 1]\n",
    " [1 1 1]]\n",
    "[[0 0 0]\n",
    " [0 0 0]\n",
    " [0 0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.fill([3,2],5)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[5 5]\n",
    " [5 5]\n",
    " [5 5]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å‡åŒ€åˆ†å¸ƒéšæœº\n",
    "tf.random.set_seed(1.0)\n",
    "a = tf.random.uniform([5],minval=0,maxval=10)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­£æ€åˆ†å¸ƒéšæœº\n",
    "b = tf.random.normal([3,3],mean=0.0,stddev=1.0)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[0.403087884 -1.0880208 -0.0630953535]\n",
    " [1.33655667 0.711760104 -0.489286453]\n",
    " [-0.764221311 -1.03724861 -1.25193381]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­£æ€åˆ†å¸ƒéšæœºï¼Œå‰”é™¤2å€æ–¹å·®ä»¥å¤–æ•°æ®é‡æ–°ç”Ÿæˆ\n",
    "c = tf.random.truncated_normal((5,5), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-0.457012236 -0.406867266 0.728577733 -0.892977774 -0.369404584]\n",
    " [0.323488563 1.19383323 0.888299048 1.25985599 -1.95951891]\n",
    " [-0.202244401 0.294496894 -0.468728036 1.29494202 1.48142183]\n",
    " [0.0810953453 1.63843894 0.556645 0.977199793 -1.17777884]\n",
    " [1.67368948 0.0647980496 -0.705142677 -0.281972528 0.126546144]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹æ®ŠçŸ©é˜µ\n",
    "I = tf.eye(3,3) #å•ä½çŸ©é˜µ\n",
    "tf.print(I)\n",
    "tf.print(\" \")\n",
    "t = tf.linalg.diag([1,2,3]) #å¯¹è§’é˜µ\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 0 0]\n",
    " [0 1 0]\n",
    " [0 0 1]]\n",
    " \n",
    "[[1 0 0]\n",
    " [0 2 0]\n",
    " [0 0 3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒ ï¼Œç´¢å¼•åˆ‡ç‰‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼ é‡çš„ç´¢å¼•åˆ‡ç‰‡æ–¹å¼å’Œnumpyå‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚åˆ‡ç‰‡æ—¶æ”¯æŒç¼ºçœå‚æ•°å’Œçœç•¥å·ã€‚\n",
    "\n",
    "å¯¹äºtf.Variable,å¯ä»¥é€šè¿‡ç´¢å¼•å’Œåˆ‡ç‰‡å¯¹éƒ¨åˆ†å…ƒç´ è¿›è¡Œä¿®æ”¹ã€‚\n",
    "\n",
    "å¯¹äºæå–å¼ é‡çš„è¿ç»­å­åŒºåŸŸï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨tf.slice.\n",
    "\n",
    "æ­¤å¤–ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨tf.gather,tf.gather_nd,tf.boolean_maskã€‚\n",
    "\n",
    "tf.boolean_maskåŠŸèƒ½æœ€ä¸ºå¼ºå¤§ï¼Œå®ƒå¯ä»¥å®ç°tf.gather,tf.gather_ndçš„åŠŸèƒ½ï¼Œå¹¶ä¸”tf.boolean_maskè¿˜å¯ä»¥å®ç°å¸ƒå°”ç´¢å¼•ã€‚\n",
    "\n",
    "å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„æŸäº›å…ƒç´ å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨tf.whereï¼Œtf.scatter_ndã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(3)\n",
    "t = tf.random.uniform([5,5],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[4 7 4 2 9]\n",
    " [9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]\n",
    " [3 7 0 0 3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç¬¬0è¡Œ\n",
    "tf.print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[4 7 4 2 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å€’æ•°ç¬¬ä¸€è¡Œ\n",
    "tf.print(t[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[3 7 0 0 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç¬¬1è¡Œç¬¬3åˆ—\n",
    "tf.print(t[1,3])\n",
    "tf.print(t[1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "4\n",
    "4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç¬¬1è¡Œè‡³ç¬¬3è¡Œ\n",
    "tf.print(t[1:4,:])\n",
    "tf.print(tf.slice(t,[1,0],[3,5])) #tf.slice(input,begin_vector,size_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]]\n",
    "[[9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç¬¬1è¡Œè‡³æœ€åä¸€è¡Œï¼Œç¬¬0åˆ—åˆ°æœ€åä¸€åˆ—æ¯éš”ä¸¤åˆ—å–ä¸€åˆ—\n",
    "tf.print(t[1:4,:4:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[9 2]\n",
    " [7 7]\n",
    " [9 9]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¹å˜é‡æ¥è¯´ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ç´¢å¼•å’Œåˆ‡ç‰‡ä¿®æ”¹éƒ¨åˆ†å…ƒç´ \n",
    "x = tf.Variable([[1,2],[3,4]],dtype = tf.float32)\n",
    "x[1,:].assign(tf.constant([0.0,0.0]))\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 2]\n",
    " [0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([3,3,3],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[7 3 9]\n",
    "  [9 0 7]\n",
    "  [9 6 7]]\n",
    "\n",
    " [[1 3 3]\n",
    "  [0 8 1]\n",
    "  [3 1 0]]\n",
    "\n",
    " [[4 0 6]\n",
    "  [6 2 2]\n",
    "  [7 9 5]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çœç•¥å·å¯ä»¥è¡¨ç¤ºå¤šä¸ªå†’å·\n",
    "tf.print(a[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[3 0 6]\n",
    " [3 8 1]\n",
    " [0 2 9]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šåˆ‡ç‰‡æ–¹å¼ç›¸å¯¹è§„åˆ™ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨tf.gather,tf.gather_nd,tf.boolean_maskã€‚\n",
    "\n",
    "è€ƒè™‘ç­çº§æˆç»©å†Œçš„ä¾‹å­ï¼Œæœ‰4ä¸ªç­çº§ï¼Œæ¯ä¸ªç­çº§10ä¸ªå­¦ç”Ÿï¼Œæ¯ä¸ªå­¦ç”Ÿ7é—¨ç§‘ç›®æˆç»©ã€‚å¯ä»¥ç”¨ä¸€ä¸ª4*10*7çš„å¼ é‡æ¥è¡¨ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.random.uniform((4,10,7),minval=0,maxval=100,dtype=tf.int32)\n",
    "tf.print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [8 36 94 ... 13 78 41]\n",
    "  [77 53 51 ... 22 91 56]\n",
    "  ...\n",
    "  [11 19 26 ... 89 86 68]\n",
    "  [60 72 0 ... 11 26 15]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [83 36 31 ... 75 38 85]\n",
    "  [54 26 67 ... 60 68 98]\n",
    "  ...\n",
    "  [20 5 18 ... 32 45 3]\n",
    "  [72 52 81 ... 88 41 20]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [78 71 54 ... 43 98 81]\n",
    "  [21 66 53 ... 97 75 77]\n",
    "  ...\n",
    "  [6 74 3 ... 53 65 43]\n",
    "  [98 36 72 ... 33 36 81]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [35 8 82 ... 11 59 97]\n",
    "  [44 6 99 ... 81 60 27]\n",
    "  ...\n",
    "  [76 26 35 ... 51 8 17]\n",
    "  [33 52 53 ... 78 37 31]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©\n",
    "p = tf.gather(scores,[0,5,9],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [24 80 70 ... 72 63 96]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [46 10 94 ... 23 18 92]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [19 12 23 ... 87 86 25]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [6 41 79 ... 97 43 13]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„ç¬¬1é—¨è¯¾ç¨‹ï¼Œç¬¬3é—¨è¯¾ç¨‹ï¼Œç¬¬6é—¨è¯¾ç¨‹æˆç»©\n",
    "q = tf.gather(tf.gather(scores,[0,5,9],axis=1),[1,3,6],axis=2)\n",
    "tf.print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[82 55 14]\n",
    "  [80 46 96]\n",
    "  [99 58 74]]\n",
    "\n",
    " [[73 48 81]\n",
    "  [10 38 92]\n",
    "  [21 86 90]]\n",
    "\n",
    " [[80 57 60]\n",
    "  [12 34 25]\n",
    "  [78 71 21]]\n",
    "\n",
    " [[57 75 3]\n",
    "  [41 47 13]\n",
    "  [27 96 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠ½å–ç¬¬0ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬2ä¸ªç­çº§çš„ç¬¬4ä¸ªå­¦ç”Ÿï¼Œç¬¬3ä¸ªç­çº§çš„ç¬¬6ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©\n",
    "#indicesçš„é•¿åº¦ä¸ºé‡‡æ ·æ ·æœ¬çš„ä¸ªæ•°ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºé‡‡æ ·ä½ç½®çš„åæ ‡\n",
    "s = tf.gather_nd(scores,indices = [(0,0),(2,4),(3,6)])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
    "array([[52, 82, 66, 55, 17, 86, 14],\n",
    "       [99, 94, 46, 70,  1, 63, 41],\n",
    "       [46, 83, 70, 80, 90, 85, 17]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Štf.gatherå’Œtf.gather_ndçš„åŠŸèƒ½ä¹Ÿå¯ä»¥ç”¨tf.boolean_maskæ¥å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©\n",
    "p = tf.boolean_mask(scores,[True,False,False,False,False,\n",
    "                            True,False,False,False,True],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [24 80 70 ... 72 63 96]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [46 10 94 ... 23 18 92]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [19 12 23 ... 87 86 25]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [6 41 79 ... 97 43 13]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŠ½å–ç¬¬0ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬2ä¸ªç­çº§çš„ç¬¬4ä¸ªå­¦ç”Ÿï¼Œç¬¬3ä¸ªç­çº§çš„ç¬¬6ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©\n",
    "s = tf.boolean_mask(scores,\n",
    "    [[True,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,True,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,True,False,False,False]])\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[52 82 66 ... 17 86 14]\n",
    " [99 94 46 ... 1 63 41]\n",
    " [46 83 70 ... 90 85 17]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ©ç”¨tf.boolean_maskå¯ä»¥å®ç°å¸ƒå°”ç´¢å¼•\n",
    "\n",
    "#æ‰¾åˆ°çŸ©é˜µä¸­å°äº0çš„å…ƒç´ \n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "tf.print(c,\"\\n\")\n",
    "\n",
    "tf.print(tf.boolean_mask(c,c<0),\"\\n\") \n",
    "tf.print(c[c<0]) #å¸ƒå°”ç´¢å¼•ï¼Œä¸ºboolean_maskçš„è¯­æ³•ç³–å½¢å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-1 1 -1]\n",
    " [2 2 -2]\n",
    " [3 -3 3]] \n",
    "\n",
    "[-1 -1 -2 -3] \n",
    "\n",
    "[-1 -1 -2 -3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šè¿™äº›æ–¹æ³•ä»…èƒ½æå–å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼ï¼Œä½†ä¸èƒ½æ›´æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ã€‚\n",
    "\n",
    "å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨tf.whereå’Œtf.scatter_ndã€‚\n",
    "\n",
    "tf.whereå¯ä»¥ç†è§£ä¸ºifçš„å¼ é‡ç‰ˆæœ¬ï¼Œæ­¤å¤–å®ƒè¿˜å¯ä»¥ç”¨äºæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ‰€æœ‰å…ƒç´ çš„ä½ç½®åæ ‡ã€‚\n",
    "\n",
    "tf.scatter_ndçš„ä½œç”¨å’Œtf.gather_ndæœ‰äº›ç›¸åï¼Œtf.gather_ndç”¨äºæ”¶é›†å¼ é‡çš„ç»™å®šä½ç½®çš„å…ƒç´ ï¼Œ\n",
    "\n",
    "è€Œtf.scatter_ndå¯ä»¥å°†æŸäº›å€¼æ’å…¥åˆ°ä¸€ä¸ªç»™å®šshapeçš„å…¨0çš„å¼ é‡çš„æŒ‡å®šä½ç½®å¤„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰¾åˆ°å¼ é‡ä¸­å°äº0çš„å…ƒç´ ,å°†å…¶æ¢æˆnp.nanå¾—åˆ°æ–°çš„å¼ é‡\n",
    "#tf.whereå’Œnp.whereä½œç”¨ç±»ä¼¼ï¼Œå¯ä»¥ç†è§£ä¸ºifçš„å¼ é‡ç‰ˆæœ¬\n",
    "\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "d = tf.where(c<0,tf.fill(c.shape,np.nan),c) \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[nan,  1., nan],\n",
    "       [ 2.,  2., nan],\n",
    "       [ 3., nan,  3.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¦‚æœwhereåªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œå°†è¿”å›æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„ä½ç½®åæ ‡\n",
    "indices = tf.where(c<0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
    "array([[0, 0],\n",
    "       [0, 2],\n",
    "       [1, 2],\n",
    "       [2, 1]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†å¼ é‡çš„ç¬¬[0,0]å’Œ[2,1]ä¸¤ä¸ªä½ç½®å…ƒç´ æ›¿æ¢ä¸º0å¾—åˆ°æ–°çš„å¼ é‡\n",
    "d = c - tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[ 0.,  1., -1.],\n",
    "       [ 2.,  2., -2.],\n",
    "       [ 3.,  0.,  3.]], dtype=float32)>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter_ndçš„ä½œç”¨å’Œgather_ndæœ‰äº›ç›¸å\n",
    "#å¯ä»¥å°†æŸäº›å€¼æ’å…¥åˆ°ä¸€ä¸ªç»™å®šshapeçš„å…¨0çš„å¼ é‡çš„æŒ‡å®šä½ç½®å¤„ã€‚\n",
    "indices = tf.where(c<0)\n",
    "tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[-1.,  0., -1.],\n",
    "       [ 0.,  0., -2.],\n",
    "       [ 0., -3.,  0.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œç»´åº¦å˜æ¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»´åº¦å˜æ¢ç›¸å…³å‡½æ•°ä¸»è¦æœ‰ tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.\n",
    "\n",
    "tf.reshape å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ã€‚\n",
    "\n",
    "tf.squeeze å¯ä»¥å‡å°‘ç»´åº¦ã€‚\n",
    "\n",
    "tf.expand_dims å¯ä»¥å¢åŠ ç»´åº¦ã€‚\n",
    "\n",
    "tf.transpose å¯ä»¥äº¤æ¢ç»´åº¦ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reshapeå¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä½†æ˜¯å…¶æœ¬è´¨ä¸Šä¸ä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºï¼Œæ‰€ä»¥ï¼Œè¯¥æ“ä½œå®é™…ä¸Šéå¸¸è¿…é€Ÿï¼Œå¹¶ä¸”æ˜¯å¯é€†çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=[1,3,3,2],\n",
    "                      minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([1, 3, 3, 2])\n",
    "[[[[135 178]\n",
    "   [26 116]\n",
    "   [29 224]]\n",
    "\n",
    "  [[179 219]\n",
    "   [153 209]\n",
    "   [111 215]]\n",
    "\n",
    "  [[39 7]\n",
    "   [138 129]\n",
    "   [59 205]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¹æˆ ï¼ˆ3,6ï¼‰å½¢çŠ¶çš„å¼ é‡\n",
    "b = tf.reshape(a,[3,6])\n",
    "tf.print(b.shape)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 6])\n",
    "[[135 178 26 116 29 224]\n",
    " [179 219 153 209 111 215]\n",
    " [39 7 138 129 59 205]]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¹å›æˆ [1,3,3,2] å½¢çŠ¶çš„å¼ é‡\n",
    "c = tf.reshape(b,[1,3,3,2])\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[[135 178]\n",
    "   [26 116]\n",
    "   [29 224]]\n",
    "\n",
    "  [[179 219]\n",
    "   [153 209]\n",
    "   [111 215]]\n",
    "\n",
    "  [[39 7]\n",
    "   [138 129]\n",
    "   [59 205]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ©ç”¨tf.squeezeå¯ä»¥æ¶ˆé™¤è¿™ä¸ªç»´åº¦ã€‚\n",
    "\n",
    "å’Œtf.reshapeç›¸ä¼¼ï¼Œå®ƒæœ¬è´¨ä¸Šä¸ä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºã€‚\n",
    "\n",
    "å¼ é‡çš„å„ä¸ªå…ƒç´ åœ¨å†…å­˜ä¸­æ˜¯çº¿æ€§å­˜å‚¨çš„ï¼Œå…¶ä¸€èˆ¬è§„å¾‹æ˜¯ï¼ŒåŒä¸€å±‚çº§ä¸­çš„ç›¸é‚»å…ƒç´ çš„ç‰©ç†åœ°å€ä¹Ÿç›¸é‚»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.squeeze(a)\n",
    "tf.print(s.shape)\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 3, 2])\n",
    "[[[135 178]\n",
    "  [26 116]\n",
    "  [29 224]]\n",
    "\n",
    " [[179 219]\n",
    "  [153 209]\n",
    "  [111 215]]\n",
    "\n",
    " [[39 7]\n",
    "  [138 129]\n",
    "  [59 205]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.expand_dims(s,axis=0) #åœ¨ç¬¬0ç»´æ’å…¥é•¿åº¦ä¸º1çš„ä¸€ä¸ªç»´åº¦\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(1, 3, 3, 2), dtype=int32, numpy=\n",
    "array([[[[135, 178],\n",
    "         [ 26, 116],\n",
    "         [ 29, 224]],\n",
    "\n",
    "        [[179, 219],\n",
    "         [153, 209],\n",
    "         [111, 215]],\n",
    "\n",
    "        [[ 39,   7],\n",
    "         [138, 129],\n",
    "         [ 59, 205]]]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.transposeå¯ä»¥äº¤æ¢å¼ é‡çš„ç»´åº¦ï¼Œä¸tf.reshapeä¸åŒï¼Œå®ƒä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºã€‚\n",
    "\n",
    "tf.transposeå¸¸ç”¨äºå›¾ç‰‡å­˜å‚¨æ ¼å¼çš„å˜æ¢ä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch,Height,Width,Channel\n",
    "a = tf.random.uniform(shape=[100,600,600,4],minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "\n",
    "# è½¬æ¢æˆ Channel,Height,Width,Batch\n",
    "s= tf.transpose(a,perm=[3,1,2,0])\n",
    "tf.print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([100, 600, 600, 4])\n",
    "TensorShape([4, 600, 600, 100])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œåˆå¹¶åˆ†å‰²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å’Œnumpyç±»ä¼¼ï¼Œå¯ä»¥ç”¨tf.concatå’Œtf.stackæ–¹æ³•å¯¹å¤šä¸ªå¼ é‡è¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥ç”¨tf.splitæ–¹æ³•æŠŠä¸€ä¸ªå¼ é‡åˆ†å‰²æˆå¤šä¸ªå¼ é‡ã€‚\n",
    "\n",
    "tf.concatå’Œtf.stackæœ‰ç•¥å¾®çš„åŒºåˆ«ï¼Œtf.concatæ˜¯è¿æ¥ï¼Œä¸ä¼šå¢åŠ ç»´åº¦ï¼Œè€Œtf.stackæ˜¯å †å ï¼Œä¼šå¢åŠ ç»´åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
    "array([[ 1.,  2.],\n",
    "       [ 3.,  4.],\n",
    "       [ 5.,  6.],\n",
    "       [ 7.,  8.],\n",
    "       [ 9., 10.],\n",
    "       [11., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat([a,b,c],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
    "array([[ 1.,  2.,  5.,  6.,  9., 10.],\n",
    "       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
    "array([[[ 1.,  2.],\n",
    "        [ 3.,  4.]],\n",
    "\n",
    "       [[ 5.,  6.],\n",
    "        [ 7.,  8.]],\n",
    "\n",
    "       [[ 9., 10.],\n",
    "        [11., 12.]]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([a,b,c],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
    "array([[[ 1.,  2.],\n",
    "        [ 5.,  6.],\n",
    "        [ 9., 10.]],\n",
    "\n",
    "       [[ 3.,  4.],\n",
    "        [ 7.,  8.],\n",
    "        [11., 12.]]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "c = tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.splitæ˜¯tf.concatçš„é€†è¿ç®—ï¼Œå¯ä»¥æŒ‡å®šåˆ†å‰²ä»½æ•°å¹³å‡åˆ†å‰²ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡è¿›è¡Œåˆ†å‰²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.split(value,num_or_size_splits,axis)\n",
    "tf.split(c,3,axis = 0)  #æŒ‡å®šåˆ†å‰²ä»½æ•°ï¼Œå¹³å‡åˆ†å‰²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[1., 2.],\n",
    "        [3., 4.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[5., 6.],\n",
    "        [7., 8.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[ 9., 10.],\n",
    "        [11., 12.]], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.split(c,[2,2,2],axis = 0) #æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[1., 2.],\n",
    "        [3., 4.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[5., 6.],\n",
    "        [7., 8.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[ 9., 10.],\n",
    "        [11., 12.]], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-2,å¼ é‡çš„æ•°å­¦è¿ç®—\n",
    "\n",
    "å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚\n",
    "\n",
    "å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚\n",
    "\n",
    "å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚\n",
    "\n",
    "æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„æ•°å­¦è¿ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œæ ‡é‡è¿ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼ é‡çš„æ•°å­¦è¿ç®—ç¬¦å¯ä»¥åˆ†ä¸ºæ ‡é‡è¿ç®—ç¬¦ã€å‘é‡è¿ç®—ç¬¦ã€ä»¥åŠçŸ©é˜µè¿ç®—ç¬¦ã€‚\n",
    "\n",
    "åŠ å‡ä¹˜é™¤ä¹˜æ–¹ï¼Œä»¥åŠä¸‰è§’å‡½æ•°ï¼ŒæŒ‡æ•°ï¼Œå¯¹æ•°ç­‰å¸¸è§å‡½æ•°ï¼Œé€»è¾‘æ¯”è¾ƒè¿ç®—ç¬¦ç­‰éƒ½æ˜¯æ ‡é‡è¿ç®—ç¬¦ã€‚\n",
    "\n",
    "æ ‡é‡è¿ç®—ç¬¦çš„ç‰¹ç‚¹æ˜¯å¯¹å¼ é‡å®æ–½é€å…ƒç´ è¿ç®—ã€‚\n",
    "\n",
    "æœ‰äº›æ ‡é‡è¿ç®—ç¬¦å¯¹å¸¸ç”¨çš„æ•°å­¦è¿ç®—ç¬¦è¿›è¡Œäº†é‡è½½ã€‚å¹¶ä¸”æ”¯æŒç±»ä¼¼numpyçš„å¹¿æ’­ç‰¹æ€§ã€‚\n",
    "\n",
    "è®¸å¤šæ ‡é‡è¿ç®—ç¬¦éƒ½åœ¨ tf.mathæ¨¡å—ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2],[-3,4.0]])\n",
    "b = tf.constant([[5.0,6],[7.0,8.0]])\n",
    "a+b  #è¿ç®—ç¬¦é‡è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 6.,  8.],\n",
    "       [ 4., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a-b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ -4.,  -4.],\n",
    "       [-10.,  -4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[  5.,  12.],\n",
    "       [-21.,  32.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.2       ,  0.33333334],\n",
    "       [-0.42857143,  0.5       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 1.,  4.],\n",
    "       [ 9., 16.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a%3 #modçš„è¿ç®—ç¬¦é‡è½½ï¼Œç­‰ä»·äºm = tf.math.mod(a,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 0], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a//3  #åœ°æ¿é™¤æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.,  0.],\n",
    "       [-1.,  1.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)&(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False, False]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)|(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[ True,  True],\n",
    "       [ True,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==5 #tf.equal(a,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False, False, False])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1.0,8.0])\n",
    "b = tf.constant([5.0,6.0])\n",
    "c = tf.constant([6.0,7.0])\n",
    "tf.add_n([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.maximum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[5 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.minimum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå‘é‡è¿ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‘é‡è¿ç®—ç¬¦åªåœ¨ä¸€ä¸ªç‰¹å®šè½´ä¸Šè¿ç®—ï¼Œå°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ ‡é‡æˆ–è€…å¦å¤–ä¸€ä¸ªå‘é‡ã€‚\n",
    "è®¸å¤šå‘é‡è¿ç®—ç¬¦éƒ½ä»¥reduceå¼€å¤´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å‘é‡reduce\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.reduce_sum(a))\n",
    "tf.print(tf.reduce_mean(a))\n",
    "tf.print(tf.reduce_max(a))\n",
    "tf.print(tf.reduce_min(a))\n",
    "tf.print(tf.reduce_prod(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "45\n",
    "5\n",
    "9\n",
    "1\n",
    "362880\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¼ é‡æŒ‡å®šç»´åº¦è¿›è¡Œreduce\n",
    "b = tf.reshape(a,(3,3))\n",
    "tf.print(tf.reduce_sum(b, axis=1, keepdims=True))\n",
    "tf.print(tf.reduce_sum(b, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[6]\n",
    " [15]\n",
    " [24]]\n",
    "[[12 15 18]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boolç±»å‹çš„reduce\n",
    "p = tf.constant([True,False,False])\n",
    "q = tf.constant([False,False,True])\n",
    "tf.print(tf.reduce_all(p))\n",
    "tf.print(tf.reduce_any(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ©ç”¨tf.foldrå®ç°tf.reduce_sum\n",
    "s = tf.foldr(lambda a,b:a+b,tf.range(10)) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "45\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumæ‰«æç´¯ç§¯\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.math.cumsum(a))\n",
    "tf.print(tf.math.cumprod(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 3 6 ... 28 36 45]\n",
    "[1 2 6 ... 5040 40320 362880]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argæœ€å¤§æœ€å°å€¼ç´¢å¼•\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.argmax(a))\n",
    "tf.print(tf.argmin(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "8\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.math.top_kå¯ä»¥ç”¨äºå¯¹å¼ é‡æ’åº\n",
    "a = tf.constant([1,3,7,5,4,8])\n",
    "\n",
    "values,indices = tf.math.top_k(a,3,sorted=True)\n",
    "tf.print(values)\n",
    "tf.print(indices)\n",
    "\n",
    "#åˆ©ç”¨tf.math.top_kå¯ä»¥åœ¨TensorFlowä¸­å®ç°KNNç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[8 7 5]\n",
    "[5 2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼ŒçŸ©é˜µè¿ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çŸ©é˜µå¿…é¡»æ˜¯äºŒç»´çš„ã€‚ç±»ä¼¼tf.constant([1,2,3])è¿™æ ·çš„ä¸æ˜¯çŸ©é˜µã€‚\n",
    "\n",
    "çŸ©é˜µè¿ç®—åŒ…æ‹¬ï¼šçŸ©é˜µä¹˜æ³•ï¼ŒçŸ©é˜µè½¬ç½®ï¼ŒçŸ©é˜µé€†ï¼ŒçŸ©é˜µæ±‚è¿¹ï¼ŒçŸ©é˜µèŒƒæ•°ï¼ŒçŸ©é˜µè¡Œåˆ—å¼ï¼ŒçŸ©é˜µæ±‚ç‰¹å¾å€¼ï¼ŒçŸ©é˜µåˆ†è§£ç­‰è¿ç®—ã€‚\n",
    "\n",
    "é™¤äº†ä¸€äº›å¸¸ç”¨çš„è¿ç®—å¤–ï¼Œå¤§éƒ¨åˆ†å’ŒçŸ©é˜µæœ‰å…³çš„è¿ç®—éƒ½åœ¨tf.linalgå­åŒ…ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µä¹˜æ³•\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[2,0],[0,2]])\n",
    "a@b  #ç­‰ä»·äºtf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[2, 4],\n",
    "       [6, 8]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µè½¬ç½®\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.transpose(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1., 3.],\n",
    "       [2., 4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µé€†ï¼Œå¿…é¡»ä¸ºtf.float32æˆ–tf.doubleç±»å‹\n",
    "a = tf.constant([[1.0,2],[3.0,4]],dtype = tf.float32)\n",
    "tf.linalg.inv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[-2.0000002 ,  1.0000001 ],\n",
    "       [ 1.5000001 , -0.50000006]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µæ±‚trace\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.trace(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µæ±‚èŒƒæ•°\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=5.477226>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µè¡Œåˆ—å¼\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.det(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µç‰¹å¾å€¼\n",
    "tf.linalg.eigvalsh(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µqråˆ†è§£\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "q,r = tf.linalg.qr(a)\n",
    "tf.print(q)\n",
    "tf.print(r)\n",
    "tf.print(q@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-0.316227794 -0.948683321]\n",
    " [-0.948683321 0.316227734]]\n",
    "[[-3.1622777 -4.4271884]\n",
    " [0 -0.632455349]]\n",
    "[[1.00000012 1.99999976]\n",
    " [3 4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#çŸ©é˜µsvdåˆ†è§£\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "v,s,d = tf.linalg.svd(a)\n",
    "tf.matmul(tf.matmul(s,tf.linalg.diag(v)),d)\n",
    "\n",
    "#åˆ©ç”¨svdåˆ†è§£å¯ä»¥åœ¨TensorFlowä¸­å®ç°ä¸»æˆåˆ†åˆ†æé™ç»´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[0.9999996, 1.9999996],\n",
    "       [2.9999998, 4.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œå¹¿æ’­æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowçš„å¹¿æ’­è§„åˆ™å’Œnumpyæ˜¯ä¸€æ ·çš„:\n",
    "\n",
    "* 1ã€å¦‚æœå¼ é‡çš„ç»´åº¦ä¸åŒï¼Œå°†ç»´åº¦è¾ƒå°çš„å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œç›´åˆ°ä¸¤ä¸ªå¼ é‡çš„ç»´åº¦éƒ½ä¸€æ ·ã€‚\n",
    "* 2ã€å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šçš„é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…å…¶ä¸­ä¸€ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šçš„é•¿åº¦ä¸º1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¯´è¿™ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šæ˜¯ç›¸å®¹çš„ã€‚\n",
    "* 3ã€å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ˜¯ç›¸å®¹çš„ï¼Œå®ƒä»¬å°±èƒ½ä½¿ç”¨å¹¿æ’­ã€‚\n",
    "* 4ã€å¹¿æ’­ä¹‹åï¼Œæ¯ä¸ªç»´åº¦çš„é•¿åº¦å°†å–ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦é•¿åº¦çš„è¾ƒå¤§å€¼ã€‚\n",
    "* 5ã€åœ¨ä»»ä½•ä¸€ä¸ªç»´åº¦ä¸Šï¼Œå¦‚æœä¸€ä¸ªå¼ é‡çš„é•¿åº¦ä¸º1ï¼Œå¦ä¸€ä¸ªå¼ é‡é•¿åº¦å¤§äº1ï¼Œé‚£ä¹ˆåœ¨è¯¥ç»´åº¦ä¸Šï¼Œå°±å¥½åƒæ˜¯å¯¹ç¬¬ä¸€ä¸ªå¼ é‡è¿›è¡Œäº†å¤åˆ¶ã€‚\n",
    "\n",
    "tf.broadcast_to ä»¥æ˜¾å¼çš„æ–¹å¼æŒ‰ç…§å¹¿æ’­æœºåˆ¶æ‰©å±•å¼ é‡çš„ç»´åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])\n",
    "b + a  #ç­‰ä»·äº b + tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [2, 3, 4],\n",
    "       [3, 4, 5]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [1, 2, 3],\n",
    "       [1, 2, 3]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®¡ç®—å¹¿æ’­åè®¡ç®—ç»“æœçš„å½¢çŠ¶ï¼Œé™æ€å½¢çŠ¶ï¼ŒTensorShapeç±»å‹å‚æ•°\n",
    "tf.broadcast_static_shape(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®¡ç®—å¹¿æ’­åè®¡ç®—ç»“æœçš„å½¢çŠ¶ï¼ŒåŠ¨æ€å½¢çŠ¶ï¼ŒTensorç±»å‹å‚æ•°\n",
    "c = tf.constant([1,2,3])\n",
    "d = tf.constant([[1],[2],[3]])\n",
    "tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¹¿æ’­æ•ˆæœ\n",
    "c+d #ç­‰ä»·äº tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[6.5760484, 7.8174157],\n",
    "       [6.8174157, 6.4239516]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ\n",
    "\n",
    "æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚\n",
    "\n",
    "TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚\n",
    "\n",
    "åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚\n",
    "\n",
    "é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚\n",
    "\n",
    "è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚\n",
    "\n",
    "å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ç€é‡ä»‹ç»Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚\n",
    "\n",
    "å¹¶ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚\n",
    "\n",
    "æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»ä½¿ç”¨Autographçš„ç¼–ç è§„èŒƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒAutographç¼–ç è§„èŒƒæ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½å¯èƒ½ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚ä¾‹å¦‚ä½¿ç”¨tf.printè€Œä¸æ˜¯printï¼Œä½¿ç”¨tf.rangeè€Œä¸æ˜¯rangeï¼Œä½¿ç”¨tf.constant(True)è€Œä¸æ˜¯True.\n",
    "\n",
    "* 2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. \n",
    "\n",
    "* 3ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼ŒAutographç¼–ç è§„èŒƒè§£æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½é‡ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def np_random():\n",
    "    a = np.random.randn(3,3)\n",
    "    tf.print(a)\n",
    "    \n",
    "@tf.function\n",
    "def tf_random():\n",
    "    a = tf.random.normal((3,3))\n",
    "    tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_randomæ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ä¸€æ ·çš„ç»“æœã€‚\n",
    "np_random()\n",
    "np_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[ 0.22619201, -0.4550123 , -0.42587565],\n",
    "       [ 0.05429906,  0.2312667 , -1.44819738],\n",
    "       [ 0.36571796,  1.45578986, -1.05348983]])\n",
    "array([[ 0.22619201, -0.4550123 , -0.42587565],\n",
    "       [ 0.05429906,  0.2312667 , -1.44819738],\n",
    "       [ 0.36571796,  1.45578986, -1.05348983]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_randomæ¯æ¬¡æ‰§è¡Œéƒ½ä¼šæœ‰é‡æ–°ç”Ÿæˆéšæœºæ•°ã€‚\n",
    "tf_random()\n",
    "tf_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-1.38956189 -0.394843668 0.420657277]\n",
    " [2.87235498 -1.33740318 -0.533843279]\n",
    " [0.918233037 0.118598573 -0.399486482]]\n",
    "[[-0.858178258 1.67509317 0.511889517]\n",
    " [-0.545829177 -2.20118237 -0.968222201]\n",
    " [0.733958483 -0.61904633 0.77440238]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable.\n",
    "\n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "@tf.function\n",
    "def outer_var():\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "outer_var() \n",
    "outer_var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inner_var():\n",
    "    x = tf.Variable(1.0,dtype = tf.float32)\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "#æ‰§è¡Œå°†æŠ¥é”™\n",
    "#inner_var()\n",
    "#inner_var()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-12-c95a7c3c1ddd> in <module>\n",
    "      7 \n",
    "      8 #æ‰§è¡Œå°†æŠ¥é”™\n",
    "----> 9 inner_var()\n",
    "     10 inner_var()\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\n",
    "    566         xla_context.Exit()\n",
    "    567     else:\n",
    "--> 568       result = self._call(*args, **kwds)\n",
    "    569 \n",
    "    570     if tracing_count == self._get_tracing_count():\n",
    "......\n",
    "ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3,è¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰ç»“æ„ç±»å‹å˜é‡ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "\n",
    "#@tf.function #åŠ ä¸Šè¿™ä¸€è¡Œåˆ‡æ¢æˆAutographç»“æœå°†ä¸ç¬¦åˆé¢„æœŸï¼ï¼ï¼\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(), dtype=float32, numpy=5.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "\n",
    "@tf.function #åŠ ä¸Šè¿™ä¸€è¡Œåˆ‡æ¢æˆAutographç»“æœå°†ä¸ç¬¦åˆé¢„æœŸï¼ï¼ï¼\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor 'x:0' shape=() dtype=float32>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-4,AutoGraphçš„æœºåˆ¶åŸç†\n",
    "\n",
    "æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚\n",
    "\n",
    "TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚\n",
    "\n",
    "åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚\n",
    "\n",
    "é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚\n",
    "\n",
    "è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚\n",
    "\n",
    "å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚\n",
    "\n",
    "æˆ‘ä»¬ä¼šä»‹ç»Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚\n",
    "\n",
    "å¹¶ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚\n",
    "\n",
    "ä¸Šç¯‡æˆ‘ä»¬ä»‹ç»äº†Autographçš„ç¼–ç è§„èŒƒï¼Œæœ¬ç¯‡æˆ‘ä»¬ä»‹ç»Autographçš„æœºåˆ¶åŸç†ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒAutographçš„æœºåˆ¶åŸç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å½“æˆ‘ä»¬ä½¿ç”¨@tf.functionè£…é¥°ä¸€ä¸ªå‡½æ•°çš„æ—¶å€™ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ**\n",
    "\n",
    "ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "@tf.function(autograph=True)\n",
    "def myadd(a,b):\n",
    "    for i in tf.range(3):\n",
    "        tf.print(i)\n",
    "    c = a+b\n",
    "    print(\"tracing\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åé¢ä»€ä¹ˆéƒ½æ²¡æœ‰å‘ç”Ÿã€‚ä»…ä»…æ˜¯åœ¨Pythonå †æ ˆä¸­è®°å½•äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°çš„ç­¾åã€‚\n",
    "\n",
    "**å½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(\"hello\"),tf.constant(\"world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‘ç”Ÿäº†2ä»¶äº‹æƒ…ï¼Œ\n",
    "\n",
    "ç¬¬ä¸€ä»¶äº‹æƒ…æ˜¯åˆ›å»ºè®¡ç®—å›¾ã€‚\n",
    "\n",
    "å³åˆ›å»ºä¸€ä¸ªé™æ€è®¡ç®—å›¾ï¼Œè·Ÿè¸ªæ‰§è¡Œä¸€éå‡½æ•°ä½“ä¸­çš„Pythonä»£ç ï¼Œç¡®å®šå„ä¸ªå˜é‡çš„Tensorç±»å‹ï¼Œå¹¶æ ¹æ®æ‰§è¡Œé¡ºåºå°†ç®—å­æ·»åŠ åˆ°è®¡ç®—å›¾ä¸­ã€‚\n",
    "åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¦‚æœå¼€å¯äº†autograph=True(é»˜è®¤å¼€å¯),ä¼šå°†Pythonæ§åˆ¶æµè½¬æ¢æˆTensorFlowå›¾å†…æ§åˆ¶æµã€‚\n",
    "ä¸»è¦æ˜¯å°†ifè¯­å¥è½¬æ¢æˆ tf.condç®—å­è¡¨è¾¾ï¼Œå°†whileå’Œforå¾ªç¯è¯­å¥è½¬æ¢æˆtf.while_loopç®—å­è¡¨è¾¾ï¼Œå¹¶åœ¨å¿…è¦çš„æ—¶å€™æ·»åŠ \n",
    "tf.control_dependenciesæŒ‡å®šæ‰§è¡Œé¡ºåºä¾èµ–å…³ç³»ã€‚\n",
    "\n",
    "ç›¸å½“äºåœ¨ tensorflow1.0æ‰§è¡Œäº†ç±»ä¼¼ä¸‹é¢çš„è¯­å¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    b = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    cond = lambda i: i<tf.constant(3)\n",
    "    def body(i):\n",
    "        tf.print(i)\n",
    "        return(i+1)\n",
    "    loop = tf.while_loop(cond,body,loop_vars=[0])\n",
    "    loop\n",
    "    with tf.control_dependencies(loop):\n",
    "        c = tf.strings.join([a,b])\n",
    "    print(\"tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬äºŒä»¶äº‹æƒ…æ˜¯æ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "ç›¸å½“äºåœ¨ tensorflow1.0ä¸­æ‰§è¡Œäº†ä¸‹é¢çš„è¯­å¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(c,feed_dict={a:tf.constant(\"hello\"),b:tf.constant(\"world\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤æˆ‘ä»¬å…ˆçœ‹åˆ°çš„æ˜¯ç¬¬ä¸€ä¸ªæ­¥éª¤çš„ç»“æœï¼šå³Pythonè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°\"tracing\"è¯­å¥ã€‚\n",
    "\n",
    "ç„¶åçœ‹åˆ°ç¬¬äºŒä¸ªæ­¥éª¤çš„ç»“æœï¼šTensorFlowè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°1,2,3ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å½“æˆ‘ä»¬å†æ¬¡ç”¨ç›¸åŒçš„è¾“å…¥å‚æ•°ç±»å‹è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(\"good\"),tf.constant(\"morning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åªä¼šå‘ç”Ÿä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯ä¸Šé¢æ­¥éª¤çš„ç¬¬äºŒæ­¥ï¼Œæ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "æ‰€ä»¥è¿™ä¸€æ¬¡æˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°æ‰“å°\"tracing\"çš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å½“æˆ‘ä»¬å†æ¬¡ç”¨ä¸åŒçš„çš„è¾“å…¥å‚æ•°ç±»å‹è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(1),tf.constant(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºè¾“å…¥å‚æ•°çš„ç±»å‹å·²ç»å‘ç”Ÿå˜åŒ–ï¼Œå·²ç»åˆ›å»ºçš„è®¡ç®—å›¾ä¸èƒ½å¤Ÿå†æ¬¡ä½¿ç”¨ã€‚\n",
    "\n",
    "éœ€è¦é‡æ–°åš2ä»¶äº‹æƒ…ï¼šåˆ›å»ºæ–°çš„è®¡ç®—å›¾ã€æ‰§è¡Œè®¡ç®—å›¾ã€‚\n",
    "\n",
    "æ‰€ä»¥æˆ‘ä»¬åˆä¼šå…ˆçœ‹åˆ°çš„æ˜¯ç¬¬ä¸€ä¸ªæ­¥éª¤çš„ç»“æœï¼šå³Pythonè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°\"tracing\"è¯­å¥ã€‚\n",
    "\n",
    "ç„¶åå†çœ‹åˆ°ç¬¬äºŒä¸ªæ­¥éª¤çš„ç»“æœï¼šTensorFlowè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°1,2,3ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœè°ƒç”¨è¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶è¾“å…¥çš„å‚æ•°ä¸æ˜¯Tensorç±»å‹ï¼Œåˆ™æ¯æ¬¡éƒ½ä¼šé‡æ–°åˆ›å»ºè®¡ç®—å›¾ã€‚**\n",
    "\n",
    "ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚ä¸¤æ¬¡éƒ½ä¼šé‡æ–°åˆ›å»ºè®¡ç®—å›¾ã€‚å› æ­¤ï¼Œä¸€èˆ¬å»ºè®®è°ƒç”¨@tf.functionæ—¶åº”ä¼ å…¥Tensorç±»å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(\"hello\",\"world\")\n",
    "myadd(\"good\",\"morning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œé‡æ–°ç†è§£Autographçš„ç¼–ç è§„èŒƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "äº†è§£äº†ä»¥ä¸ŠAutographçš„æœºåˆ¶åŸç†ï¼Œæˆ‘ä»¬ä¹Ÿå°±èƒ½å¤Ÿç†è§£Autographç¼–ç è§„èŒƒçš„3æ¡å»ºè®®äº†ã€‚\n",
    "\n",
    "1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½é‡ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚ä¾‹å¦‚ä½¿ç”¨tf.printè€Œä¸æ˜¯print.\n",
    "\n",
    "è§£é‡Šï¼šPythonä¸­çš„å‡½æ•°ä»…ä»…ä¼šåœ¨è·Ÿè¸ªæ‰§è¡Œå‡½æ•°ä»¥åˆ›å»ºé™æ€å›¾çš„é˜¶æ®µä½¿ç”¨ï¼Œæ™®é€šPythonå‡½æ•°æ˜¯æ— æ³•åµŒå…¥åˆ°é™æ€è®¡ç®—å›¾ä¸­çš„ï¼Œæ‰€ä»¥\n",
    "åœ¨è®¡ç®—å›¾æ„å»ºå¥½ä¹‹åå†æ¬¡è°ƒç”¨çš„æ—¶å€™ï¼Œè¿™äº›Pythonå‡½æ•°å¹¶æ²¡æœ‰è¢«è®¡ç®—ï¼Œè€ŒTensorFlowä¸­çš„å‡½æ•°åˆ™å¯ä»¥åµŒå…¥åˆ°è®¡ç®—å›¾ä¸­ã€‚ä½¿ç”¨æ™®é€šçš„Pythonå‡½æ•°ä¼šå¯¼è‡´\n",
    "è¢«@tf.functionä¿®é¥°å‰ã€eageræ‰§è¡Œã€‘å’Œè¢«@tf.functionä¿®é¥°åã€é™æ€å›¾æ‰§è¡Œã€‘çš„è¾“å‡ºä¸ä¸€è‡´ã€‚\n",
    "\n",
    "2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. \n",
    "\n",
    "è§£é‡Šï¼šå¦‚æœå‡½æ•°å†…éƒ¨å®šä¹‰äº†tf.Variable,é‚£ä¹ˆåœ¨ã€eageræ‰§è¡Œã€‘æ—¶ï¼Œè¿™ç§åˆ›å»ºtf.Variableçš„è¡Œä¸ºåœ¨æ¯æ¬¡å‡½æ•°è°ƒç”¨æ—¶å€™éƒ½ä¼šå‘ç”Ÿã€‚ä½†æ˜¯åœ¨ã€é™æ€å›¾æ‰§è¡Œã€‘æ—¶ï¼Œè¿™ç§åˆ›å»ºtf.Variableçš„è¡Œä¸ºåªä¼šå‘ç”Ÿåœ¨ç¬¬ä¸€æ­¥è·Ÿè¸ªPythonä»£ç é€»è¾‘åˆ›å»ºè®¡ç®—å›¾æ—¶ï¼Œè¿™ä¼šå¯¼è‡´è¢«@tf.functionä¿®é¥°å‰ã€eageræ‰§è¡Œã€‘å’Œè¢«@tf.functionä¿®é¥°åã€é™æ€å›¾æ‰§è¡Œã€‘çš„è¾“å‡ºä¸ä¸€è‡´ã€‚å®é™…ä¸Šï¼ŒTensorFlowåœ¨è¿™ç§æƒ…å†µä¸‹ä¸€èˆ¬ä¼šæŠ¥é”™ã€‚\n",
    "\n",
    "3ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡ã€‚\n",
    "\n",
    "è§£é‡Šï¼šé™æ€è®¡ç®—å›¾æ˜¯è¢«ç¼–è¯‘æˆC++ä»£ç åœ¨TensorFlowå†…æ ¸ä¸­æ‰§è¡Œçš„ã€‚Pythonä¸­çš„åˆ—è¡¨å’Œå­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡æ˜¯æ— æ³•åµŒå…¥åˆ°è®¡ç®—å›¾ä¸­ï¼Œå®ƒä»¬ä»…ä»…èƒ½å¤Ÿåœ¨åˆ›å»ºè®¡ç®—å›¾æ—¶è¢«è¯»å–ï¼Œåœ¨æ‰§è¡Œè®¡ç®—å›¾æ—¶æ˜¯æ— æ³•ä¿®æ”¹Pythonä¸­çš„åˆ—è¡¨æˆ–å­—å…¸è¿™æ ·çš„æ•°æ®ç»“æ„å˜é‡çš„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-5,AutoGraphå’Œtf.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚\n",
    "\n",
    "TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚\n",
    "\n",
    "åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚\n",
    "\n",
    "é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚\n",
    "\n",
    "è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚\n",
    "\n",
    "å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚\n",
    "\n",
    "å‰é¢æˆ‘ä»¬ä»‹ç»äº†Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚\n",
    "\n",
    "æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒAutographå’Œtf.Moduleæ¦‚è¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰é¢åœ¨ä»‹ç»Autographçš„ç¼–ç è§„èŒƒæ—¶æåˆ°æ„å»ºAutographæ—¶åº”è¯¥é¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. \n",
    "\n",
    "ä½†æ˜¯å¦‚æœåœ¨å‡½æ•°å¤–éƒ¨å®šä¹‰tf.Variableçš„è¯ï¼Œåˆä¼šæ˜¾å¾—è¿™ä¸ªå‡½æ•°æœ‰å¤–éƒ¨å˜é‡ä¾èµ–ï¼Œå°è£…ä¸å¤Ÿå®Œç¾ã€‚\n",
    "\n",
    "ä¸€ç§ç®€å•çš„æ€è·¯æ˜¯å®šä¹‰ä¸€ä¸ªç±»ï¼Œå¹¶å°†ç›¸å…³çš„tf.Variableåˆ›å»ºæ”¾åœ¨ç±»çš„åˆå§‹åŒ–æ–¹æ³•ä¸­ã€‚è€Œå°†å‡½æ•°çš„é€»è¾‘æ”¾åœ¨å…¶ä»–æ–¹æ³•ä¸­ã€‚\n",
    "\n",
    "è¿™æ ·ä¸€é¡¿çŒ›å¦‚è™çš„æ“ä½œä¹‹åï¼Œæˆ‘ä»¬ä¼šè§‰å¾—ä¸€åˆ‡éƒ½å¦‚åŒäººæ³•åœ°åœ°æ³•å¤©å¤©æ³•é“é“æ³•è‡ªç„¶èˆ¬çš„è‡ªç„¶ã€‚\n",
    "\n",
    "æƒŠå–œçš„æ˜¯ï¼ŒTensorFlowæä¾›äº†ä¸€ä¸ªåŸºç±»tf.Moduleï¼Œé€šè¿‡ç»§æ‰¿å®ƒæ„å»ºå­ç±»ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥è·å¾—ä»¥ä¸Šçš„è‡ªç„¶è€Œç„¶ï¼Œè€Œä¸”å¯ä»¥éå¸¸æ–¹ä¾¿åœ°ç®¡ç†å˜é‡ï¼Œè¿˜å¯ä»¥éå¸¸æ–¹ä¾¿åœ°ç®¡ç†å®ƒå¼•ç”¨çš„å…¶å®ƒModuleï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨tf.saved_modelä¿å­˜æ¨¡å‹å¹¶å®ç°è·¨å¹³å°éƒ¨ç½²ä½¿ç”¨ã€‚\n",
    "\n",
    "å®é™…ä¸Šï¼Œtf.keras.models.Model,tf.keras.layers.Layer éƒ½æ˜¯ç»§æ‰¿è‡ªtf.Moduleçš„ï¼Œæä¾›äº†æ–¹ä¾¿çš„å˜é‡ç®¡ç†å’Œæ‰€å¼•ç”¨çš„å­æ¨¡å—ç®¡ç†çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "**å› æ­¤ï¼Œåˆ©ç”¨tf.Moduleæä¾›çš„å°è£…ï¼Œå†ç»“åˆTensoFlowä¸°å¯Œçš„ä½é˜¶APIï¼Œå®é™…ä¸Šæˆ‘ä»¬èƒ½å¤ŸåŸºäºTensorFlowå¼€å‘ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹(è€Œéä»…ä»…æ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹)ï¼Œå¹¶å®ç°è·¨å¹³å°éƒ¨ç½²ä½¿ç”¨ã€‚**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œåº”ç”¨tf.Moduleå°è£…Autograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šä¹‰ä¸€ä¸ªç®€å•çš„functionã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "\n",
    "#åœ¨tf.functionä¸­ç”¨input_signatureé™å®šè¾“å…¥å¼ é‡çš„ç­¾åç±»å‹ï¼šshapeå’Œdtype\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    \n",
    "def add_print(a):\n",
    "    x.assign_add(a)\n",
    "    tf.print(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_print(tf.constant(3.0))\n",
    "#add_print(tf.constant(3)) #è¾“å…¥ä¸ç¬¦åˆå¼ é‡ç­¾åçš„å‚æ•°å°†æŠ¥é”™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢åˆ©ç”¨tf.Moduleçš„å­ç±»åŒ–å°†å…¶å°è£…ä¸€ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoModule(tf.Module):\n",
    "    def __init__(self,init_value = tf.constant(0.0),name=None):\n",
    "        super(DemoModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #ç›¸å½“äºwith tf.name_scope(\"demo_module\")\n",
    "            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=True)\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "    def addprint(self,a):\n",
    "        with self.name_scope:\n",
    "            self.x.assign_add(a)\n",
    "            tf.print(self.x)\n",
    "            return(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰§è¡Œ\n",
    "demo = DemoModule(init_value = tf.constant(1.0))\n",
    "result = demo.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŸ¥çœ‹æ¨¡å—ä¸­çš„å…¨éƒ¨å˜é‡å’Œå…¨éƒ¨å¯è®­ç»ƒå˜é‡\n",
    "print(demo.variables)\n",
    "print(demo.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n",
    "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŸ¥çœ‹æ¨¡å—ä¸­çš„å…¨éƒ¨å­æ¨¡å—\n",
    "demo.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨tf.saved_model ä¿å­˜æ¨¡å‹ï¼Œå¹¶æŒ‡å®šéœ€è¦è·¨å¹³å°éƒ¨ç½²çš„æ–¹æ³•\n",
    "tf.saved_model.save(demo,\"./data/demo/1\",signatures = {\"serving_default\":demo.addprint})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åŠ è½½æ¨¡å‹\n",
    "demo2 = tf.saved_model.load(\"./data/demo/1\")\n",
    "demo2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯ï¼Œçº¢æ¡†æ ‡å‡ºæ¥çš„è¾“å‡ºä¿¡æ¯åœ¨æ¨¡å‹éƒ¨ç½²å’Œè·¨å¹³å°ä½¿ç”¨æ—¶æœ‰å¯èƒ½ä¼šç”¨åˆ°\n",
    "!saved_model_cli show --dir ./data/demo/1 --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ä¿¡æ¯.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨tensorboardä¸­æŸ¥çœ‹è®¡ç®—å›¾ï¼Œæ¨¡å—ä¼šè¢«æ·»åŠ æ¨¡å—ådemo_module,æ–¹ä¾¿å±‚æ¬¡åŒ–å‘ˆç°è®¡ç®—å›¾ç»“æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# åˆ›å»ºæ—¥å¿—\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/demomodule/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "#å¼€å¯autographè·Ÿè¸ª\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "#æ‰§è¡Œautograph\n",
    "demo = DemoModule(init_value = tf.constant(0.0))\n",
    "result = demo.addprint(tf.constant(5.0))\n",
    "\n",
    "#å°†è®¡ç®—å›¾ä¿¡æ¯å†™å…¥æ—¥å¿—\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"demomodule\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯åŠ¨ tensorboardåœ¨jupyterä¸­çš„é­”æ³•å‘½ä»¤\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.start(\"--logdir ./data/demomodule/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/demomoduleçš„è®¡ç®—å›¾ç»“æ„.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é™¤äº†åˆ©ç”¨tf.Moduleçš„å­ç±»åŒ–å®ç°å°è£…ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»™tf.Moduleæ·»åŠ å±æ€§çš„æ–¹æ³•è¿›è¡Œå°è£…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodule = tf.Module()\n",
    "mymodule.x = tf.Variable(0.0)\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "def addprint(a):\n",
    "    mymodule.x.assign_add(a)\n",
    "    tf.print(mymodule.x)\n",
    "    return (mymodule.x)\n",
    "\n",
    "mymodule.addprint = addprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodule.addprint(tf.constant(1.0)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mymodule.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨tf.saved_model ä¿å­˜æ¨¡å‹\n",
    "tf.saved_model.save(mymodule,\"./data/mymodule\",\n",
    "    signatures = {\"serving_default\":mymodule.addprint})\n",
    "\n",
    "#åŠ è½½æ¨¡å‹\n",
    "mymodule2 = tf.saved_model.load(\"./data/mymodule\")\n",
    "mymodule2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "INFO:tensorflow:Assets written to: ./data/mymodule/assets\n",
    "5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œtf.Moduleå’Œtf.keras.Modelï¼Œtf.keras.layers.Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.kerasä¸­çš„æ¨¡å‹å’Œå±‚éƒ½æ˜¯ç»§æ‰¿tf.Moduleå®ç°çš„ï¼Œä¹Ÿå…·æœ‰å˜é‡ç®¡ç†å’Œå­æ¨¡å—ç®¡ç†åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,losses,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issubclass(tf.keras.Model,tf.Module))\n",
    "print(issubclass(tf.keras.layers.Layer,tf.Module))\n",
    "print(issubclass(tf.keras.Model,tf.keras.layers.Layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "True\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(4,input_shape = (10,)))\n",
    "model.add(layers.Dense(2))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 4)                 44        \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 2)                 10        \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 3         \n",
    "=================================================================\n",
    "Total params: 57\n",
    "Trainable params: 57\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Variable 'dense/kernel:0' shape=(10, 4) dtype=float32, numpy=\n",
    " array([[-0.06741005,  0.45534766,  0.5190817 , -0.01806331],\n",
    "        [-0.14258742, -0.49711505,  0.26030976,  0.18607801],\n",
    "        [-0.62806034,  0.5327399 ,  0.42206633,  0.29201728],\n",
    "        [-0.16602087, -0.18901917,  0.55159235, -0.01091868],\n",
    "        [ 0.04533798,  0.326845  , -0.582667  ,  0.19431782],\n",
    "        [ 0.6494713 , -0.16174704,  0.4062966 ,  0.48760796],\n",
    "        [ 0.58400524, -0.6280886 , -0.11265379, -0.6438277 ],\n",
    "        [ 0.26642334,  0.49275804,  0.20793378, -0.43889117],\n",
    "        [ 0.4092741 ,  0.09871006, -0.2073121 ,  0.26047975],\n",
    "        [ 0.43910992,  0.00199282, -0.07711256, -0.27966842]],\n",
    "       dtype=float32)>,\n",
    " <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
    " array([[ 0.5022683 , -0.0507431 ],\n",
    "        [-0.61540484,  0.9369011 ],\n",
    "        [-0.14412141, -0.54607415],\n",
    "        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
    " array([[-0.244825 ],\n",
    "        [-1.2101456]], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False #å†»ç»“ç¬¬0å±‚çš„å˜é‡,ä½¿å…¶ä¸å¯è®­ç»ƒ\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
    " array([[ 0.5022683 , -0.0507431 ],\n",
    "        [-0.61540484,  0.9369011 ],\n",
    "        [-0.14412141, -0.54607415],\n",
    "        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
    " array([[-0.244825 ],\n",
    "        [-1.2101456]], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.submodules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x144d8c080>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144daada0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tensorflow.python.keras.layers.core.Dense at 0x144daada0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name)\n",
    "print(model.name_scope())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sequential\n",
    "sequential\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äº”ã€TensorFlowçš„ä¸­é˜¶API\n",
    "\n",
    "TensorFlowçš„ä¸­é˜¶APIä¸»è¦åŒ…æ‹¬: \n",
    "\n",
    "* æ•°æ®ç®¡é“(tf.data)\n",
    "\n",
    "* ç‰¹å¾åˆ—(tf.feature_column)\n",
    "\n",
    "* æ¿€æ´»å‡½æ•°(tf.nn)\n",
    "\n",
    "* æ¨¡å‹å±‚(tf.keras.layers)\n",
    "\n",
    "* æŸå¤±å‡½æ•°(tf.keras.losses)\n",
    "\n",
    "* è¯„ä¼°å‡½æ•°(tf.keras.metrics)\n",
    "\n",
    "* ä¼˜åŒ–å™¨(tf.keras.optimizers)\n",
    "\n",
    "* å›è°ƒå‡½æ•°(tf.keras.callbacks)\n",
    "\n",
    "å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä¸­é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-1,æ•°æ®ç®¡é“Dataset\n",
    "\n",
    "å¦‚æœéœ€è¦è®­ç»ƒçš„æ•°æ®å¤§å°ä¸å¤§ï¼Œä¾‹å¦‚ä¸åˆ°1Gï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥å…¨éƒ¨è¯»å…¥å†…å­˜ä¸­è¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·ä¸€èˆ¬æ•ˆç‡æœ€é«˜ã€‚\n",
    "\n",
    "ä½†å¦‚æœéœ€è¦è®­ç»ƒçš„æ•°æ®å¾ˆå¤§ï¼Œä¾‹å¦‚è¶…è¿‡10Gï¼Œæ— æ³•ä¸€æ¬¡è½½å…¥å†…å­˜ï¼Œé‚£ä¹ˆé€šå¸¸éœ€è¦åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­åˆ†æ‰¹é€æ¸è¯»å…¥ã€‚\n",
    "\n",
    "ä½¿ç”¨ tf.data API å¯ä»¥æ„å»ºæ•°æ®è¾“å…¥ç®¡é“ï¼Œè½»æ¾å¤„ç†å¤§é‡çš„æ•°æ®ï¼Œä¸åŒçš„æ•°æ®æ ¼å¼ï¼Œä»¥åŠä¸åŒçš„æ•°æ®è½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œæ„å»ºæ•°æ®ç®¡é“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä» Numpy array, Pandas DataFrame, Python generator, csvæ–‡ä»¶, æ–‡æœ¬æ–‡ä»¶, æ–‡ä»¶è·¯å¾„, tfrecordsæ–‡ä»¶ç­‰æ–¹å¼æ„å»ºæ•°æ®ç®¡é“ã€‚\n",
    "\n",
    "å…¶ä¸­é€šè¿‡Numpy array, Pandas DataFrame, æ–‡ä»¶è·¯å¾„æ„å»ºæ•°æ®ç®¡é“æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚\n",
    "\n",
    "é€šè¿‡tfrecordsæ–‡ä»¶æ–¹å¼æ„å»ºæ•°æ®ç®¡é“è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦å¯¹æ ·æœ¬æ„å»ºtf.Exampleåå‹ç¼©æˆå­—ç¬¦ä¸²å†™åˆ°tfrecoredsæ–‡ä»¶ï¼Œè¯»å–åå†è§£ææˆtf.Exampleã€‚\n",
    "\n",
    "ä½†tfrecoredsæ–‡ä»¶çš„ä¼˜ç‚¹æ˜¯å‹ç¼©åæ–‡ä»¶è¾ƒå°ï¼Œä¾¿äºç½‘ç»œä¼ æ’­ï¼ŒåŠ è½½é€Ÿåº¦è¾ƒå¿«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1,ä»Numpy arrayæ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»Numpy arrayæ„å»ºæ•°æ®ç®¡é“\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "ds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"],iris[\"target\"]))\n",
    "for features,label in ds1.take(5):\n",
    "    print(features,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2,ä» Pandas DataFrameæ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä» Pandas DataFrameæ„å»ºæ•°æ®ç®¡é“\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets \n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "dfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\n",
    "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n",
    "\n",
    "for features,label in ds2.take(3):\n",
    "    print(features,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3,ä»Python generatoræ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»Python generatoræ„å»ºæ•°æ®ç®¡é“\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªä»æ–‡ä»¶ä¸­è¯»å–å›¾ç‰‡çš„generator\n",
    "image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
    "                    \"./data/cifar2/test/\",\n",
    "                    target_size=(32, 32),\n",
    "                    batch_size=20,\n",
    "                    class_mode='binary')\n",
    "\n",
    "classdict = image_generator.class_indices\n",
    "print(classdict)\n",
    "\n",
    "def generator():\n",
    "    for features,label in image_generator:\n",
    "        yield (features,label)\n",
    "\n",
    "ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds3.unbatch().take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-cifar2é¢„è§ˆ.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4,ä»csvæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»csvæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“\n",
    "ds4 = tf.data.experimental.make_csv_dataset(\n",
    "      file_pattern = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"],\n",
    "      batch_size=3, \n",
    "      label_name=\"Survived\",\n",
    "      na_value=\"\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)\n",
    "\n",
    "for data,label in ds4.take(2):\n",
    "    print(data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([540,  58, 764], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 1], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Frolicher, Miss. Hedwig Margaritha', b'Novel, Mr. Mansouer',\n",
    "       b'Carter, Mrs. William Ernest (Lucile Polk)'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'female', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([22. , 28.5, 36. ], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 2], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'13568', b'2697', b'113760'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 49.5   ,   7.2292, 120.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'B39', b'', b'B96 B98'], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'C', b'S'], dtype=object)>)]) tf.Tensor([1 0 1], shape=(3,), dtype=int32)\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([845,  66, 390], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 2], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Culumovic, Mr. Jeso', b'Moubarek, Master. Gerios',\n",
    "       b'Lehmann, Miss. Bertha'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'male', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([17.,  0., 17.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'315090', b'2661', b'SC 1748'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 8.6625, 15.2458, 12.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'S', b'C', b'C'], dtype=object)>)]) tf.Tensor([0 1 1], shape=(3,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5,ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“\n",
    "\n",
    "ds5 = tf.data.TextLineDataset(\n",
    "    filenames = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"]\n",
    "    ).skip(1) #ç•¥å»ç¬¬ä¸€è¡Œheader\n",
    "\n",
    "for line in ds5.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'687,0,3,\"Panula, Mr. Jaako Arnold\",male,14.0,4,1,3101295,39.6875,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6,ä»æ–‡ä»¶è·¯å¾„æ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6 = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "for file in ds6.take(5):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/1263.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/2837.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/4264.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/4241.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/192.jpg', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "for i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n",
    "    plt.figure(i)\n",
    "    plt.imshow((img/255.0).numpy())\n",
    "    plt.title(\"label = %d\"%label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7,ä»tfrecordsæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# inpathï¼šåŸå§‹æ•°æ®è·¯å¾„ outpath:TFRecordæ–‡ä»¶è¾“å‡ºè·¯å¾„\n",
    "def create_tfrecords(inpath,outpath): \n",
    "    writer = tf.io.TFRecordWriter(outpath)\n",
    "    dirs = os.listdir(inpath)\n",
    "    for index, name in enumerate(dirs):\n",
    "        class_path = inpath +\"/\"+ name+\"/\"\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = class_path + img_name\n",
    "            img = tf.io.read_file(img_path)\n",
    "            #img = tf.image.decode_image(img)\n",
    "            #img = tf.image.encode_jpeg(img) #ç»Ÿä¸€æˆjpegæ ¼å¼å‹ç¼©\n",
    "            example = tf.train.Example(\n",
    "               features=tf.train.Features(feature={\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n",
    "               }))\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "create_tfrecords(\"./data/cifar2/test/\",\"./data/cifar2_test.tfrecords/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def parse_example(proto):\n",
    "    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n",
    "                   'label': tf.io.FixedLenFeature([], tf.int64)} \n",
    "    example = tf.io.parse_single_example(proto, description)\n",
    "    img = tf.image.decode_jpeg(example[\"img_raw\"])   #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼\n",
    "    img = tf.image.resize(img, (32,32))\n",
    "    label = example[\"label\"]\n",
    "    return(img,label)\n",
    "\n",
    "ds7 = tf.data.TFRecordDataset(\"./data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds7.take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow((img/255.0).numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œåº”ç”¨æ•°æ®è½¬æ¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasetæ•°æ®ç»“æ„åº”ç”¨éå¸¸çµæ´»ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªSequeceåºåˆ—ï¼Œå…¶æ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯å„ç§ç±»å‹ï¼Œä¾‹å¦‚å¯ä»¥æ˜¯å¼ é‡ï¼Œåˆ—è¡¨ï¼Œå­—å…¸ï¼Œä¹Ÿå¯ä»¥æ˜¯Datasetã€‚\n",
    "\n",
    "DatasetåŒ…å«äº†éå¸¸ä¸°å¯Œçš„æ•°æ®è½¬æ¢åŠŸèƒ½ã€‚\n",
    "\n",
    "* map: å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†æ¯ä¸€ä¸ªå…ƒç´ ã€‚\n",
    "\n",
    "* flat_map: å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶å°†åµŒå¥—çš„Datasetå‹å¹³ã€‚\n",
    "\n",
    "* interleave: æ•ˆæœç±»ä¼¼flat_map,ä½†å¯ä»¥å°†ä¸åŒæ¥æºçš„æ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚\n",
    "\n",
    "* filter: è¿‡æ»¤æ‰æŸäº›å…ƒç´ ã€‚\n",
    "\n",
    "* zip: å°†ä¸¤ä¸ªé•¿åº¦ç›¸åŒçš„Datasetæ¨ªå‘é“°åˆã€‚\n",
    "\n",
    "* concatenate: å°†ä¸¤ä¸ªDatasetçºµå‘è¿æ¥ã€‚\n",
    "\n",
    "* reduce: æ‰§è¡Œå½’å¹¶æ“ä½œã€‚\n",
    "\n",
    "* batch : æ„å»ºæ‰¹æ¬¡ï¼Œæ¯æ¬¡æ”¾ä¸€ä¸ªæ‰¹æ¬¡ã€‚æ¯”åŸå§‹æ•°æ®å¢åŠ ä¸€ä¸ªç»´åº¦ã€‚ å…¶é€†æ“ä½œä¸ºunbatchã€‚\n",
    "\n",
    "* padded_batch: æ„å»ºæ‰¹æ¬¡ï¼Œç±»ä¼¼batch, ä½†å¯ä»¥å¡«å……åˆ°ç›¸åŒçš„å½¢çŠ¶ã€‚\n",
    "\n",
    "* window :æ„å»ºæ»‘åŠ¨çª—å£ï¼Œè¿”å›Dataset of Dataset.\n",
    "\n",
    "* shuffle: æ•°æ®é¡ºåºæ´—ç‰Œã€‚\n",
    "\n",
    "* repeat: é‡å¤æ•°æ®è‹¥å¹²æ¬¡ï¼Œä¸å¸¦å‚æ•°æ—¶ï¼Œé‡å¤æ— æ•°æ¬¡ã€‚\n",
    "\n",
    "* shard: é‡‡æ ·ï¼Œä»æŸä¸ªä½ç½®å¼€å§‹éš”å›ºå®šè·ç¦»é‡‡æ ·ä¸€ä¸ªå…ƒç´ ã€‚\n",
    "\n",
    "* take: é‡‡æ ·ï¼Œä»å¼€å§‹ä½ç½®å–å‰å‡ ä¸ªå…ƒç´ ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map:å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†æ¯ä¸€ä¸ªå…ƒç´ \n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_map = ds.map(lambda x:tf.strings.split(x,\" \"))\n",
    "for x in ds_map:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_map:å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶å°†åµŒå¥—çš„Datasetå‹å¹³ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_flatmap = ds.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_flatmap:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave: æ•ˆæœç±»ä¼¼flat_map,ä½†å¯ä»¥å°†ä¸åŒæ¥æºçš„æ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_interleave:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter:è¿‡æ»¤æ‰æŸäº›å…ƒç´ ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "#æ‰¾å‡ºå«æœ‰å­—æ¯aæˆ–Bçš„å…ƒç´ \n",
    "ds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, \".*[a|B].*\"))\n",
    "for x in ds_filter:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip:å°†ä¸¤ä¸ªé•¿åº¦ç›¸åŒçš„Datasetæ¨ªå‘é“°åˆã€‚\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds3 = tf.data.Dataset.range(6,9)\n",
    "ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))\n",
    "for x,y,z in ds_zip:\n",
    "    print(x.numpy(),y.numpy(),z.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 3 6\n",
    "1 4 7\n",
    "2 5 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condatenate:å°†ä¸¤ä¸ªDatasetçºµå‘è¿æ¥ã€‚\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds_concat = tf.data.Dataset.concatenate(ds1,ds2)\n",
    "for x in ds_concat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce:æ‰§è¡Œå½’å¹¶æ“ä½œã€‚\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5.0])\n",
    "result = ds.reduce(0.0,lambda x,y:tf.add(x,y))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch:æ„å»ºæ‰¹æ¬¡ï¼Œæ¯æ¬¡æ”¾ä¸€ä¸ªæ‰¹æ¬¡ã€‚æ¯”åŸå§‹æ•°æ®å¢åŠ ä¸€ä¸ªç»´åº¦ã€‚ å…¶é€†æ“ä½œä¸ºunbatchã€‚ \n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_batch = ds.batch(4)\n",
    "for x in ds_batch:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
    "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_batch:æ„å»ºæ‰¹æ¬¡ï¼Œç±»ä¼¼batch, ä½†å¯ä»¥å¡«å……åˆ°ç›¸åŒçš„å½¢çŠ¶ã€‚\n",
    "\n",
    "elements = [[1, 2],[3, 4, 5],[6, 7],[8]]\n",
    "ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n",
    "\n",
    "ds_padded_batch = ds.padded_batch(2,padded_shapes = [4,])\n",
    "for x in ds_padded_batch:\n",
    "    print(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[1 2 0 0]\n",
    " [3 4 5 0]], shape=(2, 4), dtype=int32)\n",
    "tf.Tensor(\n",
    "[[6 7 0 0]\n",
    " [8 0 0 0]], shape=(2, 4), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window:æ„å»ºæ»‘åŠ¨çª—å£ï¼Œè¿”å›Dataset of Dataset.\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "#windowè¿”å›çš„æ˜¯Dataset of Dataset,å¯ä»¥ç”¨flat_mapå‹å¹³\n",
    "ds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3,drop_remainder=True)) \n",
    "for x in ds_window:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
    "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
    "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
    "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
    "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n",
    "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle:æ•°æ®é¡ºåºæ´—ç‰Œã€‚\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shuffle = ds.shuffle(buffer_size = 5)\n",
    "for x in ds_shuffle:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(6, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(11, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(9, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "tf.Tensor(8, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat:é‡å¤æ•°æ®è‹¥å¹²æ¬¡ï¼Œä¸å¸¦å‚æ•°æ—¶ï¼Œé‡å¤æ— æ•°æ¬¡ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.range(3)\n",
    "ds_repeat = ds.repeat(3)\n",
    "for x in ds_repeat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shard:é‡‡æ ·ï¼Œä»æŸä¸ªä½ç½®å¼€å§‹éš”å›ºå®šè·ç¦»é‡‡æ ·ä¸€ä¸ªå…ƒç´ ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shard = ds.shard(3,index = 1)\n",
    "\n",
    "for x in ds_shard:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take:é‡‡æ ·ï¼Œä»å¼€å§‹ä½ç½®å–å‰å‡ ä¸ªå…ƒç´ ã€‚\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_take = ds.take(3)\n",
    "\n",
    "list(ds_take.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 1, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œæå‡ç®¡é“æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸å¸¸ä¼šéå¸¸è€—æ—¶ã€‚\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒçš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ª**æ•°æ®å‡†å¤‡**ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ª**å‚æ•°è¿­ä»£**ã€‚\n",
    "\n",
    "å‚æ•°è¿­ä»£è¿‡ç¨‹çš„è€—æ—¶é€šå¸¸ä¾èµ–äºGPUæ¥æå‡ã€‚\n",
    "\n",
    "è€Œæ•°æ®å‡†å¤‡è¿‡ç¨‹çš„è€—æ—¶åˆ™å¯ä»¥é€šè¿‡æ„å»ºé«˜æ•ˆçš„æ•°æ®ç®¡é“è¿›è¡Œæå‡ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€äº›æ„å»ºé«˜æ•ˆæ•°æ®ç®¡é“çš„å»ºè®®ã€‚\n",
    "\n",
    "* 1ï¼Œä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚\n",
    "\n",
    "* 2ï¼Œä½¿ç”¨ interleave æ–¹æ³•å¯ä»¥è®©æ•°æ®è¯»å–è¿‡ç¨‹å¤šè¿›ç¨‹æ‰§è¡Œ,å¹¶å°†ä¸åŒæ¥æºæ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚\n",
    "\n",
    "* 3ï¼Œä½¿ç”¨ map æ—¶è®¾ç½®num_parallel_calls è®©æ•°æ®è½¬æ¢è¿‡ç¨‹å¤šè¿›è¡Œæ‰§è¡Œã€‚\n",
    "\n",
    "* 4ï¼Œä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚\n",
    "\n",
    "* 5ï¼Œä½¿ç”¨ mapè½¬æ¢æ—¶ï¼Œå…ˆbatch, ç„¶åé‡‡ç”¨å‘é‡åŒ–çš„è½¬æ¢æ–¹æ³•å¯¹æ¯ä¸ªbatchè¿›è¡Œè½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ï¼Œä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹é»˜è®¤æƒ…å†µä¸‹æ˜¯ä¸²è¡Œçš„ã€‚\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡\n",
    "def generator():\n",
    "    for i in range(10):\n",
    "        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£\n",
    "def train_step():\n",
    "    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦1s\n",
    "    time.sleep(1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ 10*2+10*1+ = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for x in ds:\n",
    "    train_step()  \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚\n",
    "\n",
    "# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ max(10*2,10*1) = 20s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training with prefetch...\"))\n",
    "\n",
    "# tf.data.experimental.AUTOTUNE å¯ä»¥è®©ç¨‹åºè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å‚æ•°\n",
    "for x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):\n",
    "    train_step()  \n",
    "    \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ï¼Œä½¿ç”¨ interleave æ–¹æ³•å¯ä»¥è®©æ•°æ®è¯»å–è¿‡ç¨‹å¤šè¿›ç¨‹æ‰§è¡Œ,å¹¶å°†ä¸åŒæ¥æºæ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(4):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(8):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'181,0,3,\"Sage, Miss. Constance Gladys\",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'405,0,3,\"Oreskovic, Miss. Marija\",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'635,0,3,\"Skoog, Miss. Mabel\",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'701,1,1,\"Astor, Mrs. John Jacob (Madeleine Talmadge Force)\",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3ï¼Œä½¿ç”¨ map æ—¶è®¾ç½®num_parallel_calls è®©æ•°æ®è½¬æ¢è¿‡ç¨‹å¤šè¿›è¡Œæ‰§è¡Œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å•è¿›ç¨‹è½¬æ¢\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start transformation...\"))\n",
    "\n",
    "ds_map = ds.map(load_image)\n",
    "for _ in ds_map:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¤šè¿›ç¨‹è½¬æ¢\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start parallel transformation...\"))\n",
    "\n",
    "ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "for _ in ds_map_parallel:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end parallel transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4ï¼Œä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£\n",
    "def train_step():\n",
    "    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦0s\n",
    "    pass\n",
    "\n",
    "# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ (5*2+5*0)*3 = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "\n",
    "# ä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚\n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()\n",
    "\n",
    "# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£\n",
    "def train_step():\n",
    "    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦0s\n",
    "    time.sleep(0) \n",
    "\n",
    "# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ (5*2+5*0)+(5*0+5*0)*2 = 10s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5ï¼Œä½¿ç”¨ mapè½¬æ¢æ—¶ï¼Œå…ˆbatch, ç„¶åé‡‡ç”¨å‘é‡åŒ–çš„è½¬æ¢æ–¹æ³•å¯¹æ¯ä¸ªbatchè¿›è¡Œè½¬æ¢ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…ˆmapåbatch\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_map_batch = ds.map(lambda x:x**2).batch(20)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start scalar transformation...\"))\n",
    "for x in ds_map_batch:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end scalar transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…ˆbatchåmap\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_batch_map = ds.batch(20).map(lambda x:x**2)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start vector transformation...\"))\n",
    "for x in ds_batch_map:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end vector transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-2,ç‰¹å¾åˆ—feature_column\n",
    "\n",
    "ç‰¹å¾åˆ— é€šå¸¸ç”¨äºå¯¹ç»“æ„åŒ–æ•°æ®å®æ–½ç‰¹å¾å·¥ç¨‹æ—¶å€™ä½¿ç”¨ï¼Œå›¾åƒæˆ–è€…æ–‡æœ¬æ•°æ®ä¸€èˆ¬ä¸ä¼šç”¨åˆ°ç‰¹å¾åˆ—ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œç‰¹å¾åˆ—ç”¨æ³•æ¦‚è¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ç‰¹å¾åˆ—å¯ä»¥å°†ç±»åˆ«ç‰¹å¾è½¬æ¢ä¸ºone-hotç¼–ç ç‰¹å¾ï¼Œå°†è¿ç»­ç‰¹å¾æ„å»ºåˆ†æ¡¶ç‰¹å¾ï¼Œä»¥åŠå¯¹å¤šä¸ªç‰¹å¾ç”Ÿæˆäº¤å‰ç‰¹å¾ç­‰ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦åˆ›å»ºç‰¹å¾åˆ—ï¼Œè¯·è°ƒç”¨ tf.feature_column æ¨¡å—çš„å‡½æ•°ã€‚è¯¥æ¨¡å—ä¸­å¸¸ç”¨çš„ä¹ä¸ªå‡½æ•°å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ‰€æœ‰ä¹ä¸ªå‡½æ•°éƒ½ä¼šè¿”å›ä¸€ä¸ª Categorical-Column æˆ–ä¸€ä¸ª \n",
    "Dense-Column å¯¹è±¡ï¼Œä½†å´ä¸ä¼šè¿”å› bucketized_columnï¼Œåè€…ç»§æ‰¿è‡ªè¿™ä¸¤ä¸ªç±»ã€‚\n",
    "\n",
    "æ³¨æ„ï¼šæ‰€æœ‰çš„Catogorical Columnç±»å‹æœ€ç»ˆéƒ½è¦é€šè¿‡indicator_columnè½¬æ¢æˆDense Columnç±»å‹æ‰èƒ½ä¼ å…¥æ¨¡å‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/ç‰¹å¾åˆ—9ç§.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numeric_column æ•°å€¼åˆ—ï¼Œæœ€å¸¸ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bucketized_column åˆ†æ¡¶åˆ—ï¼Œç”±æ•°å€¼åˆ—ç”Ÿæˆï¼Œå¯ä»¥ç”±ä¸€ä¸ªæ•°å€¼åˆ—å‡ºå¤šä¸ªç‰¹å¾ï¼Œone-hotç¼–ç ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_identity åˆ†ç±»æ ‡è¯†åˆ—ï¼Œone-hotç¼–ç ï¼Œç›¸å½“äºåˆ†æ¡¶åˆ—æ¯ä¸ªæ¡¶ä¸º1ä¸ªæ•´æ•°çš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_vocabulary_list åˆ†ç±»è¯æ±‡åˆ—ï¼Œone-hotç¼–ç ï¼Œç”±listæŒ‡å®šè¯å…¸ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_vocabulary_file åˆ†ç±»è¯æ±‡åˆ—ï¼Œç”±æ–‡ä»¶fileæŒ‡å®šè¯å…¸ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_hash_bucket å“ˆå¸Œåˆ—ï¼Œæ•´æ•°æˆ–è¯å…¸è¾ƒå¤§æ—¶é‡‡ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* indicator_column æŒ‡æ ‡åˆ—ï¼Œç”±Categorical Columnç”Ÿæˆï¼Œone-hotç¼–ç "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embedding_column åµŒå…¥åˆ—ï¼Œç”±Categorical Columnç”Ÿæˆï¼ŒåµŒå…¥çŸ¢é‡åˆ†å¸ƒå‚æ•°éœ€è¦å­¦ä¹ ã€‚åµŒå…¥çŸ¢é‡ç»´æ•°å»ºè®®å–ç±»åˆ«æ•°é‡çš„ 4 æ¬¡æ–¹æ ¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crossed_column äº¤å‰åˆ—ï¼Œå¯ä»¥ç”±é™¤categorical_column_with_hash_bucketçš„ä»»æ„åˆ†ç±»åˆ—æ„æˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œç‰¹å¾åˆ—ä½¿ç”¨èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ç‰¹å¾åˆ—è§£å†³Titanicç”Ÿå­˜é—®é¢˜çš„å®Œæ•´èŒƒä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "\n",
    "#æ‰“å°æ—¥å¿—\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(info+'...\\n\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# ä¸€ï¼Œæ„å»ºæ•°æ®ç®¡é“\n",
    "#================================================================================\n",
    "printlog(\"step1: prepare dataset...\")\n",
    "\n",
    "\n",
    "dftrain_raw = pd.read_csv(\"./data/titanic/train.csv\")\n",
    "dftest_raw = pd.read_csv(\"./data/titanic/test.csv\")\n",
    "\n",
    "dfraw = pd.concat([dftrain_raw,dftest_raw])\n",
    "\n",
    "def prepare_dfdata(dfraw):\n",
    "    dfdata = dfraw.copy()\n",
    "    dfdata.columns = [x.lower() for x in dfdata.columns]\n",
    "    dfdata = dfdata.rename(columns={'survived':'label'})\n",
    "    dfdata = dfdata.drop(['passengerid','name'],axis = 1)\n",
    "    for col,dtype in dict(dfdata.dtypes).items():\n",
    "        # åˆ¤æ–­æ˜¯å¦åŒ…å«ç¼ºå¤±å€¼\n",
    "        if dfdata[col].hasnans:\n",
    "            # æ·»åŠ æ ‡è¯†æ˜¯å¦ç¼ºå¤±åˆ—\n",
    "            dfdata[col + '_nan'] = pd.isna(dfdata[col]).astype('int32')\n",
    "            # å¡«å……\n",
    "            if dtype not in [np.object,np.str,np.unicode]:\n",
    "                dfdata[col].fillna(dfdata[col].mean(),inplace = True)\n",
    "            else:\n",
    "                dfdata[col].fillna('',inplace = True)\n",
    "    return(dfdata)\n",
    "\n",
    "dfdata = prepare_dfdata(dfraw)\n",
    "dftrain = dfdata.iloc[0:len(dftrain_raw),:]\n",
    "dftest = dfdata.iloc[len(dftrain_raw):,:]\n",
    "\n",
    "\n",
    "\n",
    "# ä» dataframe å¯¼å…¥æ•°æ® \n",
    "def df_to_dataset(df, shuffle=True, batch_size=32):\n",
    "    dfdata = df.copy()\n",
    "    if 'label' not in dfdata.columns:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = 'list'))\n",
    "    else: \n",
    "        labels = dfdata.pop('label')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = 'list'), labels))  \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dfdata))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "ds_train = df_to_dataset(dftrain)\n",
    "ds_test = df_to_dataset(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# äºŒï¼Œå®šä¹‰ç‰¹å¾åˆ—\n",
    "#================================================================================\n",
    "printlog(\"step2: make feature columns...\")\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# æ•°å€¼åˆ—\n",
    "for col in ['age','fare','parch','sibsp'] + [\n",
    "    c for c in dfdata.columns if c.endswith('_nan')]:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))\n",
    "\n",
    "# åˆ†æ¡¶åˆ—\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "age_buckets = tf.feature_column.bucketized_column(age, \n",
    "             boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# ç±»åˆ«åˆ—\n",
    "# æ³¨æ„ï¼šæ‰€æœ‰çš„Catogorical Columnç±»å‹æœ€ç»ˆéƒ½è¦é€šè¿‡indicator_columnè½¬æ¢æˆDense Columnç±»å‹æ‰èƒ½ä¼ å…¥æ¨¡å‹ï¼ï¼\n",
    "sex = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='sex',vocabulary_list=[\"male\", \"female\"]))\n",
    "feature_columns.append(sex)\n",
    "\n",
    "pclass = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='pclass',vocabulary_list=[1,2,3]))\n",
    "feature_columns.append(pclass)\n",
    "\n",
    "ticket = tf.feature_column.indicator_column(\n",
    "     tf.feature_column.categorical_column_with_hash_bucket('ticket',3))\n",
    "feature_columns.append(ticket)\n",
    "\n",
    "embarked = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='embarked',vocabulary_list=['S','C','B']))\n",
    "feature_columns.append(embarked)\n",
    "\n",
    "# åµŒå…¥åˆ—\n",
    "cabin = tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('cabin',32),2)\n",
    "feature_columns.append(cabin)\n",
    "\n",
    "# äº¤å‰åˆ—\n",
    "pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "          key='pclass',vocabulary_list=[1,2,3])\n",
    "\n",
    "crossed_feature = tf.feature_column.indicator_column(\n",
    "    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=15))\n",
    "\n",
    "feature_columns.append(crossed_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# ä¸‰ï¼Œå®šä¹‰æ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step3: define model...\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "  layers.DenseFeatures(feature_columns), #å°†ç‰¹å¾åˆ—æ”¾å…¥åˆ°tf.keras.layers.DenseFeaturesä¸­!!!\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# å››ï¼Œè®­ç»ƒæ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step4: train model...\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(ds_train,\n",
    "          validation_data=ds_test,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# äº”ï¼Œè¯„ä¼°æ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step5: eval model...\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(history,\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_features (DenseFeature multiple                  64        \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  3008      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              multiple                  4160      \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              multiple                  65        \n",
    "=================================================================\n",
    "Total params: 7,297\n",
    "Trainable params: 7,297\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-2-01-æ¨¡å‹è¯„ä¼°.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-3,æ¿€æ´»å‡½æ•°activation\n",
    "\n",
    "æ¿€æ´»å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­æ‰®æ¼”ç€éå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒç»™ç½‘ç»œèµ‹äºˆäº†éçº¿æ€§ï¼Œä»è€Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿæ‹Ÿåˆä»»æ„å¤æ‚çš„å‡½æ•°ã€‚\n",
    "\n",
    "å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ— è®ºå¤šå¤æ‚çš„ç½‘ç»œï¼Œéƒ½ç­‰ä»·äºå•ä¸€çš„çº¿æ€§å˜æ¢ï¼Œæ— æ³•å¯¹éçº¿æ€§å‡½æ•°è¿›è¡Œæ‹Ÿåˆã€‚\n",
    "\n",
    "ç›®å‰ï¼Œæ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ä¸º relu, ä½†ä¹Ÿæœ‰äº›æ–°æ¨å‡ºçš„æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚ swishã€GELU æ®ç§°æ•ˆæœä¼˜äºreluæ¿€æ´»å‡½æ•°ã€‚\n",
    "\n",
    "æ¿€æ´»å‡½æ•°çš„ç»¼è¿°ä»‹ç»å¯ä»¥å‚è€ƒä¸‹é¢ä¸¤ç¯‡æ–‡ç« ã€‚\n",
    "\n",
    "[ã€Šä¸€æ–‡æ¦‚è§ˆæ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°ã€‹](https://zhuanlan.zhihu.com/p/98472075)\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/98472075\n",
    "\n",
    "[ã€Šä»ReLUåˆ°GELU,ä¸€æ–‡æ¦‚è§ˆç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°ã€‹](https://zhuanlan.zhihu.com/p/98863801)\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/98863801\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå¸¸ç”¨æ¿€æ´»å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.sigmoidï¼šå°†å®æ•°å‹ç¼©åˆ°0åˆ°1ä¹‹é—´ï¼Œä¸€èˆ¬åªåœ¨äºŒåˆ†ç±»çš„æœ€åè¾“å‡ºå±‚ä½¿ç”¨ã€‚ä¸»è¦ç¼ºé™·ä¸ºå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œè¾“å‡ºä¸ä»¥0ä¸ºä¸­å¿ƒã€‚\n",
    "\n",
    "![](./data/sigmoid.png)\n",
    "\n",
    "* tf.nn.softmaxï¼šsigmoidçš„å¤šåˆ†ç±»æ‰©å±•ï¼Œä¸€èˆ¬åªåœ¨å¤šåˆ†ç±»é—®é¢˜çš„æœ€åè¾“å‡ºå±‚ä½¿ç”¨ã€‚\n",
    "\n",
    "![](./data/softmaxè¯´æ˜.jpg)\n",
    "\n",
    "* tf.nn.tanhï¼šå°†å®æ•°å‹ç¼©åˆ°-1åˆ°1ä¹‹é—´ï¼Œè¾“å‡ºæœŸæœ›ä¸º0ã€‚ä¸»è¦ç¼ºé™·ä¸ºå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ã€‚\n",
    "\n",
    "![](./data/tanh.png)\n",
    "\n",
    "* tf.nn.reluï¼šä¿®æ­£çº¿æ€§å•å…ƒï¼Œæœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ã€‚ä¸€èˆ¬éšè—å±‚ä½¿ç”¨ã€‚ä¸»è¦ç¼ºé™·æ˜¯ï¼šè¾“å‡ºä¸ä»¥0ä¸ºä¸­å¿ƒï¼Œè¾“å…¥å°äº0æ—¶å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(æ­»äº¡relu)ã€‚\n",
    "\n",
    "![](./data/relu.png)\n",
    "\n",
    "* tf.nn.leaky_reluï¼šå¯¹ä¿®æ­£çº¿æ€§å•å…ƒçš„æ”¹è¿›ï¼Œè§£å†³äº†æ­»äº¡relué—®é¢˜ã€‚\n",
    "\n",
    "![](./data/leaky_relu.png)\n",
    "\n",
    "* tf.nn.eluï¼šæŒ‡æ•°çº¿æ€§å•å…ƒã€‚å¯¹reluçš„æ”¹è¿›ï¼Œèƒ½å¤Ÿç¼“è§£æ­»äº¡relué—®é¢˜ã€‚\n",
    "\n",
    "![](./data/elu.png)\n",
    "\n",
    "* tf.nn.seluï¼šæ‰©å±•å‹æŒ‡æ•°çº¿æ€§å•å…ƒã€‚åœ¨æƒé‡ç”¨tf.keras.initializers.lecun_normalåˆå§‹åŒ–å‰æä¸‹èƒ½å¤Ÿå¯¹ç¥ç»ç½‘ç»œè¿›è¡Œè‡ªå½’ä¸€åŒ–ã€‚ä¸å¯èƒ½å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚éœ€è¦å’ŒDropoutçš„å˜ç§AlphaDropoutä¸€èµ·ä½¿ç”¨ã€‚\n",
    "\n",
    "![](./data/selu.png)\n",
    "\n",
    "* tf.nn.swishï¼šè‡ªé—¨æ§æ¿€æ´»å‡½æ•°ã€‚è°·æ­Œå‡ºå“ï¼Œç›¸å…³ç ”ç©¶æŒ‡å‡ºç”¨swishæ›¿ä»£reluå°†è·å¾—è½»å¾®æ•ˆæœæå‡ã€‚\n",
    "\n",
    "![](./data/swish.png)\n",
    "\n",
    "* geluï¼šé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°ã€‚åœ¨Transformerä¸­è¡¨ç°æœ€å¥½ã€‚tf.nnæ¨¡å—å°šæ²¡æœ‰å®ç°è¯¥å‡½æ•°ã€‚\n",
    "\n",
    "![](./data/gelu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œåœ¨æ¨¡å‹ä¸­ä½¿ç”¨æ¿€æ´»å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨kerasæ¨¡å‹ä¸­ä½¿ç”¨æ¿€æ´»å‡½æ•°ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯ä½œä¸ºæŸäº›å±‚çš„activationå‚æ•°æŒ‡å®šï¼Œå¦ä¸€ç§æ˜¯æ˜¾å¼æ·»åŠ layers.Activationæ¿€æ´»å±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) #é€šè¿‡activationå‚æ•°æŒ‡å®š\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Activation(tf.nn.softmax))  # æ˜¾å¼æ·»åŠ layers.Activationæ¿€æ´»å±‚\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-4,æ¨¡å‹å±‚layers\n",
    "\n",
    "æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èˆ¬ç”±å„ç§æ¨¡å‹å±‚ç»„åˆè€Œæˆã€‚\n",
    "\n",
    "tf.keras.layerså†…ç½®äº†éå¸¸ä¸°å¯Œçš„å„ç§åŠŸèƒ½çš„æ¨¡å‹å±‚ã€‚ä¾‹å¦‚ï¼Œ\n",
    "\n",
    "layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout\n",
    "\n",
    "layers.Conv2D,layers.MaxPooling2D,layers.Conv1D\n",
    "\n",
    "layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectionalç­‰ç­‰ã€‚\n",
    "\n",
    "å¦‚æœè¿™äº›å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç¼–å†™tf.keras.LambdaåŒ¿åæ¨¡å‹å±‚æˆ–ç»§æ‰¿tf.keras.layers.LayeråŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚\n",
    "\n",
    "å…¶ä¸­tf.keras.LambdaåŒ¿åæ¨¡å‹å±‚åªé€‚ç”¨äºæ„é€ æ²¡æœ‰å­¦ä¹ å‚æ•°çš„æ¨¡å‹å±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå†…ç½®æ¨¡å‹å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€äº›å¸¸ç”¨çš„å†…ç½®æ¨¡å‹å±‚ç®€å•ä»‹ç»å¦‚ä¸‹ã€‚\n",
    "\n",
    "**åŸºç¡€å±‚**\n",
    "\n",
    "* Denseï¼šå¯†é›†è¿æ¥å±‚ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥å±‚ç‰¹å¾æ•°Ã— è¾“å‡ºå±‚ç‰¹å¾æ•°(weight)ï¼‹ è¾“å‡ºå±‚ç‰¹å¾æ•°(bias)\n",
    "\n",
    "* Activationï¼šæ¿€æ´»å‡½æ•°å±‚ã€‚ä¸€èˆ¬æ”¾åœ¨Denseå±‚åé¢ï¼Œç­‰ä»·äºåœ¨Denseå±‚ä¸­æŒ‡å®šactivationã€‚\n",
    "\n",
    "* Dropoutï¼šéšæœºç½®é›¶å±‚ã€‚è®­ç»ƒæœŸé—´ä»¥ä¸€å®šå‡ ç‡å°†è¾“å…¥ç½®0ï¼Œä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚\n",
    "\n",
    "* BatchNormalizationï¼šæ‰¹æ ‡å‡†åŒ–å±‚ã€‚é€šè¿‡çº¿æ€§å˜æ¢å°†è¾“å…¥æ‰¹æ¬¡ç¼©æ”¾å¹³ç§»åˆ°ç¨³å®šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚å¯ä»¥å¢å¼ºæ¨¡å‹å¯¹è¾“å…¥ä¸åŒåˆ†å¸ƒçš„é€‚åº”æ€§ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼Œæœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœã€‚ä¸€èˆ¬åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ä½¿ç”¨ã€‚\n",
    "\n",
    "* SpatialDropout2Dï¼šç©ºé—´éšæœºç½®é›¶å±‚ã€‚è®­ç»ƒæœŸé—´ä»¥ä¸€å®šå‡ ç‡å°†æ•´ä¸ªç‰¹å¾å›¾ç½®0ï¼Œä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µï¼Œæœ‰åˆ©äºé¿å…ç‰¹å¾å›¾ä¹‹é—´è¿‡é«˜çš„ç›¸å…³æ€§ã€‚\n",
    "\n",
    "* Inputï¼šè¾“å…¥å±‚ã€‚é€šå¸¸ä½¿ç”¨Functional APIæ–¹å¼æ„å»ºæ¨¡å‹æ—¶ä½œä¸ºç¬¬ä¸€å±‚ã€‚\n",
    "\n",
    "* DenseFeatureï¼šç‰¹å¾åˆ—æ¥å…¥å±‚ï¼Œç”¨äºæ¥æ”¶ä¸€ä¸ªç‰¹å¾åˆ—åˆ—è¡¨å¹¶äº§ç”Ÿä¸€ä¸ªå¯†é›†è¿æ¥å±‚ã€‚\n",
    "\n",
    "* Flattenï¼šå‹å¹³å±‚ï¼Œç”¨äºå°†å¤šç»´å¼ é‡å‹æˆä¸€ç»´ã€‚\n",
    "\n",
    "* Reshapeï¼šå½¢çŠ¶é‡å¡‘å±‚ï¼Œæ”¹å˜è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚\n",
    "\n",
    "* Concatenateï¼šæ‹¼æ¥å±‚ï¼Œå°†å¤šä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šæ‹¼æ¥ã€‚\n",
    "\n",
    "* Addï¼šåŠ æ³•å±‚ã€‚\n",
    "\n",
    "* Subtractï¼š å‡æ³•å±‚ã€‚\n",
    "\n",
    "* Maximumï¼šå–æœ€å¤§å€¼å±‚ã€‚\n",
    "\n",
    "* Minimumï¼šå–æœ€å°å€¼å±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å·ç§¯ç½‘ç»œç›¸å…³å±‚**\n",
    "\n",
    "* Conv1Dï¼šæ™®é€šä¸€ç»´å·ç§¯ï¼Œå¸¸ç”¨äºæ–‡æœ¬ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3)Ã—å·ç§¯æ ¸ä¸ªæ•°\n",
    "\n",
    "* Conv2Dï¼šæ™®é€šäºŒç»´å·ç§¯ï¼Œå¸¸ç”¨äºå›¾åƒã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•°\n",
    "\n",
    "* Conv3Dï¼šæ™®é€šä¸‰ç»´å·ç§¯ï¼Œå¸¸ç”¨äºè§†é¢‘ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•°\n",
    "\n",
    "* SeparableConv2Dï¼šäºŒç»´æ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚ã€‚ä¸åŒäºæ™®é€šå·ç§¯åŒæ—¶å¯¹åŒºåŸŸå’Œé€šé“æ“ä½œï¼Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯å…ˆæ“ä½œåŒºåŸŸï¼Œå†æ“ä½œé€šé“ã€‚å³å…ˆå¯¹æ¯ä¸ªé€šé“åšç‹¬ç«‹å·å³å…ˆæ“ä½œåŒºåŸŸï¼Œå†ç”¨1ä¹˜1å·ç§¯è·¨é€šé“ç»„åˆå³å†æ“ä½œé€šé“ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸ + è¾“å…¥é€šé“æ•°Ã—1Ã—1Ã—è¾“å‡ºé€šé“æ•°ã€‚æ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„å‚æ•°æ•°é‡ä¸€èˆ¬è¿œå°äºæ™®é€šå·ç§¯ï¼Œæ•ˆæœä¸€èˆ¬ä¹Ÿæ›´å¥½ã€‚\n",
    "\n",
    "* DepthwiseConv2Dï¼šäºŒç»´æ·±åº¦å·ç§¯å±‚ã€‚ä»…æœ‰SeparableConv2Då‰åŠéƒ¨åˆ†æ“ä½œï¼Œå³åªæ“ä½œåŒºåŸŸï¼Œä¸æ“ä½œé€šé“ï¼Œä¸€èˆ¬è¾“å‡ºé€šé“æ•°å’Œè¾“å…¥é€šé“æ•°ç›¸åŒï¼Œä½†ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®depth_multiplierè®©è¾“å‡ºé€šé“ä¸ºè¾“å…¥é€šé“çš„è‹¥å¹²å€æ•°ã€‚è¾“å‡ºé€šé“æ•° = è¾“å…¥é€šé“æ•° Ã— depth_multiplierã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸Ã— depth_multiplierã€‚\n",
    "\n",
    "* Conv2DTransposeï¼šäºŒç»´å·ç§¯è½¬ç½®å±‚ï¼Œä¿—ç§°åå·ç§¯å±‚ã€‚å¹¶éå·ç§¯çš„é€†æ“ä½œï¼Œä½†åœ¨å·ç§¯æ ¸ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå½“å…¶è¾“å…¥å°ºå¯¸æ˜¯å·ç§¯æ“ä½œè¾“å‡ºå°ºå¯¸çš„æƒ…å†µä¸‹ï¼Œå·ç§¯è½¬ç½®çš„è¾“å‡ºå°ºå¯¸æ°å¥½æ˜¯å·ç§¯æ“ä½œçš„è¾“å…¥å°ºå¯¸ã€‚\n",
    "\n",
    "* LocallyConnected2D: äºŒç»´å±€éƒ¨è¿æ¥å±‚ã€‚ç±»ä¼¼Conv2Dï¼Œå”¯ä¸€çš„å·®åˆ«æ˜¯æ²¡æœ‰ç©ºé—´ä¸Šçš„æƒå€¼å…±äº«ï¼Œæ‰€ä»¥å…¶å‚æ•°ä¸ªæ•°è¿œé«˜äºäºŒç»´å·ç§¯ã€‚\n",
    "\n",
    "* MaxPooling2D: äºŒç»´æœ€å¤§æ± åŒ–å±‚ã€‚ä¹Ÿç§°ä½œä¸‹é‡‡æ ·å±‚ã€‚æ± åŒ–å±‚æ— å‚æ•°ï¼Œä¸»è¦ä½œç”¨æ˜¯é™ç»´ã€‚\n",
    "\n",
    "* AveragePooling2D: äºŒç»´å¹³å‡æ± åŒ–å±‚ã€‚\n",
    "\n",
    "* GlobalMaxPool2D: å…¨å±€æœ€å¤§æ± åŒ–å±‚ã€‚æ¯ä¸ªé€šé“ä»…ä¿ç•™ä¸€ä¸ªå€¼ã€‚ä¸€èˆ¬ä»å·ç§¯å±‚è¿‡æ¸¡åˆ°å…¨è¿æ¥å±‚æ—¶ä½¿ç”¨ï¼Œæ˜¯Flattençš„æ›¿ä»£æ–¹æ¡ˆã€‚\n",
    "\n",
    "* GlobalAvgPool2D: å…¨å±€å¹³å‡æ± åŒ–å±‚ã€‚æ¯ä¸ªé€šé“ä»…ä¿ç•™ä¸€ä¸ªå€¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¾ªç¯ç½‘ç»œç›¸å…³å±‚**\n",
    "\n",
    "* Embeddingï¼šåµŒå…¥å±‚ã€‚ä¸€ç§æ¯”Onehotæ›´åŠ æœ‰æ•ˆçš„å¯¹ç¦»æ•£ç‰¹å¾è¿›è¡Œç¼–ç çš„æ–¹æ³•ã€‚ä¸€èˆ¬ç”¨äºå°†è¾“å…¥ä¸­çš„å•è¯æ˜ å°„ä¸ºç¨ å¯†å‘é‡ã€‚åµŒå…¥å±‚çš„å‚æ•°éœ€è¦å­¦ä¹ ã€‚\n",
    "\n",
    "* LSTMï¼šé•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€‚æœ€æ™®éä½¿ç”¨çš„å¾ªç¯ç½‘ç»œå±‚ã€‚å…·æœ‰æºå¸¦è½¨é“ï¼Œé—å¿˜é—¨ï¼Œæ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨ã€‚å¯ä»¥è¾ƒä¸ºæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚è®¾ç½®return_sequences = Trueæ—¶å¯ä»¥è¿”å›å„ä¸ªä¸­é—´æ­¥éª¤è¾“å‡ºï¼Œå¦åˆ™åªè¿”å›æœ€ç»ˆè¾“å‡ºã€‚\n",
    "\n",
    "* GRUï¼šé—¨æ§å¾ªç¯ç½‘ç»œå±‚ã€‚LSTMçš„ä½é…ç‰ˆï¼Œä¸å…·æœ‰æºå¸¦è½¨é“ï¼Œå‚æ•°æ•°é‡å°‘äºLSTMï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚\n",
    "\n",
    "* SimpleRNNï¼šç®€å•å¾ªç¯ç½‘ç»œå±‚ã€‚å®¹æ˜“å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸èƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚\n",
    "\n",
    "* ConvLSTM2Dï¼šå·ç§¯é•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€‚ç»“æ„ä¸Šç±»ä¼¼LSTMï¼Œä½†å¯¹è¾“å…¥çš„è½¬æ¢æ“ä½œå’Œå¯¹çŠ¶æ€çš„è½¬æ¢æ“ä½œéƒ½æ˜¯å·ç§¯è¿ç®—ã€‚\n",
    "\n",
    "* Bidirectionalï¼šåŒå‘å¾ªç¯ç½‘ç»œåŒ…è£…å™¨ã€‚å¯ä»¥å°†LSTMï¼ŒGRUç­‰å±‚åŒ…è£…æˆåŒå‘å¾ªç¯ç½‘ç»œã€‚ä»è€Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚\n",
    "\n",
    "* RNNï¼šRNNåŸºæœ¬å±‚ã€‚æ¥å—ä¸€ä¸ªå¾ªç¯ç½‘ç»œå•å…ƒæˆ–ä¸€ä¸ªå¾ªç¯å•å…ƒåˆ—è¡¨ï¼Œé€šè¿‡è°ƒç”¨tf.keras.backend.rnnå‡½æ•°åœ¨åºåˆ—ä¸Šè¿›è¡Œè¿­ä»£ä»è€Œè½¬æ¢æˆå¾ªç¯ç½‘ç»œå±‚ã€‚\n",
    "\n",
    "* LSTMCellï¼šLSTMå•å…ƒã€‚å’ŒLSTMåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚å¯ä»¥ç®€å•ç†è§£LSTMå³RNNåŸºæœ¬å±‚åŒ…è£¹LSTMCellã€‚\n",
    "\n",
    "* GRUCellï¼šGRUå•å…ƒã€‚å’ŒGRUåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚\n",
    "\n",
    "* SimpleRNNCellï¼šSimpleRNNå•å…ƒã€‚å’ŒSimpleRNNåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚\n",
    "\n",
    "* AbstractRNNCellï¼šæŠ½è±¡RNNå•å…ƒã€‚é€šè¿‡å¯¹å®ƒçš„å­ç±»åŒ–ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰RNNå•å…ƒï¼Œå†é€šè¿‡RNNåŸºæœ¬å±‚çš„åŒ…è£¹å®ç°ç”¨æˆ·è‡ªå®šä¹‰å¾ªç¯ç½‘ç»œå±‚ã€‚\n",
    "\n",
    "* Attentionï¼šDot-productç±»å‹æ³¨æ„åŠ›æœºåˆ¶å±‚ã€‚å¯ä»¥ç”¨äºæ„å»ºæ³¨æ„åŠ›æ¨¡å‹ã€‚\n",
    "\n",
    "* AdditiveAttentionï¼šAdditiveç±»å‹æ³¨æ„åŠ›æœºåˆ¶å±‚ã€‚å¯ä»¥ç”¨äºæ„å»ºæ³¨æ„åŠ›æ¨¡å‹ã€‚\n",
    "\n",
    "* TimeDistributedï¼šæ—¶é—´åˆ†å¸ƒåŒ…è£…å™¨ã€‚åŒ…è£…åå¯ä»¥å°†Denseã€Conv2Dç­‰ä½œç”¨åˆ°æ¯ä¸€ä¸ªæ—¶é—´ç‰‡æ®µä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œè‡ªå®šä¹‰æ¨¡å‹å±‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœè‡ªå®šä¹‰æ¨¡å‹å±‚æ²¡æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨Lamdaå±‚å®ç°ã€‚\n",
    "\n",
    "å¦‚æœè‡ªå®šä¹‰æ¨¡å‹å±‚æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œåˆ™å¯ä»¥é€šè¿‡å¯¹LayeråŸºç±»å­ç±»åŒ–å®ç°ã€‚\n",
    "\n",
    "Lamdaå±‚ç”±äºæ²¡æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œåªéœ€è¦å®šä¹‰æ­£å‘ä¼ æ’­é€»è¾‘å³å¯ï¼Œä½¿ç”¨æ¯”LayeråŸºç±»å­ç±»åŒ–æ›´åŠ ç®€å•ã€‚\n",
    "\n",
    "Lamdaå±‚çš„æ­£å‘é€»è¾‘å¯ä»¥ä½¿ç”¨Pythonçš„lambdaå‡½æ•°æ¥è¡¨è¾¾ï¼Œä¹Ÿå¯ä»¥ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°æ¥è¡¨è¾¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,regularizers\n",
    "\n",
    "mypower = layers.Lambda(lambda x:tf.math.pow(x,2))\n",
    "mypower(tf.range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layerçš„å­ç±»åŒ–ä¸€èˆ¬éœ€è¦é‡æ–°å®ç°åˆå§‹åŒ–æ–¹æ³•ï¼ŒBuildæ–¹æ³•å’ŒCallæ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€åŒ–çš„çº¿æ€§å±‚çš„èŒƒä¾‹ï¼Œç±»ä¼¼Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    #buildæ–¹æ³•ä¸€èˆ¬å®šä¹‰Layeréœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ã€‚    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(Linear,self).build(input_shape) # ç›¸å½“äºè®¾ç½®self.built = True\n",
    "\n",
    "    #callæ–¹æ³•ä¸€èˆ¬å®šä¹‰æ­£å‘ä¼ æ’­è¿ç®—é€»è¾‘ï¼Œ__call__æ–¹æ³•è°ƒç”¨äº†å®ƒã€‚    \n",
    "    def call(self, inputs): \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    #å¦‚æœè¦è®©è‡ªå®šä¹‰çš„Layeré€šè¿‡Functional API ç»„åˆæˆæ¨¡å‹æ—¶å¯ä»¥åºåˆ—åŒ–ï¼Œéœ€è¦è‡ªå®šä¹‰get_configæ–¹æ³•ã€‚\n",
    "    def get_config(self):  \n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "#æŒ‡å®šinput_shapeï¼Œæ˜¾å¼è°ƒç”¨buildæ–¹æ³•ï¼Œç¬¬0ç»´ä»£è¡¨æ ·æœ¬æ•°é‡ï¼Œç”¨Noneå¡«å……\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.built)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.compute_output_shape(input_shape = (None,16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "(None, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 16)\n",
    "print(linear.built)\n",
    "#å¦‚æœbuilt = Falseï¼Œè°ƒç”¨__call__æ—¶ä¼šå…ˆè°ƒç”¨buildæ–¹æ³•, å†è°ƒç”¨callæ–¹æ³•ã€‚\n",
    "linear(tf.random.uniform((100,64))) \n",
    "print(linear.built)\n",
    "config = linear.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "True\n",
    "{'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "#æ³¨æ„è¯¥å¤„çš„input_shapeä¼šè¢«æ¨¡å‹åŠ å·¥ï¼Œæ— éœ€ä½¿ç”¨Noneä»£è¡¨æ ·æœ¬æ•°é‡ç»´\n",
    "model.add(Linear(units = 16,input_shape = (64,)))  \n",
    "print(\"model.input_shape: \",model.input_shape)\n",
    "print(\"model.output_shape: \",model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.input_shape:  (None, 64)\n",
    "model.output_shape:  (None, 16)\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "linear (Linear)              (None, 16)                1040      \n",
    "=================================================================\n",
    "Total params: 1,040\n",
    "Trainable params: 1,040\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-5,æŸå¤±å‡½æ•°losses\n",
    "\n",
    "ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚ï¼ˆObjective = Loss + Regularizationï¼‰\n",
    "\n",
    "å¯¹äºkerasæ¨¡å‹ï¼Œç›®æ ‡å‡½æ•°ä¸­çš„æ­£åˆ™åŒ–é¡¹ä¸€èˆ¬åœ¨å„å±‚ä¸­æŒ‡å®šï¼Œä¾‹å¦‚ä½¿ç”¨Denseçš„ kernel_regularizer å’Œ bias_regularizerç­‰å‚æ•°æŒ‡å®šæƒé‡ä½¿ç”¨l1æˆ–è€…l2æ­£åˆ™åŒ–é¡¹ï¼Œæ­¤å¤–è¿˜å¯ä»¥ç”¨kernel_constraint å’Œ bias_constraintç­‰å‚æ•°çº¦æŸæƒé‡çš„å–å€¼èŒƒå›´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚\n",
    "\n",
    "æŸå¤±å‡½æ•°åœ¨æ¨¡å‹ç¼–è¯‘æ—¶å€™æŒ‡å®šã€‚å¯¹äºå›å½’æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹æŸå¤±å‡½æ•° mean_squared_errorã€‚\n",
    "\n",
    "å¯¹äºäºŒåˆ†ç±»æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•° binary_crossentropyã€‚\n",
    "\n",
    "å¯¹äºå¤šåˆ†ç±»æ¨¡å‹ï¼Œå¦‚æœlabelæ˜¯ç±»åˆ«åºå·ç¼–ç çš„ï¼Œåˆ™ä½¿ç”¨ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•° categorical_crossentropyã€‚å¦‚æœlabelè¿›è¡Œäº†one-hotç¼–ç ï¼Œåˆ™éœ€è¦ä½¿ç”¨ç¨€ç–ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•° sparse_categorical_crossentropyã€‚\n",
    "\n",
    "å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒæŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01), \n",
    "                activity_regularizer=regularizers.l1(0.01),\n",
    "                kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) \n",
    "model.add(layers.Dense(10,\n",
    "        kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "        loss = \"sparse_categorical_crossentropy\",metrics = [\"AUC\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 4,810\n",
    "Trainable params: 4,810\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå†…ç½®æŸå¤±å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å†…ç½®çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰ç±»çš„å®ç°å’Œå‡½æ•°çš„å®ç°ä¸¤ç§å½¢å¼ã€‚\n",
    "\n",
    "å¦‚ï¼šCategoricalCrossentropy å’Œ categorical_crossentropy éƒ½æ˜¯ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå‰è€…æ˜¯ç±»çš„å®ç°å½¢å¼ï¼Œåè€…æ˜¯å‡½æ•°çš„å®ç°å½¢å¼ã€‚\n",
    "\n",
    "å¸¸ç”¨çš„ä¸€äº›å†…ç½®æŸå¤±å‡½æ•°è¯´æ˜å¦‚ä¸‹ã€‚\n",
    "\n",
    "* mean_squared_errorï¼ˆå¹³æ–¹å·®è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mse, ç±»å®ç°å½¢å¼ä¸º MeanSquaredError å’Œ MSEï¼‰\n",
    "\n",
    "* mean_absolute_error (ç»å¯¹å€¼è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mae, ç±»å®ç°å½¢å¼ä¸º MeanAbsoluteError å’Œ MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mape, ç±»å®ç°å½¢å¼ä¸º MeanAbsolutePercentageError å’Œ MAPE)\n",
    "\n",
    "* Huber(HuberæŸå¤±ï¼Œåªæœ‰ç±»å®ç°å½¢å¼ï¼Œç”¨äºå›å½’ï¼Œä»‹äºmseå’Œmaeä¹‹é—´ï¼Œå¯¹å¼‚å¸¸å€¼æ¯”è¾ƒé²æ£’ï¼Œç›¸å¯¹mseæœ‰ä¸€å®šçš„ä¼˜åŠ¿)\n",
    "\n",
    "* binary_crossentropy(äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œç±»å®ç°å½¢å¼ä¸º BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(ç±»åˆ«äº¤å‰ç†µï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºonehotç¼–ç ï¼Œç±»å®ç°å½¢å¼ä¸º CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(ç¨€ç–ç±»åˆ«äº¤å‰ç†µï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºåºå·ç¼–ç å½¢å¼ï¼Œç±»å®ç°å½¢å¼ä¸º SparseCategoricalCrossentropy)\n",
    "\n",
    "* hinge(åˆé¡µæŸå¤±å‡½æ•°ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œæœ€è‘—åçš„åº”ç”¨æ˜¯ä½œä¸ºæ”¯æŒå‘é‡æœºSVMçš„æŸå¤±å‡½æ•°ï¼Œç±»å®ç°å½¢å¼ä¸º Hinge)\n",
    "\n",
    "* kld(ç›¸å¯¹ç†µæŸå¤±ï¼Œä¹Ÿå«KLæ•£åº¦ï¼Œå¸¸ç”¨äºæœ€å¤§æœŸæœ›ç®—æ³•EMçš„æŸå¤±å‡½æ•°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„ä¸€ç§ä¿¡æ¯åº¦é‡ã€‚ç±»å®ç°å½¢å¼ä¸º KLDivergence æˆ– KLD)\n",
    "\n",
    "* cosine_similarity(ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¯ç”¨äºå¤šåˆ†ç±»ï¼Œç±»å®ç°å½¢å¼ä¸º CosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥å¯¹tf.keras.losses.Lossè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™callæ–¹æ³•å®ç°æŸå¤±çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°æŸå¤±å‡½æ•°çš„ç±»çš„å®ç°ã€‚\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªFocal Lossçš„è‡ªå®šä¹‰å®ç°ç¤ºèŒƒã€‚Focal Lossæ˜¯ä¸€ç§å¯¹binary_crossentropyçš„æ”¹è¿›æŸå¤±å‡½æ•°å½¢å¼ã€‚\n",
    "\n",
    "åœ¨ç±»åˆ«ä¸å¹³è¡¡å’Œå­˜åœ¨éš¾ä»¥è®­ç»ƒæ ·æœ¬çš„æƒ…å½¢ä¸‹ç›¸å¯¹äºäºŒå…ƒäº¤å‰ç†µèƒ½å¤Ÿå–å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "è¯¦è§ã€Šå¦‚ä½•è¯„ä»·Kaimingçš„Focal Loss for Dense Object Detectionï¼Ÿã€‹\n",
    "\n",
    "https://www.zhihu.com/question/63581984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(losses.Loss):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.25):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        \n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(self.alpha * tf.pow(1. - pt_1, self.gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-self.alpha) * tf.pow( pt_0, self.gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-6,è¯„ä¼°æŒ‡æ ‡metrics\n",
    "\n",
    "æŸå¤±å‡½æ•°é™¤äº†ä½œä¸ºæ¨¡å‹è®­ç»ƒæ—¶å€™çš„ä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿèƒ½å¤Ÿä½œä¸ºæ¨¡å‹å¥½åçš„ä¸€ç§è¯„ä»·æŒ‡æ ‡ã€‚ä½†é€šå¸¸äººä»¬è¿˜ä¼šä»å…¶å®ƒè§’åº¦è¯„ä¼°æ¨¡å‹çš„å¥½åã€‚\n",
    "\n",
    "è¿™å°±æ˜¯è¯„ä¼°æŒ‡æ ‡ã€‚é€šå¸¸æŸå¤±å‡½æ•°éƒ½å¯ä»¥ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚MAE,MSE,CategoricalCrossentropyç­‰ä¹Ÿæ˜¯å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "ä½†è¯„ä¼°æŒ‡æ ‡ä¸ä¸€å®šå¯ä»¥ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚AUC,Accuracy,Precisionã€‚å› ä¸ºè¯„ä¼°æŒ‡æ ‡ä¸è¦æ±‚è¿ç»­å¯å¯¼ï¼Œè€ŒæŸå¤±å‡½æ•°é€šå¸¸è¦æ±‚è¿ç»­å¯å¯¼ã€‚\n",
    "\n",
    "ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œå¯ä»¥é€šè¿‡åˆ—è¡¨å½¢å¼æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºè¯„ä¼°å€¼ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥å¯¹tf.keras.metrics.Metricè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™åˆå§‹åŒ–æ–¹æ³•, update_stateæ–¹æ³•, resultæ–¹æ³•å®ç°è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°è¯„ä¼°æŒ‡æ ‡çš„ç±»çš„å®ç°å½¢å¼ã€‚\n",
    "\n",
    "ç”±äºè®­ç»ƒçš„è¿‡ç¨‹é€šå¸¸æ˜¯åˆ†æ‰¹æ¬¡è®­ç»ƒçš„ï¼Œè€Œè¯„ä¼°æŒ‡æ ‡è¦è·‘å®Œä¸€ä¸ªepochæ‰èƒ½å¤Ÿå¾—åˆ°æ•´ä½“çš„æŒ‡æ ‡ç»“æœã€‚å› æ­¤ï¼Œç±»å½¢å¼çš„è¯„ä¼°æŒ‡æ ‡æ›´ä¸ºå¸¸è§ã€‚å³éœ€è¦ç¼–å†™åˆå§‹åŒ–æ–¹æ³•ä»¥åˆ›å»ºä¸è®¡ç®—æŒ‡æ ‡ç»“æœç›¸å…³çš„ä¸€äº›ä¸­é—´å˜é‡ï¼Œç¼–å†™update_stateæ–¹æ³•åœ¨æ¯ä¸ªbatchåæ›´æ–°ç›¸å…³ä¸­é—´å˜é‡çš„çŠ¶æ€ï¼Œç¼–å†™resultæ–¹æ³•è¾“å‡ºæœ€ç»ˆæŒ‡æ ‡ç»“æœã€‚\n",
    "\n",
    "å¦‚æœç¼–å†™å‡½æ•°å½¢å¼çš„è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™åªèƒ½å–epochä¸­å„ä¸ªbatchè®¡ç®—çš„è¯„ä¼°æŒ‡æ ‡ç»“æœçš„å¹³å‡å€¼ä½œä¸ºæ•´ä¸ªepochä¸Šçš„è¯„ä¼°æŒ‡æ ‡ç»“æœï¼Œè¿™ä¸ªç»“æœé€šå¸¸ä¼šåç¦»æ‹¿æ•´ä¸ªepochæ•°æ®ä¸€æ¬¡è®¡ç®—çš„ç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå¸¸ç”¨çš„å†…ç½®è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MeanSquaredErrorï¼ˆå¹³æ–¹å·®è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMSEï¼Œå‡½æ•°å½¢å¼ä¸ºmseï¼‰\n",
    "\n",
    "* MeanAbsoluteError (ç»å¯¹å€¼è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMAEï¼Œå‡½æ•°å½¢å¼ä¸ºmae)\n",
    "\n",
    "* MeanAbsolutePercentageError (å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMAPEï¼Œå‡½æ•°å½¢å¼ä¸ºmape)\n",
    "\n",
    "* RootMeanSquaredError (å‡æ–¹æ ¹è¯¯å·®ï¼Œç”¨äºå›å½’)\n",
    "\n",
    "* Accuracy (å‡†ç¡®ç‡ï¼Œç”¨äºåˆ†ç±»ï¼Œå¯ä»¥ç”¨å­—ç¬¦ä¸²\"Accuracy\"è¡¨ç¤ºï¼ŒAccuracy=(TP+TN)/(TP+TN+FP+FN)ï¼Œè¦æ±‚y_trueå’Œy_predéƒ½ä¸ºç±»åˆ«åºå·ç¼–ç )\n",
    "\n",
    "* Precision (ç²¾ç¡®ç‡ï¼Œç”¨äºäºŒåˆ†ç±»ï¼ŒPrecision = TP/(TP+FP))\n",
    "\n",
    "* Recall (å¬å›ç‡ï¼Œç”¨äºäºŒåˆ†ç±»ï¼ŒRecall = TP/(TP+FN))\n",
    "\n",
    "* TruePositives (çœŸæ­£ä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)\n",
    "\n",
    "* TrueNegatives (çœŸè´Ÿä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)\n",
    "\n",
    "* FalsePositives (å‡æ­£ä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)\n",
    "\n",
    "* FalseNegatives (å‡è´Ÿä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)\n",
    "\n",
    "* AUC(ROCæ›²çº¿(TPR vs FPR)ä¸‹çš„é¢ç§¯ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œç›´è§‚è§£é‡Šä¸ºéšæœºæŠ½å–ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸€ä¸ªè´Ÿæ ·æœ¬ï¼Œæ­£æ ·æœ¬çš„é¢„æµ‹å€¼å¤§äºè´Ÿæ ·æœ¬çš„æ¦‚ç‡)\n",
    "\n",
    "* CategoricalAccuracyï¼ˆåˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸Accuracyå«ä¹‰ç›¸åŒï¼Œè¦æ±‚y_true(label)ä¸ºonehotç¼–ç å½¢å¼ï¼‰\n",
    "\n",
    "* SparseCategoricalAccuracy (ç¨€ç–åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸Accuracyå«ä¹‰ç›¸åŒï¼Œè¦æ±‚y_true(label)ä¸ºåºå·ç¼–ç å½¢å¼)\n",
    "\n",
    "* MeanIoU (Intersection-Over-Unionï¼Œå¸¸ç”¨äºå›¾åƒåˆ†å‰²)\n",
    "\n",
    "* TopKCategoricalAccuracy (å¤šåˆ†ç±»TopKå‡†ç¡®ç‡ï¼Œè¦æ±‚y_true(label)ä¸ºonehotç¼–ç å½¢å¼)\n",
    "\n",
    "* SparseTopKCategoricalAccuracy (ç¨€ç–å¤šåˆ†ç±»TopKå‡†ç¡®ç‡ï¼Œè¦æ±‚y_true(label)ä¸ºåºå·ç¼–ç å½¢å¼)\n",
    "\n",
    "* Mean (å¹³å‡å€¼)\n",
    "\n",
    "* Sum (æ±‚å’Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä»¥é‡‘èé£æ§é¢†åŸŸå¸¸ç”¨çš„KSæŒ‡æ ‡ä¸ºä¾‹ï¼Œç¤ºèŒƒè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "KSæŒ‡æ ‡é€‚åˆäºŒåˆ†ç±»é—®é¢˜ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸º KS=max(TPR-FPR).\n",
    "\n",
    "å…¶ä¸­TPR=TP/(TP+FN) , FPR = FP/(FP+TN) \n",
    "\n",
    "TPRæ›²çº¿å®é™…ä¸Šå°±æ˜¯æ­£æ ·æœ¬çš„ç´¯ç§¯åˆ†å¸ƒæ›²çº¿(CDF)ï¼ŒFPRæ›²çº¿å®é™…ä¸Šå°±æ˜¯è´Ÿæ ·æœ¬çš„ç´¯ç§¯åˆ†å¸ƒæ›²çº¿(CDF)ã€‚\n",
    "\n",
    "KSæŒ‡æ ‡å°±æ˜¯æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¯ç§¯åˆ†å¸ƒæ›²çº¿å·®å€¼çš„æœ€å¤§å€¼ã€‚\n",
    "\n",
    "![](./data/KS_curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,metrics\n",
    "\n",
    "#å‡½æ•°å½¢å¼çš„è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡\n",
    "@tf.function\n",
    "def ks(y_true,y_pred):\n",
    "    y_true = tf.reshape(y_true,(-1,))\n",
    "    y_pred = tf.reshape(y_pred,(-1,))\n",
    "    length = tf.shape(y_true)[0]\n",
    "    t = tf.math.top_k(y_pred,k = length,sorted = False)\n",
    "    y_pred_sorted = tf.gather(y_pred,t.indices)\n",
    "    y_true_sorted = tf.gather(y_true,t.indices)\n",
    "    cum_positive_ratio = tf.truediv(\n",
    "        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))\n",
    "    cum_negative_ratio = tf.truediv(\n",
    "        tf.cumsum(1 - y_true_sorted),tf.reduce_sum(1 - y_true_sorted))\n",
    "    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "    return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],[0.7],\n",
    "                      [0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "tf.print(ks(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.625\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç±»å½¢å¼çš„è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡\n",
    "class KS(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name = \"ks\", **kwargs):\n",
    "        super(KS,self).__init__(name=name,**kwargs)\n",
    "        self.true_positives = self.add_weight(\n",
    "            name = \"tp\",shape = (101,), initializer = \"zeros\")\n",
    "        self.false_positives = self.add_weight(\n",
    "            name = \"fp\",shape = (101,), initializer = \"zeros\")\n",
    "   \n",
    "    @tf.function\n",
    "    def update_state(self,y_true,y_pred):\n",
    "        y_true = tf.cast(tf.reshape(y_true,(-1,)),tf.bool)\n",
    "        y_pred = tf.cast(100*tf.reshape(y_pred,(-1,)),tf.int32)\n",
    "        \n",
    "        for i in tf.range(0,tf.shape(y_true)[0]):\n",
    "            if y_true[i]:\n",
    "                self.true_positives[y_pred[i]].assign(\n",
    "                    self.true_positives[y_pred[i]]+1.0)\n",
    "            else:\n",
    "                self.false_positives[y_pred[i]].assign(\n",
    "                    self.false_positives[y_pred[i]]+1.0)\n",
    "        return (self.true_positives,self.false_positives)\n",
    "    \n",
    "    @tf.function\n",
    "    def result(self):\n",
    "        cum_positive_ratio = tf.truediv(\n",
    "            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))\n",
    "        cum_negative_ratio = tf.truediv(\n",
    "            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))\n",
    "        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "        return ks_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],\n",
    "                      [0.7],[0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "\n",
    "myks = KS()\n",
    "myks.update_state(y_true,y_pred)\n",
    "tf.print(myks.result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.625\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "# 5-7,ä¼˜åŒ–å™¨optimizers\n",
    "\n",
    "æœºå™¨å­¦ä¹ ç•Œæœ‰ä¸€ç¾¤ç‚¼ä¸¹å¸ˆï¼Œä»–ä»¬æ¯å¤©çš„æ—¥å¸¸æ˜¯ï¼š\n",
    "\n",
    "æ‹¿æ¥è¯æï¼ˆæ•°æ®ï¼‰ï¼Œæ¶èµ·å…«å¦ç‚‰ï¼ˆæ¨¡å‹ï¼‰ï¼Œç‚¹ç€å…­å‘³çœŸç«ï¼ˆä¼˜åŒ–ç®—æ³•ï¼‰ï¼Œå°±æ‘‡ç€è’²æ‰‡ç­‰ç€ä¸¹è¯å‡ºç‚‰äº†ã€‚\n",
    "\n",
    "ä¸è¿‡ï¼Œå½“è¿‡å¨å­çš„éƒ½çŸ¥é“ï¼ŒåŒæ ·çš„é£Ÿæï¼ŒåŒæ ·çš„èœè°±ï¼Œä½†ç«å€™ä¸ä¸€æ ·äº†ï¼Œè¿™å‡ºæ¥çš„å£å‘³å¯æ˜¯åƒå·®ä¸‡åˆ«ã€‚ç«å°äº†å¤¹ç”Ÿï¼Œç«å¤§äº†æ˜“ç³Šï¼Œç«ä¸åŒ€åˆ™åŠç”ŸåŠç³Šã€‚\n",
    "\n",
    "æœºå™¨å­¦ä¹ ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæ¨¡å‹ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ç›´æ¥å…³ç³»åˆ°æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ã€‚æœ‰æ—¶å€™æ•ˆæœä¸å¥½ï¼Œæœªå¿…æ˜¯ç‰¹å¾çš„é—®é¢˜æˆ–è€…æ¨¡å‹è®¾è®¡çš„é—®é¢˜ï¼Œå¾ˆå¯èƒ½å°±æ˜¯ä¼˜åŒ–ç®—æ³•çš„é—®é¢˜ã€‚\n",
    "\n",
    "æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¤§æ¦‚ç»å†äº† SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam è¿™æ ·çš„å‘å±•å†ç¨‹ã€‚\n",
    "\n",
    "è¯¦è§ã€Šä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adamã€‹\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/32230623\n",
    "\n",
    "å¯¹äºä¸€èˆ¬æ–°æ‰‹ç‚¼ä¸¹å¸ˆï¼Œä¼˜åŒ–å™¨ç›´æ¥ä½¿ç”¨Adamï¼Œå¹¶ä½¿ç”¨å…¶é»˜è®¤å‚æ•°å°±OKäº†ã€‚\n",
    "\n",
    "ä¸€äº›çˆ±å†™è®ºæ–‡çš„ç‚¼ä¸¹å¸ˆç”±äºè¿½æ±‚è¯„ä¼°æŒ‡æ ‡æ•ˆæœï¼Œå¯èƒ½ä¼šåçˆ±å‰æœŸä½¿ç”¨Adamä¼˜åŒ–å™¨å¿«é€Ÿä¸‹é™ï¼ŒåæœŸä½¿ç”¨SGDå¹¶ç²¾è°ƒä¼˜åŒ–å™¨å‚æ•°å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚\n",
    "\n",
    "æ­¤å¤–ç›®å‰ä¹Ÿæœ‰ä¸€äº›å‰æ²¿çš„ä¼˜åŒ–ç®—æ³•ï¼Œæ®ç§°æ•ˆæœæ¯”Adamæ›´å¥½ï¼Œä¾‹å¦‚LazyAdam, Look-ahead, RAdam, Rangerç­‰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œä¼˜åŒ–å™¨çš„ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼˜åŒ–å™¨ä¸»è¦ä½¿ç”¨apply_gradientsæ–¹æ³•ä¼ å…¥å˜é‡å’Œå¯¹åº”æ¢¯åº¦ä»è€Œæ¥å¯¹ç»™å®šå˜é‡è¿›è¡Œè¿­ä»£ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨minimizeæ–¹æ³•å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚\n",
    "\n",
    "å½“ç„¶ï¼Œæ›´å¸¸è§çš„ä½¿ç”¨æ˜¯åœ¨ç¼–è¯‘æ—¶å°†ä¼˜åŒ–å™¨ä¼ å…¥kerasçš„Model,é€šè¿‡è°ƒç”¨model.fitå®ç°å¯¹Lossçš„çš„è¿­ä»£ä¼˜åŒ–ã€‚\n",
    "\n",
    "åˆå§‹åŒ–ä¼˜åŒ–å™¨æ—¶ä¼šåˆ›å»ºä¸€ä¸ªå˜é‡optimier.iterationsç”¨äºè®°å½•è¿­ä»£çš„æ¬¡æ•°ã€‚å› æ­¤ä¼˜åŒ–å™¨å’Œtf.Variableä¸€æ ·ï¼Œä¸€èˆ¬éœ€è¦åœ¨@tf.functionå¤–åˆ›å»ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼\n",
    "\n",
    "# ä½¿ç”¨optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    while tf.constant(True): \n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "        #è¿­ä»£ç»ˆæ­¢æ¡ä»¶\n",
    "        if tf.abs(dy_dx)<tf.constant(0.00001):\n",
    "            break\n",
    "            \n",
    "        if tf.math.mod(optimizer.iterations,100)==0:\n",
    "            printbar()\n",
    "            tf.print(\"step = \",optimizer.iterations)\n",
    "            tf.print(\"x = \", x)\n",
    "            tf.print(\"\")\n",
    "                \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(\"y =\",minimizef())\n",
    "tf.print(\"x =\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼\n",
    "\n",
    "# ä½¿ç”¨optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch = 1000):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    tf.print(\"epoch = \",optimizer.iterations)\n",
    "    return(f())\n",
    "\n",
    "train(1000)\n",
    "tf.print(\"y = \",f())\n",
    "tf.print(\"x = \",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼\n",
    "# ä½¿ç”¨model.fit\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class FakeModel(tf.keras.models.Model):\n",
    "    def __init__(self,a,b,c):\n",
    "        super(FakeModel,self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def build(self):\n",
    "        self.x = tf.Variable(0.0,name = \"x\")\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self,features):\n",
    "        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\n",
    "        return(tf.ones_like(features)*loss)\n",
    "    \n",
    "def myloss(y_true,y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = \n",
    "              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\n",
    "history = model.fit(tf.zeros((100,2)),\n",
    "                    tf.ones(100),batch_size = 1,epochs = 10)  #è¿­ä»£1000æ¬¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(\"x=\",model.x)\n",
    "tf.print(\"loss=\",model(tf.constant(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå†…ç½®ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¤§æ¦‚ç»å†äº† SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam è¿™æ ·çš„å‘å±•å†ç¨‹ã€‚\n",
    "\n",
    "åœ¨keras.optimizerså­æ¨¡å—ä¸­ï¼Œå®ƒä»¬åŸºæœ¬ä¸Šéƒ½æœ‰å¯¹åº”çš„ç±»çš„å®ç°ã€‚\n",
    "\n",
    "* SGD, é»˜è®¤å‚æ•°ä¸ºçº¯SGD, è®¾ç½®momentumå‚æ•°ä¸ä¸º0å®é™…ä¸Šå˜æˆSGDM, è€ƒè™‘äº†ä¸€é˜¶åŠ¨é‡, è®¾ç½® nesterovä¸ºTrueåå˜æˆNAGï¼Œå³ Nesterov Acceleration Gradientï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶è®¡ç®—çš„æ˜¯å‘å‰èµ°ä¸€æ­¥æ‰€åœ¨ä½ç½®çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "* Adagrad, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œå¯¹äºä¸åŒçš„å‚æ•°æœ‰ä¸åŒçš„å­¦ä¹ ç‡ï¼Œå³è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚ç¼ºç‚¹æ˜¯å­¦ä¹ ç‡å•è°ƒä¸‹é™ï¼Œå¯èƒ½åæœŸå­¦ä¹ é€Ÿç‡è¿‡æ…¢ä¹ƒè‡³æå‰åœæ­¢å­¦ä¹ ã€‚\n",
    "\n",
    "* RMSprop, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œå¯¹äºä¸åŒçš„å‚æ•°æœ‰ä¸åŒçš„å­¦ä¹ ç‡ï¼Œå³è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹Adagradè¿›è¡Œäº†ä¼˜åŒ–ï¼Œé€šè¿‡æŒ‡æ•°å¹³æ»‘åªè€ƒè™‘ä¸€å®šçª—å£å†…çš„äºŒé˜¶åŠ¨é‡ã€‚\n",
    "\n",
    "* Adadelta, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œä¸RMSpropç±»ä¼¼ï¼Œä½†æ˜¯æ›´åŠ å¤æ‚ä¸€äº›ï¼Œè‡ªé€‚åº”æ€§æ›´å¼ºã€‚\n",
    "\n",
    "* Adam, åŒæ—¶è€ƒè™‘äº†ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼Œå¯ä»¥çœ‹æˆRMSpropä¸Šè¿›ä¸€æ­¥è€ƒè™‘äº†Momentumã€‚\n",
    "\n",
    "* Nadam, åœ¨AdamåŸºç¡€ä¸Šè¿›ä¸€æ­¥è€ƒè™‘äº† Nesterov Accelerationã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-8,å›è°ƒå‡½æ•°callbacks\n",
    "\n",
    "tf.kerasçš„å›è°ƒå‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ä¸ªç±»ï¼Œä¸€èˆ¬æ˜¯åœ¨model.fitæ—¶ä½œä¸ºå‚æ•°æŒ‡å®šï¼Œç”¨äºæ§åˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹å¼€å§‹æˆ–è€…åœ¨è®­ç»ƒè¿‡ç¨‹ç»“æŸï¼Œåœ¨æ¯ä¸ªepochè®­ç»ƒå¼€å§‹æˆ–è€…è®­ç»ƒç»“æŸï¼Œåœ¨æ¯ä¸ªbatchè®­ç»ƒå¼€å§‹æˆ–è€…è®­ç»ƒç»“æŸæ—¶æ‰§è¡Œä¸€äº›æ“ä½œï¼Œä¾‹å¦‚æ”¶é›†ä¸€äº›æ—¥å¿—ä¿¡æ¯ï¼Œæ”¹å˜å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒè¿‡ç¨‹ç­‰ç­‰ã€‚\n",
    "\n",
    "åŒæ ·åœ°ï¼Œé’ˆå¯¹model.evaluateæˆ–è€…model.predictä¹Ÿå¯ä»¥æŒ‡å®šcallbackså‚æ•°ï¼Œç”¨äºæ§åˆ¶åœ¨è¯„ä¼°æˆ–é¢„æµ‹å¼€å§‹æˆ–è€…ç»“æŸæ—¶ï¼Œåœ¨æ¯ä¸ªbatchå¼€å§‹æˆ–è€…ç»“æŸæ—¶æ‰§è¡Œä¸€äº›æ“ä½œï¼Œä½†è¿™ç§ç”¨æ³•ç›¸å¯¹å°‘è§ã€‚\n",
    "\n",
    "å¤§éƒ¨åˆ†æ—¶å€™ï¼Œkeras.callbackså­æ¨¡å—ä¸­å®šä¹‰çš„å›è°ƒå‡½æ•°ç±»å·²ç»è¶³å¤Ÿä½¿ç”¨äº†ï¼Œå¦‚æœæœ‰ç‰¹å®šçš„éœ€è¦ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å¯¹keras.callbacks.Callbackså®æ–½å­ç±»åŒ–æ„é€ è‡ªå®šä¹‰çš„å›è°ƒå‡½æ•°ã€‚\n",
    "\n",
    "æ‰€æœ‰å›è°ƒå‡½æ•°éƒ½ç»§æ‰¿è‡³ keras.callbacks.CallbacksåŸºç±»ï¼Œæ‹¥æœ‰paramså’Œmodelè¿™ä¸¤ä¸ªå±æ€§ã€‚\n",
    "\n",
    "å…¶ä¸­params æ˜¯ä¸€ä¸ªdictï¼Œè®°å½•äº† training parameters (eg. verbosity, batch size, number of epochs...).\n",
    "\n",
    "modelå³å½“å‰å…³è”çš„æ¨¡å‹çš„å¼•ç”¨ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œå¯¹äºå›è°ƒç±»ä¸­çš„ä¸€äº›æ–¹æ³•å¦‚on_epoch_begin,on_batch_endï¼Œè¿˜ä¼šæœ‰ä¸€ä¸ªè¾“å…¥å‚æ•°logs, æä¾›æœ‰å…³å½“å‰epochæˆ–è€…batchçš„ä¸€äº›ä¿¡æ¯ï¼Œå¹¶èƒ½å¤Ÿè®°å½•è®¡ç®—ç»“æœï¼Œå¦‚æœmodel.fitæŒ‡å®šäº†å¤šä¸ªå›è°ƒå‡½æ•°ç±»ï¼Œè¿™äº›logså˜é‡å°†åœ¨è¿™äº›å›è°ƒå‡½æ•°ç±»çš„åŒåå‡½æ•°é—´ä¾é¡ºåºä¼ é€’ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå†…ç½®å›è°ƒå‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BaseLoggerï¼š æ”¶é›†æ¯ä¸ªepochä¸Šmetricsåœ¨å„ä¸ªbatchä¸Šçš„å¹³å‡å€¼ï¼Œå¯¹stateful_metricså‚æ•°ä¸­çš„å¸¦ä¸­é—´çŠ¶æ€çš„æŒ‡æ ‡ç›´æ¥æ‹¿æœ€ç»ˆå€¼æ— éœ€å¯¹å„ä¸ªbatchå¹³å‡ï¼ŒæŒ‡æ ‡å‡å€¼ç»“æœå°†æ·»åŠ åˆ°logså˜é‡ä¸­ã€‚è¯¥å›è°ƒå‡½æ•°è¢«æ‰€æœ‰æ¨¡å‹é»˜è®¤æ·»åŠ ï¼Œä¸”æ˜¯ç¬¬ä¸€ä¸ªè¢«æ·»åŠ çš„ã€‚\n",
    "\n",
    "* Historyï¼š å°†BaseLoggerè®¡ç®—çš„å„ä¸ªepochçš„metricsç»“æœè®°å½•åˆ°historyè¿™ä¸ªdictå˜é‡ä¸­ï¼Œå¹¶ä½œä¸ºmodel.fitçš„è¿”å›å€¼ã€‚è¯¥å›è°ƒå‡½æ•°è¢«æ‰€æœ‰æ¨¡å‹é»˜è®¤æ·»åŠ ï¼Œåœ¨BaseLoggerä¹‹åè¢«æ·»åŠ ã€‚\n",
    "\n",
    "* EarlyStoppingï¼š å½“è¢«ç›‘æ§æŒ‡æ ‡åœ¨è®¾å®šçš„è‹¥å¹²ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™æå‰ç»ˆæ­¢è®­ç»ƒã€‚\n",
    "\n",
    "* TensorBoardï¼š ä¸ºTensorboardå¯è§†åŒ–ä¿å­˜æ—¥å¿—ä¿¡æ¯ã€‚æ”¯æŒè¯„ä¼°æŒ‡æ ‡ï¼Œè®¡ç®—å›¾ï¼Œæ¨¡å‹å‚æ•°ç­‰çš„å¯è§†åŒ–ã€‚\n",
    "\n",
    "* ModelCheckpointï¼š åœ¨æ¯ä¸ªepochåä¿å­˜æ¨¡å‹ã€‚\n",
    "\n",
    "* ReduceLROnPlateauï¼šå¦‚æœç›‘æ§æŒ‡æ ‡åœ¨è®¾å®šçš„è‹¥å¹²ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™ä»¥ä¸€å®šçš„å› å­å‡å°‘å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "* TerminateOnNaNï¼šå¦‚æœé‡åˆ°lossä¸ºNaNï¼Œæå‰ç»ˆæ­¢è®­ç»ƒã€‚\n",
    "\n",
    "* LearningRateSchedulerï¼šå­¦ä¹ ç‡æ§åˆ¶å™¨ã€‚ç»™å®šå­¦ä¹ ç‡lrå’Œepochçš„å‡½æ•°å…³ç³»ï¼Œæ ¹æ®è¯¥å‡½æ•°å…³ç³»åœ¨æ¯ä¸ªepochå‰è°ƒæ•´å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "* CSVLoggerï¼šå°†æ¯ä¸ªepochåçš„logsç»“æœè®°å½•åˆ°CSVæ–‡ä»¶ä¸­ã€‚\n",
    "\n",
    "* ProgbarLoggerï¼šå°†æ¯ä¸ªepochåçš„logsç»“æœæ‰“å°åˆ°æ ‡å‡†è¾“å‡ºæµä¸­ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œè‡ªå®šä¹‰å›è°ƒå‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨callbacks.LambdaCallbackç¼–å†™è¾ƒä¸ºç®€å•çš„å›è°ƒå‡½æ•°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¯¹callbacks.Callbackå­ç±»åŒ–ç¼–å†™æ›´åŠ å¤æ‚çš„å›è°ƒå‡½æ•°é€»è¾‘ã€‚\n",
    "\n",
    "å¦‚æœéœ€è¦æ·±å…¥å­¦ä¹ tf.Kerasä¸­çš„å›è°ƒå‡½æ•°ï¼Œä¸è¦çŠ¹è±«é˜…è¯»å†…ç½®å›è°ƒå‡½æ•°çš„æºä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,metrics,callbacks\n",
    "import tensorflow.keras.backend as K \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºèŒƒä½¿ç”¨LambdaCallbackç¼–å†™è¾ƒä¸ºç®€å•çš„å›è°ƒå‡½æ•°\n",
    "\n",
    "import json\n",
    "json_log = open('./data/keras_log.json', mode='wt', buffering=1)\n",
    "json_logging_callback = callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps(dict(epoch = epoch,**logs)) + '\\n'),\n",
    "    on_train_end=lambda logs: json_log.close()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºèŒƒé€šè¿‡Callbackå­ç±»åŒ–ç¼–å†™å›è°ƒå‡½æ•°ï¼ˆLearningRateSchedulerçš„æºä»£ç ï¼‰\n",
    "\n",
    "class LearningRateScheduler(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, schedule, verbose=0):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        try:  \n",
    "            lr = float(K.get_value(self.model.optimizer.lr))\n",
    "            lr = self.schedule(epoch, lr)\n",
    "        except TypeError:  # Support for old API for backward compatibility\n",
    "            lr = self.schedule(epoch)\n",
    "        if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):\n",
    "            raise ValueError('The output of the \"schedule\" function '\n",
    "                             'should be float.')\n",
    "        if isinstance(lr, ops.Tensor) and not lr.dtype.is_floating:\n",
    "            raise ValueError('The dtype of Tensor should be float')\n",
    "        K.set_value(self.model.optimizer.lr, K.get_value(lr))\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n",
    "                 'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "# å…­ã€TensorFlowçš„é«˜é˜¶API\n",
    "\n",
    "TensorFlowçš„é«˜é˜¶APIä¸»è¦æ˜¯tensorflow.keras.models.\n",
    "\n",
    "æœ¬ç« æˆ‘ä»¬ä¸»è¦è¯¦ç»†ä»‹ç»tensorflow.keras.modelsç›¸å…³çš„ä»¥ä¸‹å†…å®¹ã€‚\n",
    "\n",
    "* æ¨¡å‹çš„æ„å»ºï¼ˆSequentialã€functional APIã€Modelå­ç±»åŒ–ï¼‰\n",
    "\n",
    "* æ¨¡å‹çš„è®­ç»ƒï¼ˆå†…ç½®fitæ–¹æ³•ã€å†…ç½®train_on_batchæ–¹æ³•ã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€å•GPUè®­ç»ƒæ¨¡å‹ã€å¤šGPUè®­ç»ƒæ¨¡å‹ã€TPUè®­ç»ƒæ¨¡å‹ï¼‰\n",
    "\n",
    "* æ¨¡å‹çš„éƒ¨ç½²ï¼ˆtensorflow servingéƒ¨ç½²æ¨¡å‹ã€ä½¿ç”¨spark(scala)è°ƒç”¨tensorflowæ¨¡å‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "# 6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•\n",
    "\n",
    "å¯ä»¥ä½¿ç”¨ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "å¯¹äºé¡ºåºç»“æ„çš„æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨Sequentialæ–¹æ³•æ„å»ºã€‚\n",
    "\n",
    "å¦‚æœæ¨¡å‹æœ‰å¤šè¾“å…¥æˆ–è€…å¤šè¾“å‡ºï¼Œæˆ–è€…æ¨¡å‹éœ€è¦å…±äº«æƒé‡ï¼Œæˆ–è€…æ¨¡å‹å…·æœ‰æ®‹å·®è¿æ¥ç­‰éé¡ºåºç»“æ„ï¼Œæ¨èä½¿ç”¨å‡½æ•°å¼APIè¿›è¡Œåˆ›å»ºã€‚\n",
    "\n",
    "å¦‚æœæ— ç‰¹å®šå¿…è¦ï¼Œå°½å¯èƒ½é¿å…ä½¿ç”¨Modelå­ç±»åŒ–çš„æ–¹å¼æ„å»ºæ¨¡å‹ï¼Œè¿™ç§æ–¹å¼æä¾›äº†æå¤§çš„çµæ´»æ€§ï¼Œä½†ä¹Ÿæœ‰æ›´å¤§çš„æ¦‚ç‡å‡ºé”™ã€‚\n",
    "\n",
    "ä¸‹é¢ä»¥IMDBç”µå½±è¯„è®ºçš„åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤º3ç§åˆ›å»ºæ¨¡å‹çš„æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import *\n",
    "\n",
    "\n",
    "train_token_path = \"./data/imdb/train_token.csv\"\n",
    "test_token_path = \"./data/imdb/test_token.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "MAX_LEN = 200  # We will cut reviews after 200 words\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# æ„å»ºç®¡é“\n",
    "def parse_line(line):\n",
    "    t = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.reshape(tf.cast(tf.strings.to_number(t[0]),tf.int32),(-1,))\n",
    "    features = tf.cast(tf.strings.to_number(tf.strings.split(t[1],\" \")),tf.int32)\n",
    "    return (features,label)\n",
    "\n",
    "ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒSequentialæŒ‰å±‚é¡ºåºåˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/Sequentialæ¨¡å‹ç»“æ„.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "baselogger = callbacks.BaseLogger(stateful_metrics=[\"AUC\"])\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "        epochs = 6,callbacks=[baselogger,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-fitæ¨¡å‹.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå‡½æ•°å¼APIåˆ›å»ºä»»æ„ç»“æ„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = layers.Input(shape=[MAX_LEN])\n",
    "x  = layers.Embedding(MAX_WORDS,7)(inputs)\n",
    "\n",
    "branch1 = layers.SeparableConv1D(64,3,activation=\"relu\")(x)\n",
    "branch1 = layers.MaxPool1D(3)(branch1)\n",
    "branch1 = layers.SeparableConv1D(32,3,activation=\"relu\")(branch1)\n",
    "branch1 = layers.GlobalMaxPool1D()(branch1)\n",
    "\n",
    "branch2 = layers.SeparableConv1D(64,5,activation=\"relu\")(x)\n",
    "branch2 = layers.MaxPool1D(5)(branch2)\n",
    "branch2 = layers.SeparableConv1D(32,5,activation=\"relu\")(branch2)\n",
    "branch2 = layers.GlobalMaxPool1D()(branch2)\n",
    "\n",
    "branch3 = layers.SeparableConv1D(64,7,activation=\"relu\")(x)\n",
    "branch3 = layers.MaxPool1D(7)(branch3)\n",
    "branch3 = layers.SeparableConv1D(32,7,activation=\"relu\")(branch3)\n",
    "branch3 = layers.GlobalMaxPool1D()(branch3)\n",
    "\n",
    "concat = layers.Concatenate()([branch1,branch2,branch3])\n",
    "outputs = layers.Dense(1,activation = \"sigmoid\")(concat)\n",
    "\n",
    "model = models.Model(inputs = inputs,outputs = outputs)\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding (Embedding)           (None, 200, 7)       70000       input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_2 (SeparableCo (None, 196, 64)      547         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_4 (SeparableCo (None, 194, 64)      561         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_conv1d[0][0]           \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1D)  (None, 39, 64)       0           separable_conv1d_2[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1D)  (None, 27, 64)       0           separable_conv1d_4[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_3 (SeparableCo (None, 35, 32)       2400        max_pooling1d_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_5 (SeparableCo (None, 21, 32)       2528        max_pooling1d_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d (GlobalMax (None, 32)           0           separable_conv1d_1[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_1 (GlobalM (None, 32)           0           separable_conv1d_3[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_2 (GlobalM (None, 32)           0           separable_conv1d_5[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "concatenate (Concatenate)       (None, 96)           0           global_max_pooling1d[0][0]       \n",
    "                                                                 global_max_pooling1d_1[0][0]     \n",
    "                                                                 global_max_pooling1d_2[0][0]     \n",
    "__________________________________________________________________________________________________\n",
    "dense (Dense)                   (None, 1)            97          concatenate[0][0]                \n",
    "==================================================================================================\n",
    "Total params: 78,938\n",
    "Trainable params: 78,938\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/FunctionalAPIæ¨¡å‹ç»“æ„.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 6,callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/6\n",
    "1000/1000 [==============================] - 32s 32ms/step - loss: 0.5527 - accuracy: 0.6758 - AUC: 0.7731 - val_loss: 0.3646 - val_accuracy: 0.8426 - val_AUC: 0.9192\n",
    "Epoch 2/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.3024 - accuracy: 0.8737 - AUC: 0.9444 - val_loss: 0.3281 - val_accuracy: 0.8644 - val_AUC: 0.9350\n",
    "Epoch 3/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.2158 - accuracy: 0.9159 - AUC: 0.9715 - val_loss: 0.3461 - val_accuracy: 0.8666 - val_AUC: 0.9363\n",
    "Epoch 4/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.1492 - accuracy: 0.9464 - AUC: 0.9859 - val_loss: 0.4017 - val_accuracy: 0.8568 - val_AUC: 0.9311\n",
    "Epoch 5/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.0944 - accuracy: 0.9696 - AUC: 0.9939 - val_loss: 0.4998 - val_accuracy: 0.8550 - val_AUC: 0.9233\n",
    "Epoch 6/6\n",
    "1000/1000 [==============================] - 26s 26ms/step - loss: 0.0526 - accuracy: 0.9865 - AUC: 0.9977 - val_loss: 0.6463 - val_accuracy: 0.8462 - val_AUC: 0.9138\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-2-train.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼ŒModelå­ç±»åŒ–åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆè‡ªå®šä¹‰ä¸€ä¸ªæ®‹å·®æ¨¡å—ï¼Œä¸ºè‡ªå®šä¹‰Layer\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.conv1 = layers.Conv1D(filters=64,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv2 = layers.Conv1D(filters=32,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv3 = layers.Conv1D(filters=input_shape[-1],\n",
    "                                   kernel_size=self.kernel_size,activation = \"relu\",padding=\"same\")\n",
    "        self.maxpool = layers.MaxPool1D(2)\n",
    "        super(ResBlock,self).build(input_shape) # ç›¸å½“äºè®¾ç½®self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = layers.Add()([inputs,x])\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "    #å¦‚æœè¦è®©è‡ªå®šä¹‰çš„Layeré€šè¿‡Functional API ç»„åˆæˆæ¨¡å‹æ—¶å¯ä»¥åºåˆ—åŒ–ï¼Œéœ€è¦è‡ªå®šä¹‰get_configæ–¹æ³•ã€‚\n",
    "    def get_config(self):  \n",
    "        config = super(ResBlock, self).get_config()\n",
    "        config.update({'kernel_size': self.kernel_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ResBlock\n",
    "resblock = ResBlock(kernel_size = 3)\n",
    "resblock.build(input_shape = (None,200,7))\n",
    "resblock.compute_output_shape(input_shape=(None,200,7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([None, 100, 7])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šä¹‰æ¨¡å‹ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥ä½¿ç”¨Sequentialæˆ–è€…FunctionalAPI\n",
    "\n",
    "class ImdbModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ImdbModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7)\n",
    "        self.block1 = ResBlock(7)\n",
    "        self.block2 = ResBlock(5)\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(ImdbModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = ImdbModel()\n",
    "model.build(input_shape =(None,200))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"imdb_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        multiple                  70000     \n",
    "_________________________________________________________________\n",
    "res_block (ResBlock)         multiple                  19143     \n",
    "_________________________________________________________________\n",
    "res_block_1 (ResBlock)       multiple                  13703     \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  351       \n",
    "=================================================================\n",
    "Total params: 103,197\n",
    "Trainable params: 103,197\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/Modelå­ç±»åŒ–æ¨¡å‹ç»“æ„.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "logdir = \"./tflogs/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "                    epochs = 6,callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/6\n",
    "1000/1000 [==============================] - 47s 47ms/step - loss: 0.5629 - accuracy: 0.6618 - AUC: 0.7548 - val_loss: 0.3422 - val_accuracy: 0.8510 - val_AUC: 0.9286\n",
    "Epoch 2/6\n",
    "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2648 - accuracy: 0.8903 - AUC: 0.9576 - val_loss: 0.3276 - val_accuracy: 0.8650 - val_AUC: 0.9410\n",
    "Epoch 3/6\n",
    "1000/1000 [==============================] - 42s 42ms/step - loss: 0.1573 - accuracy: 0.9439 - AUC: 0.9846 - val_loss: 0.3861 - val_accuracy: 0.8682 - val_AUC: 0.9390\n",
    "Epoch 4/6\n",
    "1000/1000 [==============================] - 42s 42ms/step - loss: 0.0849 - accuracy: 0.9706 - AUC: 0.9950 - val_loss: 0.5324 - val_accuracy: 0.8616 - val_AUC: 0.9292\n",
    "Epoch 5/6\n",
    "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0393 - accuracy: 0.9876 - AUC: 0.9986 - val_loss: 0.7693 - val_accuracy: 0.8566 - val_AUC: 0.9132\n",
    "Epoch 6/6\n",
    "1000/1000 [==============================] - 44s 44ms/step - loss: 0.0222 - accuracy: 0.9926 - AUC: 0.9994 - val_loss: 0.9328 - val_accuracy: 0.8584 - val_AUC: 0.9052\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-3-fitæ¨¡å‹.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•\n",
    "\n",
    "æ¨¡å‹çš„è®­ç»ƒä¸»è¦æœ‰å†…ç½®fitæ–¹æ³•ã€å†…ç½®tran_on_batchæ–¹æ³•ã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚\n",
    "\n",
    "æ³¨ï¼šfit_generatoræ–¹æ³•åœ¨tf.kerasä¸­ä¸æ¨èä½¿ç”¨ï¼Œå…¶åŠŸèƒ½å·²ç»è¢«fitåŒ…å«ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå†…ç½®fitæ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯¥æ–¹æ³•åŠŸèƒ½éå¸¸å¼ºå¤§, æ”¯æŒå¯¹numpy array, tf.data.Datasetä»¥åŠ Python generatoræ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n",
    "\n",
    "å¹¶ä¸”å¯ä»¥é€šè¿‡è®¾ç½®å›è°ƒå‡½æ•°å®ç°å¯¹è®­ç»ƒè¿‡ç¨‹çš„å¤æ‚æ§åˆ¶é€»è¾‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "281/281 [==============================] - 11s 37ms/step - loss: 2.0231 - sparse_categorical_accuracy: 0.4636 - sparse_top_k_categorical_accuracy: 0.7450 - val_loss: 1.7346 - val_sparse_categorical_accuracy: 0.5534 - val_sparse_top_k_categorical_accuracy: 0.7560\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 9s 31ms/step - loss: 1.5079 - sparse_categorical_accuracy: 0.6091 - sparse_top_k_categorical_accuracy: 0.7901 - val_loss: 1.5475 - val_sparse_categorical_accuracy: 0.6109 - val_sparse_top_k_categorical_accuracy: 0.7792\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 1.2204 - sparse_categorical_accuracy: 0.6823 - sparse_top_k_categorical_accuracy: 0.8448 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.6367 - val_sparse_top_k_categorical_accuracy: 0.8001\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 0.9382 - sparse_categorical_accuracy: 0.7543 - sparse_top_k_categorical_accuracy: 0.9075 - val_loss: 1.6780 - val_sparse_categorical_accuracy: 0.6398 - val_sparse_top_k_categorical_accuracy: 0.8032\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 10s 34ms/step - loss: 0.6791 - sparse_categorical_accuracy: 0.8255 - sparse_top_k_categorical_accuracy: 0.9513 - val_loss: 1.9426 - val_sparse_categorical_accuracy: 0.6376 - val_sparse_top_k_categorical_accuracy: 0.7956\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 0.5063 - sparse_categorical_accuracy: 0.8762 - sparse_top_k_categorical_accuracy: 0.9716 - val_loss: 2.2141 - val_sparse_categorical_accuracy: 0.6291 - val_sparse_top_k_categorical_accuracy: 0.7947\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 10s 37ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.9050 - sparse_top_k_categorical_accuracy: 0.9817 - val_loss: 2.4126 - val_sparse_categorical_accuracy: 0.6264 - val_sparse_top_k_categorical_accuracy: 0.7947\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 10s 35ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.9205 - sparse_top_k_categorical_accuracy: 0.9881 - val_loss: 2.5366 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7974\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 10s 36ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.9299 - sparse_top_k_categorical_accuracy: 0.9909 - val_loss: 2.6564 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 9s 30ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.9334 - sparse_top_k_categorical_accuracy: 0.9947 - val_loss: 2.7365 - val_sparse_categorical_accuracy: 0.6220 - val_sparse_top_k_categorical_accuracy: 0.8005\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå†…ç½®train_on_batchæ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯¥å†…ç½®æ–¹æ³•ç›¸æ¯”è¾ƒfitæ–¹æ³•æ›´åŠ çµæ´»ï¼Œå¯ä»¥ä¸é€šè¿‡å›è°ƒå‡½æ•°è€Œç›´æ¥åœ¨æ‰¹æ¬¡å±‚æ¬¡ä¸Šæ›´åŠ ç²¾ç»†åœ°æ§åˆ¶è®­ç»ƒçš„è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # åœ¨åæœŸé™ä½å­¦ä¹ ç‡\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:09:19\n",
    "epoch =  1\n",
    "train: {'loss': 0.82411176, 'sparse_categorical_accuracy': 0.77272725, 'sparse_top_k_categorical_accuracy': 0.8636364}\n",
    "valid: {'loss': 1.9265995, 'sparse_categorical_accuracy': 0.5743544, 'sparse_top_k_categorical_accuracy': 0.75779164}\n",
    "\n",
    "================================================================================13:09:27\n",
    "epoch =  2\n",
    "train: {'loss': 0.6006621, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}\n",
    "valid: {'loss': 1.844159, 'sparse_categorical_accuracy': 0.6126447, 'sparse_top_k_categorical_accuracy': 0.7920748}\n",
    "\n",
    "================================================================================13:09:35\n",
    "epoch =  3\n",
    "train: {'loss': 0.36935613, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}\n",
    "valid: {'loss': 2.163433, 'sparse_categorical_accuracy': 0.63312554, 'sparse_top_k_categorical_accuracy': 0.8045414}\n",
    "\n",
    "================================================================================13:09:42\n",
    "epoch =  4\n",
    "train: {'loss': 0.2304088, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 2.8911984, 'sparse_categorical_accuracy': 0.6344613, 'sparse_top_k_categorical_accuracy': 0.7978629}\n",
    "\n",
    "Lowering optimizer Learning Rate...\n",
    "\n",
    "\n",
    "================================================================================13:09:51\n",
    "epoch =  5\n",
    "train: {'loss': 0.111194365, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 3.6431572, 'sparse_categorical_accuracy': 0.6295637, 'sparse_top_k_categorical_accuracy': 0.7978629}\n",
    "\n",
    "================================================================================13:09:59\n",
    "epoch =  6\n",
    "train: {'loss': 0.07741702, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.074161, 'sparse_categorical_accuracy': 0.6255565, 'sparse_top_k_categorical_accuracy': 0.794301}\n",
    "\n",
    "================================================================================13:10:07\n",
    "epoch =  7\n",
    "train: {'loss': 0.056113098, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.4461513, 'sparse_categorical_accuracy': 0.6273375, 'sparse_top_k_categorical_accuracy': 0.79652715}\n",
    "\n",
    "================================================================================13:10:17\n",
    "epoch =  8\n",
    "train: {'loss': 0.043448802, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.7687583, 'sparse_categorical_accuracy': 0.6224399, 'sparse_top_k_categorical_accuracy': 0.79741764}\n",
    "\n",
    "================================================================================13:10:26\n",
    "epoch =  9\n",
    "train: {'loss': 0.035002146, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 5.130505, 'sparse_categorical_accuracy': 0.6175423, 'sparse_top_k_categorical_accuracy': 0.794301}\n",
    "\n",
    "================================================================================13:10:34\n",
    "epoch =  10\n",
    "train: {'loss': 0.028303564, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 5.4559293, 'sparse_categorical_accuracy': 0.6148709, 'sparse_top_k_categorical_accuracy': 0.7947462}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯æ— éœ€ç¼–è¯‘æ¨¡å‹ï¼Œç›´æ¥åˆ©ç”¨ä¼˜åŒ–å™¨æ ¹æ®æŸå¤±å‡½æ•°åå‘ä¼ æ’­è¿­ä»£å‚æ•°ï¼Œæ‹¥æœ‰æœ€é«˜çš„çµæ´»æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:12:03\n",
    "Epoch=1,Loss:2.02051544,Accuracy:0.460253835,Valid Loss:1.75700927,Valid Accuracy:0.536954582\n",
    "\n",
    "================================================================================13:12:09\n",
    "Epoch=2,Loss:1.510795,Accuracy:0.610665798,Valid Loss:1.55349839,Valid Accuracy:0.616206586\n",
    "\n",
    "================================================================================13:12:17\n",
    "Epoch=3,Loss:1.19221532,Accuracy:0.696170092,Valid Loss:1.52315605,Valid Accuracy:0.651380241\n",
    "\n",
    "================================================================================13:12:23\n",
    "Epoch=4,Loss:0.90101546,Accuracy:0.766310394,Valid Loss:1.68327653,Valid Accuracy:0.648263574\n",
    "\n",
    "================================================================================13:12:30\n",
    "Epoch=5,Loss:0.655430496,Accuracy:0.831329346,Valid Loss:1.90872383,Valid Accuracy:0.641139805\n",
    "\n",
    "================================================================================13:12:37\n",
    "Epoch=6,Loss:0.492730737,Accuracy:0.877866864,Valid Loss:2.09966016,Valid Accuracy:0.63223511\n",
    "\n",
    "================================================================================13:12:44\n",
    "Epoch=7,Loss:0.391238362,Accuracy:0.904030263,Valid Loss:2.27431226,Valid Accuracy:0.625111282\n",
    "\n",
    "================================================================================13:12:51\n",
    "Epoch=8,Loss:0.327761739,Accuracy:0.922066331,Valid Loss:2.42568827,Valid Accuracy:0.617542326\n",
    "\n",
    "================================================================================13:12:58\n",
    "Epoch=9,Loss:0.285573095,Accuracy:0.930527747,Valid Loss:2.55942106,Valid Accuracy:0.612644672\n",
    "\n",
    "================================================================================13:13:05\n",
    "Epoch=10,Loss:0.255482465,Accuracy:0.936094403,Valid Loss:2.67789412,Valid Accuracy:0.612199485\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹å¸¸å¸¸éå¸¸è€—æ—¶ï¼Œä¸€ä¸ªæ¨¡å‹è®­ç»ƒå‡ ä¸ªå°æ—¶æ˜¯å®¶å¸¸ä¾¿é¥­ï¼Œè®­ç»ƒå‡ å¤©ä¹Ÿæ˜¯å¸¸æœ‰çš„äº‹æƒ…ï¼Œæœ‰æ—¶å€™ç”šè‡³è¦è®­ç»ƒå‡ åå¤©ã€‚\n",
    "\n",
    "è®­ç»ƒè¿‡ç¨‹çš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ªæ•°æ®å‡†å¤‡ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ªå‚æ•°è¿­ä»£ã€‚\n",
    "\n",
    "å½“æ•°æ®å‡†å¤‡è¿‡ç¨‹è¿˜æ˜¯æ¨¡å‹è®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤šè¿›ç¨‹æ¥å‡†å¤‡æ•°æ®ã€‚\n",
    "\n",
    "å½“å‚æ•°è¿­ä»£è¿‡ç¨‹æˆä¸ºè®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸çš„æ–¹æ³•æ˜¯åº”ç”¨GPUæˆ–è€…Googleçš„TPUæ¥è¿›è¡ŒåŠ é€Ÿã€‚\n",
    "\n",
    "è¯¦è§ã€Šç”¨GPUåŠ é€ŸKerasæ¨¡å‹â€”â€”Colabå…è´¹GPUä½¿ç”¨æ”»ç•¥ã€‹\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/68509398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ— è®ºæ˜¯å†…ç½®fitæ–¹æ³•ï¼Œè¿˜æ˜¯è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œä»CPUåˆ‡æ¢æˆå•GPUè®­ç»ƒæ¨¡å‹éƒ½æ˜¯éå¸¸æ–¹ä¾¿çš„ï¼Œæ— éœ€æ›´æ”¹ä»»ä½•ä»£ç ã€‚å½“å­˜åœ¨å¯ç”¨çš„GPUæ—¶ï¼Œå¦‚æœä¸ç‰¹æ„æŒ‡å®šdeviceï¼Œtensorflowä¼šè‡ªåŠ¨ä¼˜å…ˆé€‰æ‹©ä½¿ç”¨GPUæ¥åˆ›å»ºå¼ é‡å’Œæ‰§è¡Œå¼ é‡è®¡ç®—ã€‚\n",
    "\n",
    "ä½†å¦‚æœæ˜¯åœ¨å…¬å¸æˆ–è€…å­¦æ ¡å®éªŒå®¤çš„æœåŠ¡å™¨ç¯å¢ƒï¼Œå­˜åœ¨å¤šä¸ªGPUå’Œå¤šä¸ªä½¿ç”¨è€…æ—¶ï¼Œä¸ºäº†ä¸è®©å•ä¸ªåŒå­¦çš„ä»»åŠ¡å ç”¨å…¨éƒ¨GPUèµ„æºå¯¼è‡´å…¶ä»–åŒå­¦æ— æ³•ä½¿ç”¨ï¼ˆtensorflowé»˜è®¤è·å–å…¨éƒ¨GPUçš„å…¨éƒ¨å†…å­˜èµ„æºæƒé™ï¼Œä½†å®é™…ä¸Šåªä½¿ç”¨ä¸€ä¸ªGPUçš„éƒ¨åˆ†èµ„æºï¼‰ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨å¼€å¤´å¢åŠ ä»¥ä¸‹å‡ è¡Œä»£ç ä»¥æ§åˆ¶æ¯ä¸ªä»»åŠ¡ä½¿ç”¨çš„GPUç¼–å·å’Œæ˜¾å­˜å¤§å°ï¼Œä»¥ä¾¿å…¶ä»–åŒå­¦ä¹Ÿèƒ½å¤ŸåŒæ—¶è®­ç»ƒæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU\n",
    "\n",
    "æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚\n",
    "\n",
    "å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_å•GPUã€‹ï¼š\n",
    "\n",
    "https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import * \n",
    "\n",
    "#æ‰“å°æ—¶é—´åˆ†å‰²çº¿\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒGPUè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä»…ä½¿ç”¨ç¬¬0ä¸ªGPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #è®¾ç½®GPUæ˜¾å­˜ç”¨é‡æŒ‰éœ€ä½¿ç”¨\n",
    "    # æˆ–è€…ä¹Ÿå¯ä»¥è®¾ç½®GPUæ˜¾å­˜ä¸ºå›ºå®šä½¿ç”¨é‡(ä¾‹å¦‚ï¼š4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯”è¾ƒGPUå’ŒCPUçš„è®¡ç®—é€Ÿåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printbar()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n",
    "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n",
    "    c = a@b\n",
    "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\n",
    "printbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:37:01\n",
    "2.24953778e+11\n",
    "================================================================================17:37:01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printbar()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n",
    "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n",
    "    c = a@b\n",
    "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\n",
    "printbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:37:34\n",
    "2.24953795e+11\n",
    "================================================================================17:37:40\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:13:26\n",
    "Epoch=1,Loss:1.96735072,Accuracy:0.489200622,Valid Loss:1.64124215,Valid Accuracy:0.582813919\n",
    "\n",
    "================================================================================17:13:28\n",
    "Epoch=2,Loss:1.4640888,Accuracy:0.624805152,Valid Loss:1.5559175,Valid Accuracy:0.607747078\n",
    "\n",
    "================================================================================17:13:30\n",
    "Epoch=3,Loss:1.20681274,Accuracy:0.68581605,Valid Loss:1.58494771,Valid Accuracy:0.622439921\n",
    "\n",
    "================================================================================17:13:31\n",
    "Epoch=4,Loss:0.937500894,Accuracy:0.75361836,Valid Loss:1.77466083,Valid Accuracy:0.621994674\n",
    "\n",
    "================================================================================17:13:33\n",
    "Epoch=5,Loss:0.693960547,Accuracy:0.822199941,Valid Loss:2.00267363,Valid Accuracy:0.6197685\n",
    "\n",
    "================================================================================17:13:35\n",
    "Epoch=6,Loss:0.519614,Accuracy:0.870296121,Valid Loss:2.23463202,Valid Accuracy:0.613980412\n",
    "\n",
    "================================================================================17:13:37\n",
    "Epoch=7,Loss:0.408562034,Accuracy:0.901246965,Valid Loss:2.46969271,Valid Accuracy:0.612199485\n",
    "\n",
    "================================================================================17:13:39\n",
    "Epoch=8,Loss:0.339028627,Accuracy:0.920062363,Valid Loss:2.68585229,Valid Accuracy:0.615316093\n",
    "\n",
    "================================================================================17:13:41\n",
    "Epoch=9,Loss:0.293798745,Accuracy:0.92930305,Valid Loss:2.88995624,Valid Accuracy:0.613535166\n",
    "\n",
    "================================================================================17:13:43\n",
    "Epoch=10,Loss:0.263130337,Accuracy:0.936651051,Valid Loss:3.09705234,Valid Accuracy:0.612644672\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "å¦‚æœä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹ï¼Œæ¨èä½¿ç”¨å†…ç½®fitæ–¹æ³•ï¼Œè¾ƒä¸ºæ–¹ä¾¿ï¼Œä»…éœ€æ·»åŠ 2è¡Œä»£ç ã€‚\n",
    "\n",
    "åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU\n",
    "\n",
    "æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚\n",
    "\n",
    "å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_å¤šGPUã€‹ï¼š\n",
    "\n",
    "https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MirroredStrategyè¿‡ç¨‹ç®€ä»‹ï¼š\n",
    "\n",
    "* è®­ç»ƒå¼€å§‹å‰ï¼Œè¯¥ç­–ç•¥åœ¨æ‰€æœ‰ N ä¸ªè®¡ç®—è®¾å¤‡ä¸Šå‡å„å¤åˆ¶ä¸€ä»½å®Œæ•´çš„æ¨¡å‹ï¼›\n",
    "* æ¯æ¬¡è®­ç»ƒä¼ å…¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ—¶ï¼Œå°†æ•°æ®åˆ†æˆ N ä»½ï¼Œåˆ†åˆ«ä¼ å…¥ N ä¸ªè®¡ç®—è®¾å¤‡ï¼ˆå³æ•°æ®å¹¶è¡Œï¼‰ï¼›\n",
    "* N ä¸ªè®¡ç®—è®¾å¤‡ä½¿ç”¨æœ¬åœ°å˜é‡ï¼ˆé•œåƒå˜é‡ï¼‰åˆ†åˆ«è®¡ç®—è‡ªå·±æ‰€è·å¾—çš„éƒ¨åˆ†æ•°æ®çš„æ¢¯åº¦ï¼›\n",
    "* ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—çš„ All-reduce æ“ä½œï¼Œåœ¨è®¡ç®—è®¾å¤‡é—´é«˜æ•ˆäº¤æ¢æ¢¯åº¦æ•°æ®å¹¶è¿›è¡Œæ±‚å’Œï¼Œä½¿å¾—æœ€ç»ˆæ¯ä¸ªè®¾å¤‡éƒ½æœ‰äº†æ‰€æœ‰è®¾å¤‡çš„æ¢¯åº¦ä¹‹å’Œï¼›\n",
    "* ä½¿ç”¨æ¢¯åº¦æ±‚å’Œçš„ç»“æœæ›´æ–°æœ¬åœ°å˜é‡ï¼ˆé•œåƒå˜é‡ï¼‰ï¼›\n",
    "* å½“æ‰€æœ‰è®¾å¤‡å‡æ›´æ–°æœ¬åœ°å˜é‡åï¼Œè¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒï¼ˆå³è¯¥å¹¶è¡Œç­–ç•¥æ˜¯åŒæ­¥çš„ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­¤å¤„åœ¨colabä¸Šä½¿ç”¨1ä¸ªGPUæ¨¡æ‹Ÿå‡ºä¸¤ä¸ªé€»è¾‘GPUè¿›è¡Œå¤šGPUè®­ç»ƒ\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # è®¾ç½®ä¸¤ä¸ªé€»è¾‘GPUæ¨¡æ‹Ÿå¤šGPUè®­ç»ƒ\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¢åŠ ä»¥ä¸‹ä¸¤è¡Œä»£ç \n",
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "with strategy.scope(): \n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    \n",
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
    "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "281/281 [==============================] - 15s 53ms/step - loss: 2.0270 - sparse_categorical_accuracy: 0.4653 - sparse_top_k_categorical_accuracy: 0.7481 - val_loss: 1.7517 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7578\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 1.5206 - sparse_categorical_accuracy: 0.6045 - sparse_top_k_categorical_accuracy: 0.7938 - val_loss: 1.5715 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 1.2178 - sparse_categorical_accuracy: 0.6843 - sparse_top_k_categorical_accuracy: 0.8547 - val_loss: 1.5232 - val_sparse_categorical_accuracy: 0.6327 - val_sparse_top_k_categorical_accuracy: 0.8112\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 4s 13ms/step - loss: 0.9127 - sparse_categorical_accuracy: 0.7648 - sparse_top_k_categorical_accuracy: 0.9113 - val_loss: 1.6527 - val_sparse_categorical_accuracy: 0.6296 - val_sparse_top_k_categorical_accuracy: 0.8201\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.6606 - sparse_categorical_accuracy: 0.8321 - sparse_top_k_categorical_accuracy: 0.9525 - val_loss: 1.8791 - val_sparse_categorical_accuracy: 0.6158 - val_sparse_top_k_categorical_accuracy: 0.8219\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.4919 - sparse_categorical_accuracy: 0.8799 - sparse_top_k_categorical_accuracy: 0.9725 - val_loss: 2.1282 - val_sparse_categorical_accuracy: 0.6037 - val_sparse_top_k_categorical_accuracy: 0.8112\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.3947 - sparse_categorical_accuracy: 0.9051 - sparse_top_k_categorical_accuracy: 0.9814 - val_loss: 2.3033 - val_sparse_categorical_accuracy: 0.6046 - val_sparse_top_k_categorical_accuracy: 0.8094\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.3335 - sparse_categorical_accuracy: 0.9207 - sparse_top_k_categorical_accuracy: 0.9863 - val_loss: 2.4255 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.8099\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.9304 - sparse_top_k_categorical_accuracy: 0.9911 - val_loss: 2.5571 - val_sparse_categorical_accuracy: 0.6020 - val_sparse_top_k_categorical_accuracy: 0.8126\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.2617 - sparse_categorical_accuracy: 0.9342 - sparse_top_k_categorical_accuracy: 0.9937 - val_loss: 2.6700 - val_sparse_categorical_accuracy: 0.6077 - val_sparse_top_k_categorical_accuracy: 0.8148\n",
    "CPU times: user 1min 2s, sys: 8.59 s, total: 1min 10s\n",
    "Wall time: 58.5 s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)\n",
    "\n",
    "# 6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "å¦‚æœæƒ³å°è¯•ä½¿ç”¨Google Colabä¸Šçš„TPUæ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ˜¯éå¸¸æ–¹ä¾¿ï¼Œä»…éœ€æ·»åŠ 6è¡Œä»£ç ã€‚\n",
    "\n",
    "åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© TPU\n",
    "\n",
    "æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚\n",
    "\n",
    "å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_TPUã€‹ï¼š\n",
    "\n",
    "https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¢åŠ ä»¥ä¸‹6è¡Œä»£ç \n",
    "import os\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
    "WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
    "INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470\n",
    "INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470\n",
    "INFO:tensorflow:Clearing out eager caches\n",
    "INFO:tensorflow:Clearing out eager caches\n",
    "INFO:tensorflow:Finished initializing TPU system.\n",
    "INFO:tensorflow:Finished initializing TPU system.\n",
    "INFO:tensorflow:Found TPU system:\n",
    "INFO:tensorflow:Found TPU system:\n",
    "INFO:tensorflow:*** Num TPU Cores: 8\n",
    "INFO:tensorflow:*** Num TPU Cores: 8\n",
    "INFO:tensorflow:*** Num TPU Workers: 1\n",
    "INFO:tensorflow:*** Num TPU Workers: 1\n",
    "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
    "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "281/281 [==============================] - 12s 43ms/step - loss: 3.4466 - sparse_categorical_accuracy: 0.4332 - sparse_top_k_categorical_accuracy: 0.7180 - val_loss: 3.3179 - val_sparse_categorical_accuracy: 0.5352 - val_sparse_top_k_categorical_accuracy: 0.7195\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 6s 20ms/step - loss: 3.3251 - sparse_categorical_accuracy: 0.5405 - sparse_top_k_categorical_accuracy: 0.7302 - val_loss: 3.3082 - val_sparse_categorical_accuracy: 0.5463 - val_sparse_top_k_categorical_accuracy: 0.7235\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 6s 20ms/step - loss: 3.2961 - sparse_categorical_accuracy: 0.5729 - sparse_top_k_categorical_accuracy: 0.7280 - val_loss: 3.3026 - val_sparse_categorical_accuracy: 0.5499 - val_sparse_top_k_categorical_accuracy: 0.7217\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2751 - sparse_categorical_accuracy: 0.5924 - sparse_top_k_categorical_accuracy: 0.7276 - val_loss: 3.2957 - val_sparse_categorical_accuracy: 0.5543 - val_sparse_top_k_categorical_accuracy: 0.7217\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2655 - sparse_categorical_accuracy: 0.6008 - sparse_top_k_categorical_accuracy: 0.7290 - val_loss: 3.3022 - val_sparse_categorical_accuracy: 0.5490 - val_sparse_top_k_categorical_accuracy: 0.7231\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2616 - sparse_categorical_accuracy: 0.6041 - sparse_top_k_categorical_accuracy: 0.7295 - val_loss: 3.3015 - val_sparse_categorical_accuracy: 0.5503 - val_sparse_top_k_categorical_accuracy: 0.7235\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 6s 21ms/step - loss: 3.2595 - sparse_categorical_accuracy: 0.6059 - sparse_top_k_categorical_accuracy: 0.7322 - val_loss: 3.3064 - val_sparse_categorical_accuracy: 0.5454 - val_sparse_top_k_categorical_accuracy: 0.7266\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 6s 21ms/step - loss: 3.2591 - sparse_categorical_accuracy: 0.6063 - sparse_top_k_categorical_accuracy: 0.7327 - val_loss: 3.3025 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7231\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2588 - sparse_categorical_accuracy: 0.6062 - sparse_top_k_categorical_accuracy: 0.7332 - val_loss: 3.2992 - val_sparse_categorical_accuracy: 0.5521 - val_sparse_top_k_categorical_accuracy: 0.7257\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 5s 18ms/step - loss: 3.2577 - sparse_categorical_accuracy: 0.6073 - sparse_top_k_categorical_accuracy: 0.7363 - val_loss: 3.2981 - val_sparse_categorical_accuracy: 0.5516 - val_sparse_top_k_categorical_accuracy: 0.7306\n",
    "CPU times: user 18.9 s, sys: 3.86 s, total: 22.7 s\n",
    "Wall time: 1min 1s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹\n",
    "\n",
    "TensorFlowè®­ç»ƒå¥½çš„æ¨¡å‹ä»¥tensorflowåŸç”Ÿæ–¹å¼ä¿å­˜æˆprotobufæ–‡ä»¶åå¯ä»¥ç”¨è®¸å¤šæ–¹å¼éƒ¨ç½²è¿è¡Œã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šé€šè¿‡ tensorflow-js å¯ä»¥ç”¨javascripè„šæœ¬åŠ è½½æ¨¡å‹å¹¶åœ¨æµè§ˆå™¨ä¸­è¿è¡Œæ¨¡å‹ã€‚\n",
    "\n",
    "é€šè¿‡ tensorflow-lite å¯ä»¥åœ¨ç§»åŠ¨å’ŒåµŒå…¥å¼è®¾å¤‡ä¸ŠåŠ è½½å¹¶è¿è¡ŒTensorFlowæ¨¡å‹ã€‚\n",
    "\n",
    "é€šè¿‡ tensorflow-serving å¯ä»¥åŠ è½½æ¨¡å‹åæä¾›ç½‘ç»œæ¥å£APIæœåŠ¡ï¼Œé€šè¿‡ä»»æ„ç¼–ç¨‹è¯­è¨€å‘é€ç½‘ç»œè¯·æ±‚éƒ½å¯ä»¥è·å–æ¨¡å‹é¢„æµ‹ç»“æœã€‚\n",
    "\n",
    "é€šè¿‡ tensorFlow for Javaæ¥å£ï¼Œå¯ä»¥åœ¨Javaæˆ–è€…spark(scala)ä¸­è°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä¸»è¦ä»‹ç»tensorflow servingéƒ¨ç½²æ¨¡å‹ã€ä½¿ç”¨spark(scala)è°ƒç”¨tensorflowæ¨¡å‹çš„æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã€‡ï¼Œtensorflow servingæ¨¡å‹éƒ¨ç½²æ¦‚è¿°\n",
    "ä½¿ç”¨ tensorflow serving éƒ¨ç½²æ¨¡å‹è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ã€‚\n",
    "\n",
    "* (1) å‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶ã€‚\n",
    "\n",
    "* (2) å®‰è£…tensorflow servingã€‚\n",
    "\n",
    "* (3) å¯åŠ¨tensorflow serving æœåŠ¡ã€‚\n",
    "\n",
    "* (4) å‘APIæœåŠ¡å‘é€è¯·æ±‚ï¼Œè·å–é¢„æµ‹ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_servingã€‹ï¼š\n",
    "https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨tf.keras è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ä¿å­˜æˆprotobufæ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "## æ ·æœ¬æ•°é‡\n",
    "n = 800\n",
    "\n",
    "## ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],\n",
    "    mean = 0.0,stddev= 2.0) # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨\n",
    "\n",
    "## å»ºç«‹æ¨¡å‹\n",
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape = (2,),name =\"inputs\") #è®¾ç½®è¾“å…¥åå­—ä¸ºinputs\n",
    "outputs = layers.Dense(1, name = \"outputs\")(inputs) #è®¾ç½®è¾“å‡ºåå­—ä¸ºoutputs\n",
    "linear = models.Model(inputs = inputs,outputs = outputs)\n",
    "linear.summary()\n",
    "\n",
    "## ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ\n",
    "linear.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 8,epochs = 100)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[1].kernel)\n",
    "tf.print(\"b = \",linear.layers[1].bias)\n",
    "\n",
    "## å°†æ¨¡å‹ä¿å­˜æˆpbæ ¼å¼æ–‡ä»¶\n",
    "export_path = \"./data/linear_model/\"\n",
    "version = \"1\"       #åç»­å¯ä»¥é€šè¿‡ç‰ˆæœ¬å·è¿›è¡Œæ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸ç®¡ç†\n",
    "linear.save(export_path+version, save_format=\"tf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æŸ¥çœ‹ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶\n",
    "!ls {export_path+version}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "assets\tsaved_model.pb\tvariables\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯\n",
    "!saved_model_cli show --dir {export_path+str(version)} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['__saved_model_init_op']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['__saved_model_init_op'] tensor_info:\n",
    "        dtype: DT_INVALID\n",
    "        shape: unknown_rank\n",
    "        name: NoOp\n",
    "  Method name is: \n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['inputs'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 2)\n",
    "        name: serving_default_inputs:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['outputs'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 1)\n",
    "        name: StatefulPartitionedCall:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "If using Keras pass *_constraint arguments to layers.\n",
    "\n",
    "Defined Functions:\n",
    "  Function Name: '__call__'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "\n",
    "  Function Name: '_default_save_signature'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "\n",
    "  Function Name: 'call_and_return_all_conditional_losses'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®‰è£… tensorflow serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®‰è£… tensorflow serving æœ‰2ç§ä¸»è¦æ–¹æ³•ï¼šé€šè¿‡Dockeré•œåƒå®‰è£…ï¼Œé€šè¿‡aptå®‰è£…ã€‚\n",
    "\n",
    "é€šè¿‡Dockeré•œåƒå®‰è£…æ˜¯æœ€ç®€å•ï¼Œæœ€ç›´æ¥çš„æ–¹æ³•ï¼Œæ¨èé‡‡ç”¨ã€‚\n",
    "\n",
    "Dockerå¯ä»¥ç†è§£æˆä¸€ç§å®¹å™¨ï¼Œå…¶ä¸Šé¢å¯ä»¥ç»™å„ç§ä¸åŒçš„ç¨‹åºæä¾›ç‹¬ç«‹çš„è¿è¡Œç¯å¢ƒã€‚\n",
    "\n",
    "ä¸€èˆ¬ä¸šåŠ¡ä¸­ç”¨åˆ°tensorflowçš„ä¼ä¸šéƒ½ä¼šæœ‰è¿ç»´åŒå­¦é€šè¿‡Docker æ­å»º tensorflow serving.\n",
    "\n",
    "æ— éœ€ç®—æ³•å·¥ç¨‹å¸ˆåŒå­¦åŠ¨æ‰‹å®‰è£…ï¼Œä»¥ä¸‹å®‰è£…è¿‡ç¨‹ä»…ä¾›å‚è€ƒã€‚\n",
    "\n",
    "ä¸åŒæ“ä½œç³»ç»Ÿæœºå™¨ä¸Šå®‰è£…Dockerçš„æ–¹æ³•å¯ä»¥å‚ç…§ä»¥ä¸‹é“¾æ¥ã€‚\n",
    "\n",
    "Windows: https://www.runoob.com/docker/windows-docker-install.html\n",
    "\n",
    "MacOs: https://www.runoob.com/docker/macos-docker-install.html\n",
    "\n",
    "CentOS: https://www.runoob.com/docker/centos-docker-install.html\n",
    "\n",
    "å®‰è£…DockeræˆåŠŸåï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤åŠ è½½ tensorflow/serving é•œåƒåˆ°Dockerä¸­\n",
    "\n",
    "docker pull tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œå¯åŠ¨ tensorflow serving æœåŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"/Users/.../data/linear_model/\" \\\n",
    "    -e MODEL_NAME=linear_model \\\n",
    "    tensorflow/serving & >server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œå‘APIæœåŠ¡å‘é€è¯·æ±‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨ä»»ä½•ç¼–ç¨‹è¯­è¨€çš„httpåŠŸèƒ½å‘é€è¯·æ±‚ï¼Œä¸‹é¢ç¤ºèŒƒlinuxçš„ curl å‘½ä»¤å‘é€è¯·æ±‚ï¼Œä»¥åŠPythonçš„requestsåº“å‘é€è¯·æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n",
    "    -X POST http://localhost:8501/v1/models/linear_model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"predictions\": [[3.06546211], [5.01313448]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,requests\n",
    "\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": [[1.0, 2.0], [5.0,7.0]]})\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/linear_model:predict', \n",
    "        data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)[\"predictions\"]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[3.06546211], [6.02843142]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflow2.0è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "\n",
    "æœ¬ç¯‡æ–‡ç« ä»‹ç»åœ¨sparkä¸­è°ƒç”¨è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹çš„æ–¹æ³•ã€‚\n",
    "\n",
    "æœ¬æ–‡å†…å®¹çš„å­¦ä¹ éœ€è¦ä¸€å®šçš„sparkå’ŒscalaåŸºç¡€ã€‚\n",
    "\n",
    "å¦‚æœä½¿ç”¨pysparkçš„è¯ä¼šæ¯”è¾ƒç®€å•ï¼Œåªéœ€è¦åœ¨æ¯ä¸ªexcutorä¸Šç”¨PythonåŠ è½½æ¨¡å‹åˆ†åˆ«é¢„æµ‹å°±å¯ä»¥äº†ã€‚\n",
    "\n",
    "ä½†å·¥ç¨‹ä¸Šä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯scalaç‰ˆæœ¬çš„sparkã€‚\n",
    "\n",
    "æœ¬ç¯‡æ–‡ç« æˆ‘ä»¬é€šè¿‡TensorFlow for Java åœ¨sparkä¸­è°ƒç”¨è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹ã€‚\n",
    "\n",
    "åˆ©ç”¨sparkçš„åˆ†å¸ƒå¼è®¡ç®—èƒ½åŠ›ï¼Œä»è€Œå¯ä»¥è®©è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹åœ¨æˆç™¾ä¸Šåƒçš„æœºå™¨ä¸Šåˆ†å¸ƒå¼å¹¶è¡Œæ‰§è¡Œæ¨¡å‹æ¨æ–­ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã€‡ï¼Œspark-scalaè°ƒç”¨tensorflowæ¨¡å‹æ¦‚è¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨spark(scala)ä¸­è°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹éœ€è¦å®Œæˆä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ã€‚\n",
    "\n",
    "ï¼ˆ1ï¼‰å‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶\n",
    "\n",
    "ï¼ˆ2ï¼‰åˆ›å»ºspark(scala)é¡¹ç›®ï¼Œåœ¨é¡¹ç›®ä¸­æ·»åŠ javaç‰ˆæœ¬çš„tensorflowå¯¹åº”çš„jaråŒ…ä¾èµ–\n",
    "\n",
    "ï¼ˆ3ï¼‰åœ¨spark(scala)é¡¹ç›®ä¸­driverç«¯åŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ\n",
    "\n",
    "ï¼ˆ4ï¼‰åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡RDDåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ\n",
    "\n",
    "ï¼ˆ5ï¼‰ åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡DataFrameåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä½¿ç”¨tf.keras è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ä¿å­˜æˆprotobufæ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "## æ ·æœ¬æ•°é‡\n",
    "n = 800\n",
    "\n",
    "## ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨\n",
    "\n",
    "## å»ºç«‹æ¨¡å‹\n",
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape = (2,),name =\"inputs\") #è®¾ç½®è¾“å…¥åå­—ä¸ºinputs\n",
    "outputs = layers.Dense(1, name = \"outputs\")(inputs) #è®¾ç½®è¾“å‡ºåå­—ä¸ºoutputs\n",
    "linear = models.Model(inputs = inputs,outputs = outputs)\n",
    "linear.summary()\n",
    "\n",
    "## ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ\n",
    "linear.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 8,epochs = 100)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[1].kernel)\n",
    "tf.print(\"b = \",linear.layers[1].bias)\n",
    "\n",
    "## å°†æ¨¡å‹ä¿å­˜æˆpbæ ¼å¼æ–‡ä»¶\n",
    "export_path = \"./data/linear_model/\"\n",
    "version = \"1\"       #åç»­å¯ä»¥é€šè¿‡ç‰ˆæœ¬å·è¿›è¡Œæ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸ç®¡ç†\n",
    "linear.save(export_path+version, save_format=\"tf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {export_path+version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯\n",
    "!saved_model_cli show --dir {export_path+str(version)} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹æ–‡ä»¶ä¿¡æ¯ä¸­è¿™äº›æ ‡çº¢çš„éƒ¨åˆ†éƒ½æ˜¯åé¢æœ‰å¯èƒ½ä¼šç”¨åˆ°çš„ã€‚\n",
    "\n",
    "![](./data/æ¨¡å‹æ–‡ä»¶ä¿¡æ¯.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼Œåˆ›å»ºspark(scala)é¡¹ç›®ï¼Œåœ¨é¡¹ç›®ä¸­æ·»åŠ javaç‰ˆæœ¬çš„tensorflowå¯¹åº”çš„jaråŒ…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½¿ç”¨mavenç®¡ç†é¡¹ç›®ï¼Œéœ€è¦æ·»åŠ å¦‚ä¸‹ jaråŒ…ä¾èµ–\n",
    "\n",
    "```\n",
    "<!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow -->\n",
    "<dependency>\n",
    "    <groupId>org.tensorflow</groupId>\n",
    "    <artifactId>tensorflow</artifactId>\n",
    "    <version>1.15.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "ä¹Ÿå¯ä»¥ä»ä¸‹é¢ç½‘å€ä¸­ç›´æ¥ä¸‹è½½ org.tensorflow.tensorflowçš„jaråŒ…\n",
    "\n",
    "ä»¥åŠå…¶ä¾èµ–çš„org.tensorflow.libtensorflow å’Œ org.tensorflowlibtensorflow_jniçš„jaråŒ… æ”¾åˆ°é¡¹ç›®ä¸­ã€‚\n",
    "\n",
    "https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œ åœ¨spark(scala)é¡¹ç›®ä¸­driverç«¯åŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çš„ç¤ºèŒƒä»£ç åœ¨jupyter notebookä¸­è¿›è¡Œæ¼”ç¤ºï¼Œéœ€è¦å®‰è£…toreeä»¥æ”¯æŒspark(scala)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "//æ³¨ï¼šloadå‡½æ•°çš„ç¬¬äºŒä¸ªå‚æ•°ä¸€èˆ¬éƒ½æ˜¯â€œserveâ€ï¼Œå¯ä»¥ä»æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯ä¸­æ‰¾åˆ°\n",
    "\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//æ³¨ï¼šåœ¨javaç‰ˆæœ¬çš„tensorflowä¸­è¿˜æ˜¯ç±»ä¼¼tensorflow1.0ä¸­é™æ€è®¡ç®—å›¾çš„æ¨¡å¼ï¼Œéœ€è¦å»ºç«‹Session, æŒ‡å®šfeedçš„æ•°æ®å’Œfetchçš„ç»“æœ, ç„¶å run.\n",
    "//æ³¨ï¼šå¦‚æœæœ‰å¤šä¸ªæ•°æ®éœ€è¦å–‚å…¥ï¼Œå¯ä»¥è¿ç»­ç”¨ç”¨å¤šä¸ªfeedæ–¹æ³•\n",
    "//æ³¨ï¼šè¾“å…¥å¿…é¡»æ˜¯floatç±»å‹\n",
    "\n",
    "val sess = bundle.session()\n",
    "val x = tf.Tensor.create(Array(Array(1.0f,2.0f),Array(2.0f,3.0f)))\n",
    "val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "         .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "y.copyTo(result)\n",
    "\n",
    "if(x != null) x.close()\n",
    "if(y != null) y.close()\n",
    "if(sess != null) sess.close()\n",
    "if(bundle != null) bundle.close()  \n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¾“å‡ºå¦‚ä¸‹ï¼š\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9878292))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfDriver.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œåœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡RDDåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬é€šè¿‡å¹¿æ’­æœºåˆ¶å°†Driverç«¯åŠ è½½çš„TensorFlowæ¨¡å‹ä¼ é€’åˆ°å„ä¸ªexcutorä¸Šï¼Œå¹¶åœ¨excutorä¸Šåˆ†å¸ƒå¼åœ°è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"TfRDD\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "//åœ¨Driverç«¯åŠ è½½æ¨¡å‹\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//åˆ©ç”¨å¹¿æ’­å°†æ¨¡å‹å‘é€åˆ°excutorä¸Š\n",
    "val broads = sc.broadcast(bundle)\n",
    "\n",
    "//æ„é€ æ•°æ®é›†\n",
    "val rdd_data = sc.makeRDD(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(6.0f,7.0f),Array(8.0f,3.0f)))\n",
    "\n",
    "//é€šè¿‡mapPartitionsè°ƒç”¨æ¨¡å‹è¿›è¡Œæ‰¹é‡æ¨æ–­\n",
    "val rdd_result = rdd_data.mapPartitions(iter => {\n",
    "    \n",
    "    val arr = iter.toArray\n",
    "    val model = broads.value\n",
    "    val sess = model.session()\n",
    "    val x = tf.Tensor.create(arr)\n",
    "    val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "             .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "    //å°†é¢„æµ‹ç»“æœæ‹·è´åˆ°ç›¸åŒshapeçš„Floatç±»å‹çš„Arrayä¸­\n",
    "    val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "    y.copyTo(result)\n",
    "    result.iterator\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "rdd_result.take(5)\n",
    "bundle.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¾“å‡ºå¦‚ä¸‹ï¼š\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9264367), Array(7.8607616), Array(15.974984))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfRDD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œ åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡DataFrameåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é™¤äº†å¯ä»¥åœ¨Sparkçš„RDDæ•°æ®ä¸Šè°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨æ–­ï¼Œ\n",
    "\n",
    "æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨DataFrameæ•°æ®ä¸Šè°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨æ–­ã€‚\n",
    "\n",
    "ä¸»è¦æ€è·¯æ˜¯å°†æ¨æ–­æ–¹æ³•æ³¨å†Œæˆä¸ºä¸€ä¸ªsparkSQLå‡½æ•°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "object TfDataFrame extends Serializable{\n",
    "    \n",
    "    \n",
    "    def main(args:Array[String]):Unit = {\n",
    "        \n",
    "        val spark = SparkSession\n",
    "        .builder()\n",
    "        .appName(\"TfDataFrame\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "        val sc = spark.sparkContext\n",
    "        \n",
    "        \n",
    "        import spark.implicits._\n",
    "\n",
    "        val bundle = tf.SavedModelBundle \n",
    "           .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "        val broads = sc.broadcast(bundle)\n",
    "        \n",
    "        //æ„é€ é¢„æµ‹å‡½æ•°ï¼Œå¹¶å°†å…¶æ³¨å†ŒæˆsparkSQLçš„udf\n",
    "        val tfpredict = (features:WrappedArray[Float])  => {\n",
    "            val bund = broads.value\n",
    "            val sess = bund.session()\n",
    "            val x = tf.Tensor.create(Array(features.toArray))\n",
    "            val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "                     .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "            val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "            y.copyTo(result)\n",
    "            val y_pred = result(0)(0)\n",
    "            y_pred\n",
    "        }\n",
    "        spark.udf.register(\"tfpredict\",tfpredict)\n",
    "        \n",
    "        //æ„é€ DataFrameæ•°æ®é›†ï¼Œå°†featuresæ”¾åˆ°ä¸€åˆ—ä¸­\n",
    "        val dfdata = sc.parallelize(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(7.0f,8.0f))).toDF(\"features\")\n",
    "        dfdata.show \n",
    "        \n",
    "        //è°ƒç”¨sparkSQLé¢„æµ‹å‡½æ•°ï¼Œå¢åŠ ä¸€ä¸ªæ–°çš„åˆ—ä½œä¸ºy_preds\n",
    "        val dfresult = dfdata.selectExpr(\"features\",\"tfpredict(features) as y_preds\")\n",
    "        dfresult.show \n",
    "        bundle.close\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "TfDataFrame.main(Array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+----------+\n",
    "|  features|\n",
    "+----------+\n",
    "|[1.0, 2.0]|\n",
    "|[3.0, 5.0]|\n",
    "|[7.0, 8.0]|\n",
    "+----------+\n",
    "\n",
    "+----------+---------+\n",
    "|  features|  y_preds|\n",
    "+----------+---------+\n",
    "|[1.0, 2.0]| 3.019596|\n",
    "|[3.0, 5.0]|3.9264367|\n",
    "|[7.0, 8.0]| 8.828995|\n",
    "+----------+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šæˆ‘ä»¬åˆ†åˆ«åœ¨spark çš„RDDæ•°æ®ç»“æ„å’ŒDataFrameæ•°æ®ç»“æ„ä¸Šå®ç°äº†è°ƒç”¨ä¸€ä¸ªtf.keraså®ç°çš„çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æ¨æ–­ã€‚\n",
    "\n",
    "åœ¨æœ¬ä¾‹åŸºç¡€ä¸Šç¨ä½œä¿®æ”¹åˆ™å¯ä»¥ç”¨sparkè°ƒç”¨è®­ç»ƒå¥½çš„å„ç§å¤æ‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æ¨æ–­ã€‚\n",
    "\n",
    "ä½†å®é™…ä¸Štensorflowå¹¶ä¸ä»…ä»…é€‚åˆå®ç°ç¥ç»ç½‘ç»œï¼Œå…¶åº•å±‚çš„è®¡ç®—å›¾è¯­è¨€å¯ä»¥è¡¨è¾¾å„ç§æ•°å€¼è®¡ç®—è¿‡ç¨‹ã€‚\n",
    "\n",
    "åˆ©ç”¨å…¶ä¸°å¯Œçš„ä½é˜¶APIï¼Œæˆ‘ä»¬å¯ä»¥åœ¨tensorflow2.0ä¸Šå®ç°ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œ\n",
    "\n",
    "ç»“åˆtf.Moduleæä¾›çš„ä¾¿æ·çš„å°è£…åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒå¥½çš„ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹å¯¼å‡ºæˆæ¨¡å‹æ–‡ä»¶å¹¶åœ¨sparkä¸Šåˆ†å¸ƒå¼è°ƒç”¨æ‰§è¡Œã€‚\n",
    "\n",
    "è¿™æ— ç–‘ä¸ºæˆ‘ä»¬çš„å·¥ç¨‹åº”ç”¨æä¾›äº†å·¨å¤§çš„æƒ³è±¡ç©ºé—´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
