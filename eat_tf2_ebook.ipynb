{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《30天吃掉那只 TensorFlow2.0 》开篇辞 🔥🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 gitbook电子书地址： https://lyhue1991.github.io/eat_tensorflow2_in_30_days\n",
    "\n",
    "🚀 github项目地址：https://github.com/lyhue1991/eat_tensorflow2_in_30_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，TensorFlow2 🍎 还是 Pytorch🔥\n",
    "\n",
    "先说结论:\n",
    "\n",
    "**如果是工程师，应该优先选TensorFlow2.**\n",
    "\n",
    "**如果是学生或者研究人员，应该优先选择Pytorch.**\n",
    "\n",
    "**如果时间足够，最好TensorFlow2和Pytorch都要学习掌握。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理由如下：\n",
    "\n",
    "* 1，**在工业界最重要的是模型落地，目前国内的大部分互联网企业只支持TensorFlow模型的在线部署，不支持Pytorch。** 并且工业界更加注重的是模型的高可用性，许多时候使用的都是成熟的模型架构，调试需求并不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2，**研究人员最重要的是快速迭代发表文章，需要尝试一些较新的模型架构。而Pytorch在易用性上相比TensorFlow2有一些优势，更加方便调试。** 并且在2019年以来在学术界占领了大半壁江山，能够找到的相应最新研究成果更多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3，TensorFlow2和Pytorch实际上整体风格已经非常相似了，学会了其中一个，学习另外一个将比较容易。两种框架都掌握的话，能够参考的开源模型案例更多，并且可以方便地在两种框架之间切换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，Keras🍏 和 tf.keras 🍎\n",
    "\n",
    "先说结论：\n",
    "\n",
    "**Keras库在2.3.0版本后将不再更新，用户应该使用tf.keras。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras可以看成是一种深度学习框架的高阶接口规范，它帮助用户以更简洁的形式定义和训练深度学习网络。\n",
    "\n",
    "使用pip安装的Keras库同时在tensorflow,theano,CNTK等后端基础上进行了这种高阶接口规范的实现。\n",
    "\n",
    "而tf.keras是在TensorFlow中以TensorFlow低阶API为基础实现的这种高阶接口，它是Tensorflow的一个子模块。\n",
    "\n",
    "tf.keras绝大部分功能和兼容多种后端的Keras库用法完全一样，但并非全部，它和TensorFlow之间的结合更为紧密。\n",
    "\n",
    "随着谷歌对Keras的收购，Keras库2.3.0版本后也将不再进行更新，用户应当使用tf.keras而不是使用pip安装的Keras.\n",
    "\n",
    "### 三，本书📖面向读者 👼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本书假定读者有一定的机器学习和深度学习基础，使用过Keras或者Tensorflow1.0或者Pytorch搭建训练过模型。**\n",
    "\n",
    "**对于没有任何机器学习和深度学习基础的同学，建议在学习本书时同步参考学习《Python深度学习》一书。**\n",
    "\n",
    "《Python深度学习》这本书是Keras之父Francois Chollet所著，该书假定读者无任何机器学习知识，以Keras为工具，\n",
    "\n",
    "使用丰富的范例示范深度学习的最佳实践，该书通俗易懂，**全书没有一个数学公式，注重培养读者的深度学习直觉。**。\n",
    "\n",
    "该书电子版下载链接：https://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA 提取码：wtzo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，本书写作风格 🍉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本书是一本对人类用户极其友善的TensorFlow2.0入门工具书，不刻意恶心读者是本书的底限要求，Don't let me think是本书的最高追求。**\n",
    "\n",
    "本书主要是在参考TensorFlow官方文档和函数doc文档基础上整理写成的。\n",
    "\n",
    "但本书在篇章结构和范例选取上做了大量的优化。\n",
    "\n",
    "不同于官方文档混乱的篇章结构，既有教程又有指南，缺少整体的编排逻辑。\n",
    "\n",
    "本书按照内容难易程度、读者检索习惯和TensorFlow自身的层次结构设计内容，循序渐进，层次清晰，方便按照功能查找相应范例。\n",
    "\n",
    "不同于官方文档冗长的范例代码，本书在范例设计上尽可能简约化和结构化，增强范例易读性和通用性，大部分代码片段在实践中可即取即用。\n",
    "\n",
    "**如果说通过学习TensorFlow官方文档掌握TensorFlow2.0的难度大概是9的话，那么通过学习本书掌握TensorFlow2.0的难度应该大概是3.**\n",
    "\n",
    "谨以下图对比一下TensorFlow官方教程与本教程的差异。\n",
    "\n",
    "![](./data/30天吃掉那个TF2.0.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，本书学习方案 ⏰\n",
    "\n",
    "**1，学习计划**\n",
    "\n",
    "本书是作者利用工作之余和疫情放假期间大概2个月写成的，大部分读者应该在30天可以完全学会。\n",
    "\n",
    "预计每天花费的学习时间在30分钟到2个小时之间。\n",
    "\n",
    "当然，本书也非常适合作为TensorFlow的工具手册在工程落地时作为范例库参考。\n",
    "\n",
    "**点击学习内容蓝色标题即可进入该章节。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|日期 | 学习内容                                                       | 内容难度   | 预计学习时间 | 更新状态|\n",
    "|----:|:--------------------------------------------------------------|-----------:|----------:|-----:|\n",
    "|&nbsp;|[**一、TensorFlow的建模流程**](./一、TensorFlow的建模流程.md)    |⭐️   |   0hour   |✅    |\n",
    "|day1 |  [1-1,结构化数据建模流程范例](./1-1,结构化数据建模流程范例.md)    | ⭐️⭐️⭐️ |   1hour    |✅    |\n",
    "|day2 |[1-2,图片数据建模流程范例](./1-2,图片数据建模流程范例.md)    | ⭐️⭐️⭐️⭐️  |   2hour    |✅    |\n",
    "|day3 |  [1-3,文本数据建模流程范例](./1-3,文本数据建模流程范例.md)   | ⭐️⭐️⭐️⭐️⭐️  |   2hour    |✅    |\n",
    "|day4 |  [1-4,时间序列数据建模流程范例](./1-4,时间序列数据建模流程范例.md)   | ⭐️⭐️⭐️⭐️⭐️  |   2hour    |✅    |\n",
    "|&nbsp;    |[**二、TensorFlow的核心概念**](./二、TensorFlow的核心概念.md)  | ⭐️  |  0hour |✅  |\n",
    "|day5 |  [2-1,张量数据结构](./2-1,张量数据结构.md)  | ⭐️⭐️⭐️⭐️   |   1hour    |✅    |\n",
    "|day6 |  [2-2,三种计算图](./2-2,三种计算图.md)  | ⭐️⭐️⭐️⭐️⭐️   |   2hour    |✅    |\n",
    "|day7 |  [2-3,自动微分机制](./2-3,自动微分机制.md)  | ⭐️⭐️⭐️   |   1hour    |✅    |\n",
    "|&nbsp; |[**三、TensorFlow的层次结构**](./三、TensorFlow的层次结构.md) |   ⭐️  |  0hour   |✅  |\n",
    "|day8 |  [3-1,低阶API示范](./3-1,低阶API示范.md)   | ⭐️⭐️   |   0.5hour    |✅   |\n",
    "|day9 |  [3-2,中阶API示范](./3-2,中阶API示范.md)   | ⭐️⭐️⭐️   |   0.5hour    |✅  |\n",
    "|day10 |  [3-3,高阶API示范](./3-3,高阶API示范.md)  | ⭐️⭐️⭐️   |   0.5hour    |✅  |\n",
    "|&nbsp; |[**四、TensorFlow的低阶API**](./四、TensorFlow的低阶API.md) |⭐️    | 0hour|✅  |\n",
    "|day11|  [4-1,张量的结构操作](./4-1,张量的结构操作.md)  | ⭐️⭐️⭐️⭐️⭐️   |   2hour    |✅   |\n",
    "|day12|  [4-2,张量的数学运算](./4-2,张量的数学运算.md)   | ⭐️⭐️⭐️⭐️   |   1hour    |✅  |\n",
    "|day13|  [4-3,AutoGraph的使用规范](./4-3,AutoGraph的使用规范.md)| ⭐️⭐️⭐️   |   0.5hour    |✅  |\n",
    "|day14|  [4-4,AutoGraph的机制原理](./4-4,AutoGraph的机制原理.md)    | ⭐️⭐️⭐️⭐️⭐️   |   2hour    |✅  |\n",
    "|day15|  [4-5,AutoGraph和tf.Module](./4-5,AutoGraph和tf.Module.md)  | ⭐️⭐️⭐️⭐️   |   1hour    |✅  |\n",
    "|&nbsp; |[**五、TensorFlow的中阶API**](./五、TensorFlow的中阶API.md) |  ⭐️  | 0hour|✅ |\n",
    "|day16|  [5-1,数据管道Dataset](./5-1,数据管道Dataset.md)   | ⭐️⭐️⭐️⭐️⭐️   |   2hour    |✅  |\n",
    "|day17|  [5-2,特征列feature_column](./5-2,特征列feature_column.md)   | ⭐️⭐️⭐️⭐️   |   1hour    |✅  |\n",
    "|day18|  [5-3,激活函数activation](./5-3,激活函数activation.md)    | ⭐️⭐️⭐️   |   0.5hour    |✅   |\n",
    "|day19|  [5-4,模型层layers](./5-4,模型层layers.md)  | ⭐️⭐️⭐️   |   1hour    |✅  |\n",
    "|day20|  [5-5,损失函数losses](./5-5,损失函数losses.md)    | ⭐️⭐️⭐️   |   1hour    |✅  |\n",
    "|day21|  [5-6,评估指标metrics](./5-6,评估指标metrics.md)    | ⭐️⭐️⭐️   |   1hour    |✅   |\n",
    "|day22|  [5-7,优化器optimizers](./5-7,优化器optimizers.md)    | ⭐️⭐️⭐️   |   0.5hour    |✅   |\n",
    "|day23|  [5-8,回调函数callbacks](./5-8,回调函数callbacks.md)   | ⭐️⭐️⭐️⭐️   |   1hour    |✅   |\n",
    "|&nbsp; |[**六、TensorFlow的高阶API**](./六、TensorFlow的高阶API.md)|    ⭐️ | 0hour|✅  |\n",
    "|day24|  [6-1,构建模型的3种方法](./6-1,构建模型的3种方法.md)   | ⭐️⭐️⭐️   |   1hour    |✅ |\n",
    "|day25|  [6-2,训练模型的3种方法](./6-2,训练模型的3种方法.md)  | ⭐️⭐️⭐️⭐️   |   1hour    |✅   |\n",
    "|day26|  [6-3,使用单GPU训练模型](./6-3,使用单GPU训练模型.md)    | ⭐️⭐️   |   0.5hour    |✅   |\n",
    "|day27|  [6-4,使用多GPU训练模型](./6-4,使用多GPU训练模型.md)    | ⭐️⭐️   |   0.5hour    |✅  |\n",
    "|day28|  [6-5,使用TPU训练模型](./6-5,使用TPU训练模型.md)   | ⭐️⭐️   |   0.5hour    |✅  |\n",
    "|day29| [6-6,使用tensorflow-serving部署模型](./6-6,使用tensorflow-serving部署模型.md) | ⭐️⭐️⭐️⭐️| 1hour |✅   |\n",
    "|day30| [6-7,使用spark-scala调用tensorflow模型](./6-7,使用spark-scala调用tensorflow模型.md) | ⭐️⭐️⭐️⭐️⭐️|2hour|✅  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，学习环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本书全部源码在jupyter中编写测试通过，建议通过git克隆到本地，并在jupyter中交互式运行学习。\n",
    "\n",
    "为了直接能够在jupyter中打开markdown文件，建议安装jupytext，将markdown转换成ipnb。\n",
    "\n",
    "此外，也可以关注公众号”**Python与算法之美**“ ，后台回复关键字：**tf**，获取本书ipynb源码的下载链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#克隆本书源码到本地\n",
    "#!git clone https://github.com/lyhue1991/eat_tensorflow2_in_30_days\n",
    "\n",
    "#建议在jupyter notebook 上安装jupytext，以便能够将本书各章节markdown文件视作ipynb文件运行\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext\n",
    "    \n",
    "#建议在jupyter notebook 上安装最新版本tensorflow 测试本书中的代码\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#注：本书全部代码在tensorflow 2.1版本测试通过\n",
    "tf.print(\"tensorflow version:\",tf.__version__)\n",
    "\n",
    "a = tf.constant(\"hello\")\n",
    "b = tf.constant(\"tensorflow2\")\n",
    "c = tf.strings.join([a,b],\" \")\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow version: 2.1.0\n",
    "hello tensorflow2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，鼓励和联系作者 🎈🎈\n",
    "\n",
    "**如果本书对你有所帮助，想鼓励一下作者，记得给本项目加一颗星星star⭐️，并分享给你的朋友们喔😊!** \n",
    "\n",
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"**Python与算法之美**\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "如果有想要获取本书的jupyter notebook源代码的小伙伴，也可以关注公众号，在后台回复关键字：**tf**，获取本书全部代码和数据集下载链接。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、TensorFlow的建模流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管TensorFlow设计上足够灵活，可以用于进行各种复杂的数值计算。\n",
    "\n",
    "但通常人们使用TensorFlow来实现机器学习模型，尤其常用于实现神经网络模型。\n",
    "\n",
    "从原理上说可以使用张量构建计算图来定义神经网络，并通过自动微分机制训练模型。\n",
    "\n",
    "但为简洁起见，一般推荐使用TensorFlow的高层次keras接口来实现神经网络网模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用TensorFlow实现神经网络模型的一般流程包括：\n",
    "\n",
    "1，准备数据\n",
    "\n",
    "2，定义模型\n",
    "\n",
    "3，训练模型\n",
    "\n",
    "4，评估模型\n",
    "\n",
    "5，使用模型\n",
    "\n",
    "6，保存模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对新手来说，其中最困难的部分实际上是准备数据过程。** \n",
    "\n",
    "我们在实践中通常会遇到的数据类型包括结构化数据，图片数据，文本数据，时间序列数据。\n",
    "\n",
    "我们将分别以titanic生存预测问题，cifar2图片分类问题，imdb电影评论分类问题，国内新冠疫情结束时间预测问题为例，演示应用tensorflow对这四类数据的建模方法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![](./data/Python与算法之美logo.jpg)\n",
    "# 1-1,结构化数据建模流程范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic数据集的目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存。\n",
    "\n",
    "结构化数据一般会使用Pandas中的DataFrame进行预处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers\n",
    "\n",
    "dftrain_raw = pd.read_csv('./data/titanic/train.csv')\n",
    "dftest_raw = pd.read_csv('./data/titanic/test.csv')\n",
    "dftrain_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-数据集展示.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字段说明：\n",
    "\n",
    "* Survived:0代表死亡，1代表存活【y标签】\n",
    "* Pclass:乘客所持票类，有三种值(1,2,3) 【转换成onehot编码】\n",
    "* Name:乘客姓名 【舍去】\n",
    "* Sex:乘客性别 【转换成bool特征】\n",
    "* Age:乘客年龄(有缺失) 【数值特征，添加“年龄是否缺失”作为辅助特征】\n",
    "* SibSp:乘客兄弟姐妹/配偶的个数(整数值) 【数值特征】\n",
    "* Parch:乘客父母/孩子的个数(整数值)【数值特征】\n",
    "* Ticket:票号(字符串)【舍去】\n",
    "* Fare:乘客所持票的价格(浮点数，0-500不等) 【数值特征】\n",
    "* Cabin:乘客所在船舱(有缺失) 【添加“所在船舱是否缺失”作为辅助特征】\n",
    "* Embarked:乘客登船港口:S、C、Q(有缺失)【转换成onehot编码，四维度 S,C,Q,nan】\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Pandas的数据可视化功能我们可以简单地进行探索性数据分析EDA（Exploratory Data Analysis）。\n",
    "\n",
    "label分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw['Survived'].value_counts().plot(kind = 'bar',\n",
    "     figsize = (12,8),fontsize=15,rot = 0)\n",
    "ax.set_ylabel('Counts',fontsize = 15)\n",
    "ax.set_xlabel('Survived',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-Label分布.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年龄分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw['Age'].plot(kind = 'hist',bins = 20,color= 'purple',\n",
    "                    figsize = (12,8),fontsize=15)\n",
    "\n",
    "ax.set_ylabel('Frequency',fontsize = 15)\n",
    "ax.set_xlabel('Age',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-年龄分布.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年龄和label的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "ax = dftrain_raw.query('Survived == 0')['Age'].plot(kind = 'density',\n",
    "                      figsize = (12,8),fontsize=15)\n",
    "dftrain_raw.query('Survived == 1')['Age'].plot(kind = 'density',\n",
    "                      figsize = (12,8),fontsize=15)\n",
    "ax.legend(['Survived==0','Survived==1'],fontsize = 12)\n",
    "ax.set_ylabel('Density',fontsize = 15)\n",
    "ax.set_xlabel('Age',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-年龄相关性.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面为正式的数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dfdata):\n",
    "\n",
    "    dfresult= pd.DataFrame()\n",
    "\n",
    "    #Pclass\n",
    "    dfPclass = pd.get_dummies(dfdata['Pclass'])\n",
    "    dfPclass.columns = ['Pclass_' +str(x) for x in dfPclass.columns ]\n",
    "    dfresult = pd.concat([dfresult,dfPclass],axis = 1)\n",
    "\n",
    "    #Sex\n",
    "    dfSex = pd.get_dummies(dfdata['Sex'])\n",
    "    dfresult = pd.concat([dfresult,dfSex],axis = 1)\n",
    "\n",
    "    #Age\n",
    "    dfresult['Age'] = dfdata['Age'].fillna(0)\n",
    "    dfresult['Age_null'] = pd.isna(dfdata['Age']).astype('int32')\n",
    "\n",
    "    #SibSp,Parch,Fare\n",
    "    dfresult['SibSp'] = dfdata['SibSp']\n",
    "    dfresult['Parch'] = dfdata['Parch']\n",
    "    dfresult['Fare'] = dfdata['Fare']\n",
    "\n",
    "    #Carbin\n",
    "    dfresult['Cabin_null'] =  pd.isna(dfdata['Cabin']).astype('int32')\n",
    "\n",
    "    #Embarked\n",
    "    dfEmbarked = pd.get_dummies(dfdata['Embarked'],dummy_na=True)\n",
    "    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]\n",
    "    dfresult = pd.concat([dfresult,dfEmbarked],axis = 1)\n",
    "\n",
    "    return(dfresult)\n",
    "\n",
    "x_train = preprocessing(dftrain_raw)\n",
    "y_train = dftrain_raw['Survived'].values\n",
    "\n",
    "x_test = preprocessing(dftest_raw)\n",
    "y_test = dftest_raw['Survived'].values\n",
    "\n",
    "print(\"x_train.shape =\", x_train.shape )\n",
    "print(\"x_test.shape =\", x_test.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x_train.shape = (712, 15)\n",
    "x_test.shape = (179, 15)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "此处选择使用最简单的Sequential，按层顺序模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20,activation = 'relu',input_shape=(15,)))\n",
    "model.add(layers.Dense(10,activation = 'relu' ))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid' ))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 20)                320       \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                210       \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 541\n",
    "Trainable params: 541\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们选择最常用也最简单的内置fit方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二分类问题选择二元交叉熵损失函数\n",
    "model.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['AUC'])\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    batch_size= 64,\n",
    "                    epochs= 30,\n",
    "                    validation_split=0.2 #分割一部分训练数据用于验证\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train on 569 samples, validate on 143 samples\n",
    "Epoch 1/30\n",
    "569/569 [==============================] - 1s 2ms/sample - loss: 3.5841 - AUC: 0.4079 - val_loss: 3.4429 - val_AUC: 0.4129\n",
    "Epoch 2/30\n",
    "569/569 [==============================] - 0s 102us/sample - loss: 2.6093 - AUC: 0.3967 - val_loss: 2.4886 - val_AUC: 0.4139\n",
    "Epoch 3/30\n",
    "569/569 [==============================] - 0s 68us/sample - loss: 1.8375 - AUC: 0.4003 - val_loss: 1.7383 - val_AUC: 0.4223\n",
    "Epoch 4/30\n",
    "569/569 [==============================] - 0s 83us/sample - loss: 1.2545 - AUC: 0.4390 - val_loss: 1.1936 - val_AUC: 0.4765\n",
    "Epoch 5/30\n",
    "569/569 [==============================] - ETA: 0s - loss: 1.4435 - AUC: 0.375 - 0s 90us/sample - loss: 0.9141 - AUC: 0.5192 - val_loss: 0.8274 - val_AUC: 0.5584\n",
    "Epoch 6/30\n",
    "569/569 [==============================] - 0s 110us/sample - loss: 0.7052 - AUC: 0.6290 - val_loss: 0.6596 - val_AUC: 0.6880\n",
    "Epoch 7/30\n",
    "569/569 [==============================] - 0s 90us/sample - loss: 0.6410 - AUC: 0.7086 - val_loss: 0.6519 - val_AUC: 0.6845\n",
    "Epoch 8/30\n",
    "569/569 [==============================] - 0s 93us/sample - loss: 0.6246 - AUC: 0.7080 - val_loss: 0.6480 - val_AUC: 0.6846\n",
    "Epoch 9/30\n",
    "569/569 [==============================] - 0s 73us/sample - loss: 0.6088 - AUC: 0.7113 - val_loss: 0.6497 - val_AUC: 0.6838\n",
    "Epoch 10/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.6051 - AUC: 0.7117 - val_loss: 0.6454 - val_AUC: 0.6873\n",
    "Epoch 11/30\n",
    "569/569 [==============================] - 0s 96us/sample - loss: 0.5972 - AUC: 0.7218 - val_loss: 0.6369 - val_AUC: 0.6888\n",
    "Epoch 12/30\n",
    "569/569 [==============================] - 0s 92us/sample - loss: 0.5918 - AUC: 0.7294 - val_loss: 0.6330 - val_AUC: 0.6908\n",
    "Epoch 13/30\n",
    "569/569 [==============================] - 0s 75us/sample - loss: 0.5864 - AUC: 0.7363 - val_loss: 0.6281 - val_AUC: 0.6948\n",
    "Epoch 14/30\n",
    "569/569 [==============================] - 0s 104us/sample - loss: 0.5832 - AUC: 0.7426 - val_loss: 0.6240 - val_AUC: 0.7030\n",
    "Epoch 15/30\n",
    "569/569 [==============================] - 0s 74us/sample - loss: 0.5777 - AUC: 0.7507 - val_loss: 0.6200 - val_AUC: 0.7066\n",
    "Epoch 16/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.5726 - AUC: 0.7569 - val_loss: 0.6155 - val_AUC: 0.7132\n",
    "Epoch 17/30\n",
    "569/569 [==============================] - 0s 99us/sample - loss: 0.5674 - AUC: 0.7643 - val_loss: 0.6070 - val_AUC: 0.7255\n",
    "Epoch 18/30\n",
    "569/569 [==============================] - 0s 97us/sample - loss: 0.5631 - AUC: 0.7721 - val_loss: 0.6061 - val_AUC: 0.7305\n",
    "Epoch 19/30\n",
    "569/569 [==============================] - 0s 73us/sample - loss: 0.5580 - AUC: 0.7792 - val_loss: 0.6027 - val_AUC: 0.7332\n",
    "Epoch 20/30\n",
    "569/569 [==============================] - 0s 85us/sample - loss: 0.5533 - AUC: 0.7861 - val_loss: 0.5997 - val_AUC: 0.7366\n",
    "Epoch 21/30\n",
    "569/569 [==============================] - 0s 87us/sample - loss: 0.5497 - AUC: 0.7926 - val_loss: 0.5961 - val_AUC: 0.7433\n",
    "Epoch 22/30\n",
    "569/569 [==============================] - 0s 101us/sample - loss: 0.5454 - AUC: 0.7987 - val_loss: 0.5943 - val_AUC: 0.7438\n",
    "Epoch 23/30\n",
    "569/569 [==============================] - 0s 100us/sample - loss: 0.5398 - AUC: 0.8057 - val_loss: 0.5926 - val_AUC: 0.7492\n",
    "Epoch 24/30\n",
    "569/569 [==============================] - 0s 79us/sample - loss: 0.5328 - AUC: 0.8122 - val_loss: 0.5912 - val_AUC: 0.7493\n",
    "Epoch 25/30\n",
    "569/569 [==============================] - 0s 86us/sample - loss: 0.5283 - AUC: 0.8147 - val_loss: 0.5902 - val_AUC: 0.7509\n",
    "Epoch 26/30\n",
    "569/569 [==============================] - 0s 67us/sample - loss: 0.5246 - AUC: 0.8196 - val_loss: 0.5845 - val_AUC: 0.7552\n",
    "Epoch 27/30\n",
    "569/569 [==============================] - 0s 72us/sample - loss: 0.5205 - AUC: 0.8271 - val_loss: 0.5837 - val_AUC: 0.7584\n",
    "Epoch 28/30\n",
    "569/569 [==============================] - 0s 74us/sample - loss: 0.5144 - AUC: 0.8302 - val_loss: 0.5848 - val_AUC: 0.7561\n",
    "Epoch 29/30\n",
    "569/569 [==============================] - 0s 77us/sample - loss: 0.5099 - AUC: 0.8326 - val_loss: 0.5809 - val_AUC: 0.7583\n",
    "Epoch 30/30\n",
    "569/569 [==============================] - 0s 80us/sample - loss: 0.5071 - AUC: 0.8349 - val_loss: 0.5816 - val_AUC: 0.7605\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先评估一下模型在训练集和验证集上的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-Loss曲线.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-1-AUC曲线.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再看一下模型在测试集上的效果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x = x_test,y = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测概率\n",
    "model.predict(x_test[0:10])\n",
    "#model(tf.constant(x_test[0:10].values,dtype = tf.float32)) #等价写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.26501188],\n",
    "       [0.40970832],\n",
    "       [0.44285864],\n",
    "       [0.78408605],\n",
    "       [0.47650957],\n",
    "       [0.43849158],\n",
    "       [0.27426785],\n",
    "       [0.5962582 ],\n",
    "       [0.59476686],\n",
    "       [0.17882936]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测类别\n",
    "model.predict_classes(x_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0],\n",
    "       [0],\n",
    "       [0],\n",
    "       [1],\n",
    "       [0],\n",
    "       [0],\n",
    "       [0],\n",
    "       [1],\n",
    "       [1],\n",
    "       [0]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用Keras方式保存模型，也可以使用TensorFlow原生方式保存。前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署。\n",
    "\n",
    "推荐使用后一种方式进行保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，Keras方式保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构及权重\n",
    "\n",
    "model.save('./data/keras_model.h5')  \n",
    "\n",
    "del model  #删除现有模型\n",
    "\n",
    "# identical to the previous one\n",
    "model = models.load_model('./data/keras_model.h5')\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构\n",
    "json_str = model.to_json()\n",
    "\n",
    "# 恢复模型结构\n",
    "model_json = models.model_from_json(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型权重\n",
    "model.save_weights('./data/keras_model_weight.h5')\n",
    "\n",
    "# 恢复模型结构\n",
    "model_json = models.model_from_json(json_str)\n",
    "model_json.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "# 加载权重\n",
    "model_json.load_weights('./data/keras_model_weight.h5')\n",
    "model_json.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191367897907448, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，TensorFlow原生方式保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存权重，该方式仅仅保存权重张量\n",
    "model.save_weights('./data/tf_model_weights.ckpt',save_format = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署\n",
    "\n",
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5191365896656527, 0.8122605]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2,图片数据建模流程范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar2数据集为cifar10数据集的子集，只包括前两种类别airplane和automobile。\n",
    "\n",
    "训练集有airplane和automobile图片各5000张，测试集有airplane和automobile图片各1000张。\n",
    "\n",
    "cifar2任务的目标是训练一个模型来对飞机airplane和机动车automobile两种图片进行分类。\n",
    "\n",
    "我们准备的Cifar2数据集的文件结构如下所示。\n",
    "\n",
    "![](./data/cifar2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在tensorflow中准备图片数据的常用方案有两种，第一种是使用tf.keras中的ImageDataGenerator工具构建图片数据生成器。\n",
    "\n",
    "第二种是使用tf.data.Dataset搭配tf.image中的一些图片处理方法构建数据管道。\n",
    "\n",
    "第一种方法更为简单，其使用范例可以参考以下文章。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67466552\n",
    "\n",
    "第二种方法是TensorFlow的原生方法，更加灵活，使用得当的话也可以获得更好的性能。\n",
    "\n",
    "我们此处介绍第二种方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import datasets,layers,models\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = tf.constant(1,tf.int8) if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") \\\n",
    "            else tf.constant(0,tf.int8)\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img,size)/255.0\n",
    "    return(img,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用并行化预处理num_parallel_calls 和预存数据prefetch来提升性能\n",
    "ds_train = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\") \\\n",
    "           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "           .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "           .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "\n",
    "ds_test = tf.data.Dataset.list_files(\"./data/cifar2/test/*/*.jpg\") \\\n",
    "           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "           .batch(BATCH_SIZE) \\\n",
    "           .prefetch(tf.data.experimental.AUTOTUNE)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "#查看部分样本\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "for i,(img,label) in enumerate(ds_train.unbatch().take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-图片预览.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ds_train.take(1):\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(100, 32, 32, 3) (100,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "此处选择使用函数式API构建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #清空会话\n",
    "\n",
    "inputs = layers.Input(shape=(32,32,3))\n",
    "x = layers.Conv2D(32,kernel_size=(3,3))(inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(64,kernel_size=(5,5))(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32,activation='relu')(x)\n",
    "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs = inputs,outputs = outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 5, 5, 64)          0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 1600)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 32)                51232     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 33        \n",
    "=================================================================\n",
    "Total params: 103,425\n",
    "Trainable params: 103,425\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们选择最常用也最简单的内置fit方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "history = model.fit(ds_train,epochs= 10,validation_data=ds_test,\n",
    "                    callbacks = [tensorboard_callback],workers = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 100 steps, validate for 20 steps\n",
    "Epoch 1/10\n",
    "100/100 [==============================] - 16s 156ms/step - loss: 0.4830 - accuracy: 0.7697 - val_loss: 0.3396 - val_accuracy: 0.8475\n",
    "Epoch 2/10\n",
    "100/100 [==============================] - 14s 142ms/step - loss: 0.3437 - accuracy: 0.8469 - val_loss: 0.2997 - val_accuracy: 0.8680\n",
    "Epoch 3/10\n",
    "100/100 [==============================] - 13s 131ms/step - loss: 0.2871 - accuracy: 0.8777 - val_loss: 0.2390 - val_accuracy: 0.9015\n",
    "Epoch 4/10\n",
    "100/100 [==============================] - 12s 117ms/step - loss: 0.2410 - accuracy: 0.9040 - val_loss: 0.2005 - val_accuracy: 0.9195\n",
    "Epoch 5/10\n",
    "100/100 [==============================] - 13s 130ms/step - loss: 0.1992 - accuracy: 0.9213 - val_loss: 0.1949 - val_accuracy: 0.9180\n",
    "Epoch 6/10\n",
    "100/100 [==============================] - 14s 136ms/step - loss: 0.1737 - accuracy: 0.9323 - val_loss: 0.1723 - val_accuracy: 0.9275\n",
    "Epoch 7/10\n",
    "100/100 [==============================] - 14s 139ms/step - loss: 0.1531 - accuracy: 0.9412 - val_loss: 0.1670 - val_accuracy: 0.9310\n",
    "Epoch 8/10\n",
    "100/100 [==============================] - 13s 134ms/step - loss: 0.1299 - accuracy: 0.9525 - val_loss: 0.1553 - val_accuracy: 0.9340\n",
    "Epoch 9/10\n",
    "100/100 [==============================] - 14s 137ms/step - loss: 0.1158 - accuracy: 0.9556 - val_loss: 0.1581 - val_accuracy: 0.9340\n",
    "Epoch 10/10\n",
    "100/100 [==============================] - 14s 142ms/step - loss: 0.1006 - accuracy: 0.9617 - val_loss: 0.1614 - val_accuracy: 0.9345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ./data/keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在tensorboard中查看模型\n",
    "notebook.start(\"--logdir ./data/keras_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-tensorboard.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dfhistory = pd.DataFrame(history.history)\n",
    "dfhistory.index = range(1,len(dfhistory) + 1)\n",
    "dfhistory.index.name = 'epoch'\n",
    "\n",
    "dfhistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-dfhistory.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-Loss曲线.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-2-Accuracy曲线.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以使用evaluate对数据进行评估\n",
    "val_loss,val_accuracy = model.evaluate(ds_test,workers=4)\n",
    "print(val_loss,val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.16139143370091916 0.9345\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用model.predict(ds_test)进行预测。\n",
    "\n",
    "也可以使用model.predict_on_batch(x_test)对一个批量进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[9.9996173e-01],\n",
    "       [9.5104784e-01],\n",
    "       [2.8648047e-04],\n",
    "       ...,\n",
    "       [1.1484033e-03],\n",
    "       [3.5589080e-02],\n",
    "       [9.8537153e-01]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ds_test.take(1):\n",
    "    print(model.predict_on_batch(x[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[3.8065155e-05]\n",
    " [8.8236779e-01]\n",
    " [9.1433197e-01]\n",
    " [9.9921846e-01]\n",
    " [6.4052093e-01]\n",
    " [4.9970779e-03]\n",
    " [2.6735585e-04]\n",
    " [9.9842811e-01]\n",
    " [7.9198682e-01]\n",
    " [7.4823302e-01]\n",
    " [8.7208226e-03]\n",
    " [9.3951421e-03]\n",
    " [9.9790359e-01]\n",
    " [9.9998581e-01]\n",
    " [2.1642199e-05]\n",
    " [1.7915063e-02]\n",
    " [2.5839690e-02]\n",
    " [9.7538447e-01]\n",
    " [9.7393811e-01]\n",
    " [9.7333014e-01]], shape=(20, 1), dtype=float32)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用TensorFlow原生方式保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存权重，该方式仅仅保存权重张量\n",
    "model.save_weights('./data/tf_model_weights.ckpt',save_format = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署\n",
    "\n",
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.16139124035835267, 0.9345]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "# 1-3,文本数据建模流程范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。\n",
    "\n",
    "训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。\n",
    "\n",
    "文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在tensorflow中完成文本数据预处理的常用方案有两种，第一种是利用tf.keras.preprocessing中的Tokenizer词典构建工具和tf.keras.utils.Sequence构建文本数据生成器管道。\n",
    "\n",
    "第二种是使用tf.data.Dataset搭配.keras.layers.experimental.preprocessing.TextVectorization预处理层。\n",
    "\n",
    "第一种方法较为复杂，其使用范例可以参考以下文章。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67697840\n",
    "\n",
    "第二种方法为TensorFlow原生方式，相对也更加简单一些。\n",
    "\n",
    "我们此处介绍第二种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/电影评论.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path =  \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "#构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)\n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "\n",
    "#单词编码\n",
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not', b'you', b'his', b'are', b'have', b'be', b'he', b'one', b'its', b'at', b'all', b'by', b'an', b'they', b'from', b'who', b'so', b'like', b'her', b'just', b'or', b'about', b'has', b'if', b'out', b'some', b'there', b'what', b'good', b'more', b'when', b'very', b'she', b'even', b'my', b'no', b'would', b'up', b'time', b'only', b'which', b'story', b'really', b'their', b'were', b'had', b'see', b'can', b'me', b'than', b'we', b'much', b'well', b'get', b'been', b'will', b'into', b'people', b'also', b'other', b'do', b'bad', b'because', b'great', b'first', b'how', b'him', b'most', b'dont', b'made', b'then', b'them', b'films', b'movies', b'way', b'make', b'could', b'too', b'any', b'after', b'characters']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "此处选择使用继承Model基类构建自定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示自定义模型范例，实际上应该优先使用Sequential或者函数式API\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\n",
    "        self.pool = layers.MaxPool1D()\n",
    "        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(CnnModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n",
    "    \n",
    "model = CnnModel()\n",
    "model.build(input_shape =(None,MAX_LEN))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"cnn_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        multiple                  70000     \n",
    "_________________________________________________________________\n",
    "conv_1 (Conv1D)              multiple                  576       \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) multiple                  0         \n",
    "_________________________________________________________________\n",
    "conv_2 (Conv1D)              multiple                  4224      \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            multiple                  0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  6145      \n",
    "=================================================================\n",
    "Total params: 80,945\n",
    "Trainable params: 80,945\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们通过自定义训练循环训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features,training = False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "        \n",
    "        #此处logs模板需要根据metric具体情况修改\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' \n",
    "        \n",
    "        if epoch%1==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,epochs = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:54:08\n",
    "Epoch=1,Loss:0.442317516,Accuracy:0.7695,Valid Loss:0.323672801,Valid Accuracy:0.8614\n",
    "\n",
    "================================================================================13:54:20\n",
    "Epoch=2,Loss:0.245737702,Accuracy:0.90215,Valid Loss:0.356488883,Valid Accuracy:0.8554\n",
    "\n",
    "================================================================================13:54:32\n",
    "Epoch=3,Loss:0.17360799,Accuracy:0.93455,Valid Loss:0.361132562,Valid Accuracy:0.8674\n",
    "\n",
    "================================================================================13:54:44\n",
    "Epoch=4,Loss:0.113476314,Accuracy:0.95975,Valid Loss:0.483677238,Valid Accuracy:0.856\n",
    "\n",
    "================================================================================13:54:57\n",
    "Epoch=5,Loss:0.0698405355,Accuracy:0.9768,Valid Loss:0.607856631,Valid Accuracy:0.857\n",
    "\n",
    "================================================================================13:55:15\n",
    "Epoch=6,Loss:0.0366807655,Accuracy:0.98825,Valid Loss:0.745884955,Valid Accuracy:0.854\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model,ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "         valid_step(model,features,labels)\n",
    "    logs = 'Valid Loss:{},Valid Accuracy:{}' \n",
    "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model,ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Valid Loss:0.745884418,Valid Accuracy:0.854\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用以下方法:\n",
    "\n",
    "* model.predict(ds_test)\n",
    "* model(x_test)\n",
    "* model.call(x_test)\n",
    "* model.predict_on_batch(x_test)\n",
    "\n",
    "推荐优先使用model.predict(ds_test)方法，既可以对Dataset，也可以对Tensor使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.7864823 ],\n",
    "       [0.9999901 ],\n",
    "       [0.99944776],\n",
    "       ...,\n",
    "       [0.8498302 ],\n",
    "       [0.13382755],\n",
    "       [1.        ]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_test,_ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    #以下方法等价：\n",
    "    #print(model.call(x_test))\n",
    "    #print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[7.8648227e-01]\n",
    " [9.9999011e-01]\n",
    " [9.9944776e-01]\n",
    " [3.7153201e-09]\n",
    " [9.4462049e-01]\n",
    " [2.3522753e-04]\n",
    " [1.2044354e-04]\n",
    " [9.3752089e-07]\n",
    " [9.9996352e-01]\n",
    " [9.3435925e-01]\n",
    " [9.8746723e-01]\n",
    " [9.9908626e-01]\n",
    " [4.1563155e-08]\n",
    " [4.1808244e-03]\n",
    " [8.0184749e-05]\n",
    " [8.3910513e-01]\n",
    " [3.5167937e-05]\n",
    " [7.2113985e-01]\n",
    " [4.5228912e-03]\n",
    " [9.9942589e-01]], shape=(20, 1), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用TensorFlow原生方式保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[0.7864823 ],\n",
    "       [0.9999901 ],\n",
    "       [0.99944776],\n",
    "       ...,\n",
    "       [0.8498302 ],\n",
    "       [0.13382755],\n",
    "       [1.        ]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4,时间序列数据建模流程范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "国内的新冠肺炎疫情从发现至今已经持续3个多月了，这场起源于吃野味的灾难给大家的生活造成了诸多方面的影响。\n",
    "\n",
    "有的同学是收入上的，有的同学是感情上的，有的同学是心理上的，还有的同学是体重上的。\n",
    "\n",
    "那么国内的新冠肺炎疫情何时结束呢？什么时候我们才可以重获自由呢？\n",
    "\n",
    "本篇文章将利用TensorFlow2.0建立时间序列RNN模型，对国内的新冠肺炎疫情结束时间进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/疫情前后对比.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文的数据集取自tushare，获取该数据集的方法参考了以下文章。\n",
    "\n",
    "《https://zhuanlan.zhihu.com/p/109556102》\n",
    "\n",
    "![](./data/1-4-新增人数.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers,losses,metrics,callbacks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "df = pd.read_csv(\"./data/covid-19.csv\",sep = \"\\t\")\n",
    "df.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-累积曲线.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.set_index(\"date\")\n",
    "dfdiff = dfdata.diff(periods=1).dropna()\n",
    "dfdiff = dfdiff.reset_index(\"date\")\n",
    "\n",
    "dfdiff.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60)\n",
    "dfdiff = dfdiff.drop(\"date\",axis = 1).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-新增曲线.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用某日前8天窗口数据作为输入预测该日数据\n",
    "WINDOW_SIZE = 8\n",
    "\n",
    "def batch_dataset(dataset):\n",
    "    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=True)\n",
    "    return dataset_batched\n",
    "\n",
    "ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \\\n",
    "   .window(WINDOW_SIZE,shift=1).flat_map(batch_dataset)\n",
    "\n",
    "ds_label = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))\n",
    "\n",
    "#数据较小，可以将全部训练数据放入到一个batch中，提升性能\n",
    "ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(38).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "此处选择使用函数式API构建任意结构模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Block, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x_input,x):\n",
    "        x_out = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "        return x_out\n",
    "    \n",
    "    def get_config(self):  \n",
    "        config = super(Block, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "x_input = layers.Input(shape = (None,3),dtype = tf.float32)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x_input)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,input_shape=(None,3))(x)\n",
    "x = layers.Dense(3)(x)\n",
    "\n",
    "#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构\n",
    "#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "x = Block()(x_input,x)\n",
    "model = models.Model(inputs = [x_input],outputs = [x])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, None, 3)]         0         \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, None, 3)           84        \n",
    "_________________________________________________________________\n",
    "lstm_3 (LSTM)                (None, 3)                 84        \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 3)                 12        \n",
    "_________________________________________________________________\n",
    "block (Block)                (None, 3)                 0         \n",
    "=================================================================\n",
    "Total params: 348\n",
    "Trainable params: 348\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们选择最常用也最简单的内置fit方法。\n",
    "\n",
    "注：循环神经网络调试较为困难，需要设置多个不同的学习率多次尝试，以取得较好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义损失函数，考虑平方差和预测目标的比值\n",
    "class MSPE(losses.Loss):\n",
    "    def call(self,y_true,y_pred):\n",
    "        err_percent = (y_true - y_pred)**2/(tf.maximum(y_true**2,1e-7))\n",
    "        mean_err_percent = tf.reduce_mean(err_percent)\n",
    "        return mean_err_percent\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MSPE, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer,loss=MSPE(name = \"MSPE\"))\n",
    "\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "#如果loss在100个epoch后没有提升，学习率减半。\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor = 0.5, patience = 100)\n",
    "#当loss在200个epoch后没有提升，则提前终止训练。\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience= 200)\n",
    "callbacks_list = [tb_callback,lr_callback,stop_callback]\n",
    "\n",
    "history = model.fit(ds_train,epochs=500,callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 371/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.1184\n",
    "Epoch 372/500\n",
    "1/1 [==============================] - 0s 64ms/step - loss: 0.1177\n",
    "Epoch 373/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1169\n",
    "Epoch 374/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.1161\n",
    "Epoch 375/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1154\n",
    "Epoch 376/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1147\n",
    "Epoch 377/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.1140\n",
    "Epoch 378/500\n",
    "1/1 [==============================] - 0s 93ms/step - loss: 0.1133\n",
    "Epoch 379/500\n",
    "1/1 [==============================] - 0s 85ms/step - loss: 0.1126\n",
    "Epoch 380/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.1119\n",
    "Epoch 381/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1113\n",
    "Epoch 382/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.1107\n",
    "Epoch 383/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.1100\n",
    "Epoch 384/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1094\n",
    "Epoch 385/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.1088\n",
    "Epoch 386/500\n",
    "1/1 [==============================] - 0s 74ms/step - loss: 0.1082\n",
    "Epoch 387/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1077\n",
    "Epoch 388/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1071\n",
    "Epoch 389/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1066\n",
    "Epoch 390/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1060\n",
    "Epoch 391/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.1055\n",
    "Epoch 392/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1050\n",
    "Epoch 393/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.1045\n",
    "Epoch 394/500\n",
    "1/1 [==============================] - 0s 65ms/step - loss: 0.1040\n",
    "Epoch 395/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.1035\n",
    "Epoch 396/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.1031\n",
    "Epoch 397/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.1026\n",
    "Epoch 398/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.1022\n",
    "Epoch 399/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.1017\n",
    "Epoch 400/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.1013\n",
    "Epoch 401/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.1009\n",
    "Epoch 402/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.1005\n",
    "Epoch 403/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.1001\n",
    "Epoch 404/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0997\n",
    "Epoch 405/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0993\n",
    "Epoch 406/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0990\n",
    "Epoch 407/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0986\n",
    "Epoch 408/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.0982\n",
    "Epoch 409/500\n",
    "1/1 [==============================] - 0s 67ms/step - loss: 0.0979\n",
    "Epoch 410/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0976\n",
    "Epoch 411/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0972\n",
    "Epoch 412/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0969\n",
    "Epoch 413/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0966\n",
    "Epoch 414/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0963\n",
    "Epoch 415/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0960\n",
    "Epoch 416/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0957\n",
    "Epoch 417/500\n",
    "1/1 [==============================] - 0s 69ms/step - loss: 0.0954\n",
    "Epoch 418/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0951\n",
    "Epoch 419/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0948\n",
    "Epoch 420/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0946\n",
    "Epoch 421/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0943\n",
    "Epoch 422/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0941\n",
    "Epoch 423/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0938\n",
    "Epoch 424/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0936\n",
    "Epoch 425/500\n",
    "1/1 [==============================] - 0s 100ms/step - loss: 0.0933\n",
    "Epoch 426/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0931\n",
    "Epoch 427/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0929\n",
    "Epoch 428/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0926\n",
    "Epoch 429/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0924\n",
    "Epoch 430/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0922\n",
    "Epoch 431/500\n",
    "1/1 [==============================] - 0s 75ms/step - loss: 0.0920\n",
    "Epoch 432/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0918\n",
    "Epoch 433/500\n",
    "1/1 [==============================] - 0s 77ms/step - loss: 0.0916\n",
    "Epoch 434/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0914\n",
    "Epoch 435/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0912\n",
    "Epoch 436/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0911\n",
    "Epoch 437/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0909\n",
    "Epoch 438/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0907\n",
    "Epoch 439/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0905\n",
    "Epoch 440/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0904\n",
    "Epoch 441/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0902\n",
    "Epoch 442/500\n",
    "1/1 [==============================] - 0s 73ms/step - loss: 0.0901\n",
    "Epoch 443/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0899\n",
    "Epoch 444/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0898\n",
    "Epoch 445/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0896\n",
    "Epoch 446/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.0895\n",
    "Epoch 447/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0893\n",
    "Epoch 448/500\n",
    "1/1 [==============================] - 0s 64ms/step - loss: 0.0892\n",
    "Epoch 449/500\n",
    "1/1 [==============================] - 0s 70ms/step - loss: 0.0891\n",
    "Epoch 450/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0889\n",
    "Epoch 451/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0888\n",
    "Epoch 452/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0887\n",
    "Epoch 453/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0886\n",
    "Epoch 454/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0885\n",
    "Epoch 455/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0883\n",
    "Epoch 456/500\n",
    "1/1 [==============================] - 0s 71ms/step - loss: 0.0882\n",
    "Epoch 457/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0881\n",
    "Epoch 458/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0880\n",
    "Epoch 459/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0879\n",
    "Epoch 460/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0878\n",
    "Epoch 461/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0878\n",
    "Epoch 462/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0879\n",
    "Epoch 463/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0879\n",
    "Epoch 464/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0888\n",
    "Epoch 465/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0875\n",
    "Epoch 466/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0873\n",
    "Epoch 467/500\n",
    "1/1 [==============================] - 0s 49ms/step - loss: 0.0872\n",
    "Epoch 468/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0872\n",
    "Epoch 469/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0871\n",
    "Epoch 470/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0871\n",
    "Epoch 471/500\n",
    "1/1 [==============================] - 0s 59ms/step - loss: 0.0870\n",
    "Epoch 472/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0871\n",
    "Epoch 473/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0869\n",
    "Epoch 474/500\n",
    "1/1 [==============================] - 0s 61ms/step - loss: 0.0870\n",
    "Epoch 475/500\n",
    "1/1 [==============================] - 0s 47ms/step - loss: 0.0868\n",
    "Epoch 476/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0868\n",
    "Epoch 477/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0866\n",
    "Epoch 478/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0867\n",
    "Epoch 479/500\n",
    "1/1 [==============================] - 0s 60ms/step - loss: 0.0865\n",
    "Epoch 480/500\n",
    "1/1 [==============================] - 0s 65ms/step - loss: 0.0866\n",
    "Epoch 481/500\n",
    "1/1 [==============================] - 0s 58ms/step - loss: 0.0864\n",
    "Epoch 482/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0865\n",
    "Epoch 483/500\n",
    "1/1 [==============================] - 0s 53ms/step - loss: 0.0863\n",
    "Epoch 484/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0864\n",
    "Epoch 485/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0862\n",
    "Epoch 486/500\n",
    "1/1 [==============================] - 0s 55ms/step - loss: 0.0863\n",
    "Epoch 487/500\n",
    "1/1 [==============================] - 0s 52ms/step - loss: 0.0861\n",
    "Epoch 488/500\n",
    "1/1 [==============================] - 0s 68ms/step - loss: 0.0862\n",
    "Epoch 489/500\n",
    "1/1 [==============================] - 0s 62ms/step - loss: 0.0860\n",
    "Epoch 490/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0861\n",
    "Epoch 491/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0859\n",
    "Epoch 492/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0860\n",
    "Epoch 493/500\n",
    "1/1 [==============================] - 0s 51ms/step - loss: 0.0859\n",
    "Epoch 494/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0860\n",
    "Epoch 495/500\n",
    "1/1 [==============================] - 0s 50ms/step - loss: 0.0858\n",
    "Epoch 496/500\n",
    "1/1 [==============================] - 0s 69ms/step - loss: 0.0859\n",
    "Epoch 497/500\n",
    "1/1 [==============================] - 0s 63ms/step - loss: 0.0857\n",
    "Epoch 498/500\n",
    "1/1 [==============================] - 0s 56ms/step - loss: 0.0858\n",
    "Epoch 499/500\n",
    "1/1 [==============================] - 0s 54ms/step - loss: 0.0857\n",
    "Epoch 500/500\n",
    "1/1 [==============================] - 0s 57ms/step - loss: 0.0858\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估模型一般要设置验证集或者测试集，由于此例数据较少，我们仅仅可视化损失函数在训练集上的迭代情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.title('Training '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-损失函数曲线.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们使用模型预测疫情结束时间，即 新增确诊病例为0 的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用dfresult记录现有数据以及此后预测的疫情数据\n",
    "dfresult = dfdiff[[\"confirmed_num\",\"cured_num\",\"dead_num\"]].copy()\n",
    "dfresult.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-日期3月10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测此后100天的新增走势,将其结果添加到dfresult中\n",
    "for i in range(100):\n",
    "    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[-38:,:],axis = 0)))\n",
    "\n",
    "    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),\n",
    "                columns = dfresult.columns)\n",
    "    dfresult = dfresult.append(dfpredict,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"confirmed_num==0\").head()\n",
    "\n",
    "# 第55天开始新增确诊降为0，第45天对应3月10日，也就是10天后，即预计3月20日新增确诊降为0\n",
    "# 注：该预测偏乐观"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-预测确诊.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"cured_num==0\").head()\n",
    "\n",
    "# 第164天开始新增治愈降为0，第45天对应3月10日，也就是大概4个月后，即7月10日左右全部治愈。\n",
    "# 注: 该预测偏悲观，并且存在问题，如果将每天新增治愈人数加起来，将超过累计确诊人数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-预测治愈.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.query(\"dead_num==0\").head()\n",
    "\n",
    "# 第60天开始，新增死亡降为0，第45天对应3月10日，也就是大概15天后，即20200325\n",
    "# 该预测较为合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/1-4-预测死亡.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用TensorFlow原生方式保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel',compile=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_loaded.compile(optimizer=optimizer,loss=MSPE(name = \"MSPE\"))\n",
    "model_loaded.predict(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、TensorFlow的核心概念\n",
    "\n",
    "TensorFlow™ 是一个采用 **数据流图**（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以**在多种平台上展开计算**，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，**用于机器学习和深度神经网络**方面的研究，但这个系统的通用性使其也可**广泛用于其他计算领域**。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow的主要优点：\n",
    "\n",
    "* 灵活性：支持底层数值计算，C++自定义操作符\n",
    "\n",
    "* 可移植性：从服务器到PC到手机，从CPU到GPU到TPU\n",
    "\n",
    "* 分布式计算：分布式并行计算，可指定操作符对应计算设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "俗话说，万丈高楼平地起，TensorFlow这座大厦也有它的地基。\n",
    "\n",
    "Tensorflow底层最核心的概念是张量，计算图以及自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1,张量数据结构\n",
    "\n",
    "程序 = 数据结构+算法。\n",
    "\n",
    "TensorFlow程序 = 张量数据结构 + 计算图算法语言\n",
    "\n",
    "张量和计算图是 TensorFlow的核心概念。\n",
    "\n",
    "Tensorflow的基本数据结构是张量Tensor。张量即多维数组。Tensorflow的张量和numpy中的array很类似。\n",
    "\n",
    "从行为特性来看，有两种类型的张量，常量constant和变量Variable.\n",
    "\n",
    "常量的值在计算图中不可以被重新赋值，变量可以在计算图中用assign等算子重新赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，常量张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的数据类型和numpy.array基本一一对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "i = tf.constant(1) # tf.int32 类型常量\n",
    "l = tf.constant(1,dtype = tf.int64) # tf.int64 类型常量\n",
    "f = tf.constant(1.23) #tf.float32 类型常量\n",
    "d = tf.constant(3.14,dtype = tf.double) # tf.double 类型常量\n",
    "s = tf.constant(\"hello world\") # tf.string类型常量\n",
    "b = tf.constant(True) #tf.bool类型常量\n",
    "\n",
    "\n",
    "print(tf.int64 == np.int64) \n",
    "print(tf.bool == np.bool)\n",
    "print(tf.double == np.float64)\n",
    "print(tf.string == np.unicode) # tf.string类型和np.unicode类型不等价\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "True\n",
    "True\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同类型的数据可以用不同维度(rank)的张量来表示。\n",
    "\n",
    "标量为0维张量，向量为1维张量，矩阵为2维张量。\n",
    "\n",
    "彩色图像有rgb三个通道，可以表示为3维张量。\n",
    "\n",
    "视频还有时间维，可以表示为4维张量。\n",
    "\n",
    "可以简单地总结为：有几层中括号，就是多少维的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = tf.constant(True)  #标量，0维张量\n",
    "\n",
    "print(tf.rank(scalar))\n",
    "print(scalar.numpy().ndim)  # tf.rank的作用和numpy的ndim方法相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([1.0,2.0,3.0,4.0]) #向量，1维张量\n",
    "\n",
    "print(tf.rank(vector))\n",
    "print(np.ndim(vector.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.constant([[1.0,2.0],[3.0,4.0]]) #矩阵, 2维张量\n",
    "\n",
    "print(tf.rank(matrix).numpy())\n",
    "print(np.ndim(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor3 = tf.constant([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])  # 3维张量\n",
    "print(tensor3)\n",
    "print(tf.rank(tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[[1. 2.]\n",
    "  [3. 4.]]\n",
    "\n",
    " [[5. 6.]\n",
    "  [7. 8.]]], shape=(2, 2, 2), dtype=float32)\n",
    "tf.Tensor(3, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor4 = tf.constant([[[[1.0,1.0],[2.0,2.0]],[[3.0,3.0],[4.0,4.0]]],\n",
    "                        [[[5.0,5.0],[6.0,6.0]],[[7.0,7.0],[8.0,8.0]]]])  # 4维张量\n",
    "print(tensor4)\n",
    "print(tf.rank(tensor4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[[[1. 1.]\n",
    "   [2. 2.]]\n",
    "\n",
    "  [[3. 3.]\n",
    "   [4. 4.]]]\n",
    "\n",
    "\n",
    " [[[5. 5.]\n",
    "   [6. 6.]]\n",
    "\n",
    "  [[7. 7.]\n",
    "   [8. 8.]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用tf.cast改变张量的数据类型。\n",
    "\n",
    "可以用numpy方法将tensorflow中的张量转化成numpy中的张量。\n",
    "\n",
    "可以用shape方法查看张量的尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.constant([123,456],dtype = tf.int32)\n",
    "f = tf.cast(h,tf.float32)\n",
    "print(h.dtype, f.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<dtype: 'int32'> <dtype: 'float32'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "print(y.numpy()) #转换成np.array\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1. 2.]\n",
    " [3. 4.]]\n",
    "(2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = tf.constant(u\"你好 世界\")\n",
    "print(u.numpy())  \n",
    "print(u.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd \\xe4\\xb8\\x96\\xe7\\x95\\x8c'\n",
    "你好 世界\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，变量张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型中需要被训练的参数一般被设置成变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常量值不可以改变，常量的重新赋值相当于创造新的内存空间\n",
    "c = tf.constant([1.0,2.0])\n",
    "print(c)\n",
    "print(id(c))\n",
    "c = c + tf.constant([1.0,1.0])\n",
    "print(c)\n",
    "print(id(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
    "5276289568\n",
    "tf.Tensor([2. 3.], shape=(2,), dtype=float32)\n",
    "5276290240\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值\n",
    "v = tf.Variable([1.0,2.0],name = \"v\")\n",
    "print(v)\n",
    "print(id(v))\n",
    "v.assign_add([1.0,1.0])\n",
    "print(v)\n",
    "print(id(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
    "5276259888\n",
    "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n",
    "5276259888\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2,三种计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph.\n",
    "\n",
    "在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。\n",
    "\n",
    "而在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。\n",
    "\n",
    "使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。\n",
    "\n",
    "使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。\n",
    "\n",
    "如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做 Autograph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，计算图简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图由节点(nodes)和线(edges)组成。\n",
    "\n",
    "节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。\n",
    "\n",
    "实线表示有数据传递依赖，传递的数据即张量。\n",
    "\n",
    "虚线通常可以表示控制依赖，即执行先后顺序。\n",
    "\n",
    "![](./data/strjoin_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，静态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 1.0静态计算图范例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#定义计算图\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #placeholder为占位符，执行会话时候指定填充对象\n",
    "    x = tf.placeholder(name='x', shape=[], dtype=tf.string)  \n",
    "    y = tf.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.string_join([x,y],name = 'join',separator=' ')\n",
    "\n",
    "#执行计算图\n",
    "with tf.Session(graph = g) as sess:\n",
    "    print(sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"}))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow2.0 怀旧版静态计算图**\n",
    "\n",
    "TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。\n",
    "\n",
    "可称之为怀旧版静态计算图，已经不推荐使用了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.compat.v1.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)\n",
    "    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.strings.join([x,y],name = \"join\",separator = \" \")\n",
    "\n",
    "with tf.compat.v1.Session(graph = g) as sess:\n",
    "    # fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。\n",
    "    result = sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "b'hello world'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow2.0中，使用的是动态计算图和Autograph.\n",
    "\n",
    "在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。\n",
    "\n",
    "动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为 Eager Excution.\n",
    "\n",
    "Eager这个英文单词的原意是\"迫不及待的\"，也就是立即执行的意思。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态计算图在每个算子处都进行构建，构建后立即执行\n",
    "\n",
    "x = tf.constant(\"hello\")\n",
    "y = tf.constant(\"world\")\n",
    "z = tf.strings.join([x,y],separator=\" \")\n",
    "\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以将动态计算图代码的输入和输出关系封装成函数\n",
    "\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，TensorFlow2.0的Autograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态计算图运行效率相对较低。\n",
    "\n",
    "可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。\n",
    "\n",
    "在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。\n",
    "\n",
    "在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。\n",
    "\n",
    "不需要使用会话了，一些都像原始的Python语法一样自然。\n",
    "\n",
    "实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。\n",
    "\n",
    "当然，@tf.function的使用需要遵循一定的规范，我们后面章节将重点介绍。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 使用autograph构建静态图\n",
    "\n",
    "@tf.function\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hello world\n",
    "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 创建日志\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/autograph/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "#开启autograph跟踪\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "#执行autograph\n",
    "result = strjoin(\"hello\",\"world\")\n",
    "\n",
    "#将计算图信息写入日志\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"autograph\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动 tensorboard在jupyter中的魔法命令\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动tensorboard\n",
    "%tensorboard --logdir ./data/autograph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/2-2-tensorboard计算图.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3,自动微分机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。\n",
    "\n",
    "而深度学习框架可以帮助我们自动地完成这种求梯度运算。\n",
    "\n",
    "Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。\n",
    "\n",
    "这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，利用梯度磁带求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx = tape.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(-2.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对常量张量也可以求导，需要增加watch\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a,b,c])\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])\n",
    "print(dy_da)\n",
    "print(dy_dc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0.0, shape=(), dtype=float32)\n",
    "tf.Tensor(1.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以求二阶导数\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:   \n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape1.gradient(y,x)   \n",
    "dy2_dx2 = tape2.gradient(dy_dx,x)\n",
    "\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(2.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以在autograph中使用\n",
    "\n",
    "@tf.function\n",
    "def f(x):   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    # 自变量转换成tf.float32\n",
    "    x = tf.cast(x,tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = a*tf.pow(x,2)+b*x+c\n",
    "    dy_dx = tape.gradient(y,x) \n",
    "    \n",
    "    return((dy_dx,y))\n",
    "\n",
    "tf.print(f(tf.constant(0.0)))\n",
    "tf.print(f(tf.constant(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(-2, 1)\n",
    "(0, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，利用梯度磁带和优化器求最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape.gradient(y,x)\n",
    "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "    \n",
    "tf.print(\"y =\",y,\"; x =\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = 0 ; x = 0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用optimizer.minimize\n",
    "# optimizer.minimize相当于先用tape求gradient,再apply_gradient\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "\n",
    "#注意f()无参数\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "for _ in range(1000):\n",
    "    optimizer.minimize(f,[x])   \n",
    "    \n",
    "tf.print(\"y =\",f(),\"; x =\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = 0 ; x = 0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    for _ in tf.range(1000): #注意autograph时使用tf.range(1000)而不是range(1000)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(minimizef())\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "# 使用optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "@tf.function\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    return(f())\n",
    "\n",
    "\n",
    "tf.print(train(1000))\n",
    "tf.print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "0.999998569\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、TensorFlow的层次结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章我们介绍TensorFlow中5个不同的层次结构：即硬件层，内核层，低阶API，中阶API，高阶API。并以线性回归为例，直观对比展示在不同层级实现模型的特点。\n",
    "\n",
    "TensorFlow的层次结构从低到高可以分成如下五层。\n",
    "\n",
    "最底层为硬件层，TensorFlow支持CPU、GPU或TPU加入计算资源池。\n",
    "\n",
    "第二层为C++实现的内核，kernel可以跨平台分布运行。\n",
    "\n",
    "第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、计算图、自动微分.\n",
    "如tf.Variable,tf.constant,tf.function,tf.GradientTape,tf.nn.softmax...\n",
    "如果把模型比作一个房子，那么第三层API就是【模型之砖】。\n",
    "\n",
    "第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。\n",
    "如tf.keras.layers,tf.keras.losses,tf.keras.metrics,tf.keras.optimizers,tf.data.DataSet,tf.feature_column...\n",
    "如果把模型比作一个房子，那么第四层API就是【模型之墙】。\n",
    "\n",
    "第五层为Python实现的模型成品，一般为按照OOP方式封装的高级API，主要为tf.keras.models提供的模型的类接口。\n",
    "如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/tensorflow_structure.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1,低阶API示范\n",
    "\n",
    "下面的范例使用TensorFlow的低阶API实现线性回归模型。\n",
    "\n",
    "低阶API主要包括张量操作，计算图和自动微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本数量\n",
    "n = 400\n",
    "\n",
    "# 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用动态图调试\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #正向传播求损失\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # 反向传播求梯度\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # 梯度下降法更新参数\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "            \n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-1-输出01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##使用autograph机制转换成静态图加速\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "@tf.function\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #正向传播求损失\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # 反向传播求梯度\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # 梯度下降法更新参数\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-1-输出02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-2,中阶API示范\n",
    "\n",
    "下面的范例使用TensorFlow的中阶API实现线性回归模型。\n",
    "\n",
    "TensorFlow的中阶API主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,losses,metrics,optimizers\n",
    "\n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本数量\n",
    "n = 800\n",
    "\n",
    "# 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动\n",
    "\n",
    "#构建输入数据管道\n",
    "ds = tf.data.Dataset.from_tensor_slices((X,Y)) \\\n",
    "     .shuffle(buffer_size = 1000).batch(100) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "\n",
    "#定义优化器\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = layers.Dense(units = 1)\n",
    "linear.build(input_shape = (2,)) \n",
    "\n",
    "@tf.function\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        L = tf.constant(0.0) #使用L记录loss值\n",
    "        for X_batch,Y_batch in ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                Y_hat = linear(X_batch)\n",
    "                loss = losses.mean_squared_error(tf.reshape(Y_hat,[-1]),tf.reshape(Y_batch,[-1]))\n",
    "            grads = tape.gradient(loss,linear.variables)\n",
    "            optimizer.apply_gradients(zip(grads,linear.variables))\n",
    "            L = loss\n",
    "        \n",
    "        if(epoch%100==0):\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\"loss =\",L)\n",
    "            tf.print(\"w =\",linear.kernel)\n",
    "            tf.print(\"b =\",linear.bias)\n",
    "            tf.print(\"\")\n",
    "\n",
    "train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-2-输出01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-3,高阶API示范\n",
    "\n",
    "下面的范例使用TensorFlow的高阶API实现线性回归模型。\n",
    "\n",
    "TensorFlow的高阶API主要为tf.keras.models提供的模型的类接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "此处分别演示使用Sequential按层顺序构建模型以及继承Model基类构建自定义模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，使用Sequential按层顺序构建模型【面向新手】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "#样本数量\n",
    "n = 800\n",
    "\n",
    "# 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "linear = models.Sequential()\n",
    "linear.add(layers.Dense(1,input_shape =(2,)))\n",
    "linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-序列结构.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用fit方法进行训练\n",
    "\n",
    "linear.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 20,epochs = 200)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[0].kernel)\n",
    "tf.print(\"b = \",linear.layers[0].bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-内置训练.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，继承Model基类构建自定义模型【面向专家】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers,losses,metrics\n",
    "\n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本数量\n",
    "n = 800\n",
    "\n",
    "# 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X[0:n*3//4,:],Y[0:n*3//4,:])) \\\n",
    "     .shuffle(buffer_size = 1000).batch(20) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
    "     .cache()\n",
    "\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X[n*3//4:,:],Y[n*3//4:,:])) \\\n",
    "     .shuffle(buffer_size = 1000).batch(20) \\\n",
    "     .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
    "     .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class MyModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.dense1 = layers.Dense(1)   \n",
    "        super(MyModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        y = self.dense1(x)\n",
    "        return(y)\n",
    "\n",
    "model = MyModel()\n",
    "model.build(input_shape =(None,2))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-模型结构.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 自定义训练循环(专家教程)\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam()\n",
    "loss_func = losses.MeanSquaredError()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_metric = tf.keras.metrics.MeanAbsoluteError(name='train_mae')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_metric = tf.keras.metrics.MeanAbsoluteError(name='valid_mae')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},MAE:{},Valid Loss:{},Valid MAE:{}'\n",
    "        \n",
    "        if  epoch%100 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"w=\",model.layers[0].kernel)\n",
    "            tf.print(\"b=\",model.layers[0].bias)\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_valid,400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/3-3-自定义训练.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、TensorFlow的低阶API\n",
    "\n",
    "TensorFlow的低阶API主要包括张量操作，计算图和自动微分。\n",
    "\n",
    "如果把模型比作一个房子，那么低阶API就是【模型之砖】。\n",
    "\n",
    "在低阶API层次上，可以把TensorFlow当做一个增强版的numpy来使用。\n",
    "\n",
    "TensorFlow提供的方法比numpy更全面，运算速度更快，如果需要的话，还可以使用GPU进行加速。\n",
    "\n",
    "前面几章我们对低阶API已经有了一个整体的认识，本章我们将重点详细介绍张量操作和Autograph计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的操作主要包括张量的结构操作和张量的数学运算。\n",
    "\n",
    "张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。\n",
    "\n",
    "张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "Autograph计算图我们将介绍使用Autograph的规范建议，Autograph的机制原理，Autograph和tf.Module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-1,张量的结构操作\n",
    "\n",
    "张量的操作主要包括张量的结构操作和张量的数学运算。\n",
    "\n",
    "张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。\n",
    "\n",
    "张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "本篇我们介绍张量的结构操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，创建张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量创建的许多方法和numpy中创建array的方法很像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3],dtype = tf.float32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.range(1,10,delta = 2)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 3 5 7 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.linspace(0.0,2*3.14,100)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.zeros([3,3])\n",
    "tf.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[0 0 0]\n",
    " [0 0 0]\n",
    " [0 0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones([3,3])\n",
    "b = tf.zeros_like(a,dtype= tf.float32)\n",
    "tf.print(a)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 1 1]\n",
    " [1 1 1]\n",
    " [1 1 1]]\n",
    "[[0 0 0]\n",
    " [0 0 0]\n",
    " [0 0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.fill([3,2],5)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[5 5]\n",
    " [5 5]\n",
    " [5 5]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#均匀分布随机\n",
    "tf.random.set_seed(1.0)\n",
    "a = tf.random.uniform([5],minval=0,maxval=10)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正态分布随机\n",
    "b = tf.random.normal([3,3],mean=0.0,stddev=1.0)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[0.403087884 -1.0880208 -0.0630953535]\n",
    " [1.33655667 0.711760104 -0.489286453]\n",
    " [-0.764221311 -1.03724861 -1.25193381]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正态分布随机，剔除2倍方差以外数据重新生成\n",
    "c = tf.random.truncated_normal((5,5), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-0.457012236 -0.406867266 0.728577733 -0.892977774 -0.369404584]\n",
    " [0.323488563 1.19383323 0.888299048 1.25985599 -1.95951891]\n",
    " [-0.202244401 0.294496894 -0.468728036 1.29494202 1.48142183]\n",
    " [0.0810953453 1.63843894 0.556645 0.977199793 -1.17777884]\n",
    " [1.67368948 0.0647980496 -0.705142677 -0.281972528 0.126546144]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊矩阵\n",
    "I = tf.eye(3,3) #单位矩阵\n",
    "tf.print(I)\n",
    "tf.print(\" \")\n",
    "t = tf.linalg.diag([1,2,3]) #对角阵\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 0 0]\n",
    " [0 1 0]\n",
    " [0 0 1]]\n",
    " \n",
    "[[1 0 0]\n",
    " [0 2 0]\n",
    " [0 0 3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二 ，索引切片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。\n",
    "\n",
    "对于tf.Variable,可以通过索引和切片对部分元素进行修改。\n",
    "\n",
    "对于提取张量的连续子区域，也可以使用tf.slice.\n",
    "\n",
    "此外，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。\n",
    "\n",
    "tf.boolean_mask功能最为强大，它可以实现tf.gather,tf.gather_nd的功能，并且tf.boolean_mask还可以实现布尔索引。\n",
    "\n",
    "如果要通过修改张量的某些元素得到新的张量，可以使用tf.where，tf.scatter_nd。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(3)\n",
    "t = tf.random.uniform([5,5],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[4 7 4 2 9]\n",
    " [9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]\n",
    " [3 7 0 0 3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第0行\n",
    "tf.print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[4 7 4 2 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#倒数第一行\n",
    "tf.print(t[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[3 7 0 0 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第1行第3列\n",
    "tf.print(t[1,3])\n",
    "tf.print(t[1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "4\n",
    "4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第1行至第3行\n",
    "tf.print(t[1:4,:])\n",
    "tf.print(tf.slice(t,[1,0],[3,5])) #tf.slice(input,begin_vector,size_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]]\n",
    "[[9 1 2 4 7]\n",
    " [7 2 7 4 0]\n",
    " [9 6 9 7 2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第1行至最后一行，第0列到最后一列每隔两列取一列\n",
    "tf.print(t[1:4,:4:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[9 2]\n",
    " [7 7]\n",
    " [9 9]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对变量来说，还可以使用索引和切片修改部分元素\n",
    "x = tf.Variable([[1,2],[3,4]],dtype = tf.float32)\n",
    "x[1,:].assign(tf.constant([0.0,0.0]))\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[1 2]\n",
    " [0 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([3,3,3],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[7 3 9]\n",
    "  [9 0 7]\n",
    "  [9 6 7]]\n",
    "\n",
    " [[1 3 3]\n",
    "  [0 8 1]\n",
    "  [3 1 0]]\n",
    "\n",
    " [[4 0 6]\n",
    "  [6 2 2]\n",
    "  [7 9 5]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#省略号可以表示多个冒号\n",
    "tf.print(a[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[3 0 6]\n",
    " [3 8 1]\n",
    " [0 2 9]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上切片方式相对规则，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。\n",
    "\n",
    "考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4*10*7的张量来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.random.uniform((4,10,7),minval=0,maxval=100,dtype=tf.int32)\n",
    "tf.print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [8 36 94 ... 13 78 41]\n",
    "  [77 53 51 ... 22 91 56]\n",
    "  ...\n",
    "  [11 19 26 ... 89 86 68]\n",
    "  [60 72 0 ... 11 26 15]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [83 36 31 ... 75 38 85]\n",
    "  [54 26 67 ... 60 68 98]\n",
    "  ...\n",
    "  [20 5 18 ... 32 45 3]\n",
    "  [72 52 81 ... 88 41 20]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [78 71 54 ... 43 98 81]\n",
    "  [21 66 53 ... 97 75 77]\n",
    "  ...\n",
    "  [6 74 3 ... 53 65 43]\n",
    "  [98 36 72 ... 33 36 81]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [35 8 82 ... 11 59 97]\n",
    "  [44 6 99 ... 81 60 27]\n",
    "  ...\n",
    "  [76 26 35 ... 51 8 17]\n",
    "  [33 52 53 ... 78 37 31]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.gather(scores,[0,5,9],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [24 80 70 ... 72 63 96]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [46 10 94 ... 23 18 92]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [19 12 23 ... 87 86 25]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [6 41 79 ... 97 43 13]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩\n",
    "q = tf.gather(tf.gather(scores,[0,5,9],axis=1),[1,3,6],axis=2)\n",
    "tf.print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[82 55 14]\n",
    "  [80 46 96]\n",
    "  [99 58 74]]\n",
    "\n",
    " [[73 48 81]\n",
    "  [10 38 92]\n",
    "  [21 86 90]]\n",
    "\n",
    " [[80 57 60]\n",
    "  [12 34 25]\n",
    "  [78 71 21]]\n",
    "\n",
    " [[57 75 3]\n",
    "  [41 47 13]\n",
    "  [27 96 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "#indices的长度为采样样本的个数，每个元素为采样位置的坐标\n",
    "s = tf.gather_nd(scores,indices = [(0,0),(2,4),(3,6)])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
    "array([[52, 82, 66, 55, 17, 86, 14],\n",
    "       [99, 94, 46, 70,  1, 63, 41],\n",
    "       [46, 83, 70, 80, 90, 85, 17]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上tf.gather和tf.gather_nd的功能也可以用tf.boolean_mask来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.boolean_mask(scores,[True,False,False,False,False,\n",
    "                            True,False,False,False,True],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[52 82 66 ... 17 86 14]\n",
    "  [24 80 70 ... 72 63 96]\n",
    "  [24 99 38 ... 97 44 74]]\n",
    "\n",
    " [[79 73 73 ... 35 3 81]\n",
    "  [46 10 94 ... 23 18 92]\n",
    "  [0 21 89 ... 53 10 90]]\n",
    "\n",
    " [[52 80 22 ... 29 25 60]\n",
    "  [19 12 23 ... 87 86 25]\n",
    "  [61 78 70 ... 7 59 21]]\n",
    "\n",
    " [[56 57 45 ... 23 15 3]\n",
    "  [6 41 79 ... 97 43 13]\n",
    "  [71 27 44 ... 0 52 16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "s = tf.boolean_mask(scores,\n",
    "    [[True,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,True,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,True,False,False,False]])\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[52 82 66 ... 17 86 14]\n",
    " [99 94 46 ... 1 63 41]\n",
    " [46 83 70 ... 90 85 17]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用tf.boolean_mask可以实现布尔索引\n",
    "\n",
    "#找到矩阵中小于0的元素\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "tf.print(c,\"\\n\")\n",
    "\n",
    "tf.print(tf.boolean_mask(c,c<0),\"\\n\") \n",
    "tf.print(c[c<0]) #布尔索引，为boolean_mask的语法糖形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-1 1 -1]\n",
    " [2 2 -2]\n",
    " [3 -3 3]] \n",
    "\n",
    "[-1 -1 -2 -3] \n",
    "\n",
    "[-1 -1 -2 -3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。\n",
    "\n",
    "如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。\n",
    "\n",
    "tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。\n",
    "\n",
    "tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，\n",
    "\n",
    "而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到张量中小于0的元素,将其换成np.nan得到新的张量\n",
    "#tf.where和np.where作用类似，可以理解为if的张量版本\n",
    "\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "d = tf.where(c<0,tf.fill(c.shape,np.nan),c) \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[nan,  1., nan],\n",
    "       [ 2.,  2., nan],\n",
    "       [ 3., nan,  3.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果where只有一个参数，将返回所有满足条件的位置坐标\n",
    "indices = tf.where(c<0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
    "array([[0, 0],\n",
    "       [0, 2],\n",
    "       [1, 2],\n",
    "       [2, 1]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量\n",
    "d = c - tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[ 0.,  1., -1.],\n",
    "       [ 2.,  2., -2.],\n",
    "       [ 3.,  0.,  3.]], dtype=float32)>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter_nd的作用和gather_nd有些相反\n",
    "#可以将某些值插入到一个给定shape的全0的张量的指定位置处。\n",
    "indices = tf.where(c<0)\n",
    "tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[-1.,  0., -1.],\n",
    "       [ 0.,  0., -2.],\n",
    "       [ 0., -3.,  0.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，维度变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维度变换相关函数主要有 tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.\n",
    "\n",
    "tf.reshape 可以改变张量的形状。\n",
    "\n",
    "tf.squeeze 可以减少维度。\n",
    "\n",
    "tf.expand_dims 可以增加维度。\n",
    "\n",
    "tf.transpose 可以交换维度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reshape可以改变张量的形状，但是其本质上不会改变张量元素的存储顺序，所以，该操作实际上非常迅速，并且是可逆的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=[1,3,3,2],\n",
    "                      minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([1, 3, 3, 2])\n",
    "[[[[135 178]\n",
    "   [26 116]\n",
    "   [29 224]]\n",
    "\n",
    "  [[179 219]\n",
    "   [153 209]\n",
    "   [111 215]]\n",
    "\n",
    "  [[39 7]\n",
    "   [138 129]\n",
    "   [59 205]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改成 （3,6）形状的张量\n",
    "b = tf.reshape(a,[3,6])\n",
    "tf.print(b.shape)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 6])\n",
    "[[135 178 26 116 29 224]\n",
    " [179 219 153 209 111 215]\n",
    " [39 7 138 129 59 205]]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改回成 [1,3,3,2] 形状的张量\n",
    "c = tf.reshape(b,[1,3,3,2])\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[[[135 178]\n",
    "   [26 116]\n",
    "   [29 224]]\n",
    "\n",
    "  [[179 219]\n",
    "   [153 209]\n",
    "   [111 215]]\n",
    "\n",
    "  [[39 7]\n",
    "   [138 129]\n",
    "   [59 205]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果张量在某个维度上只有一个元素，利用tf.squeeze可以消除这个维度。\n",
    "\n",
    "和tf.reshape相似，它本质上不会改变张量元素的存储顺序。\n",
    "\n",
    "张量的各个元素在内存中是线性存储的，其一般规律是，同一层级中的相邻元素的物理地址也相邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.squeeze(a)\n",
    "tf.print(s.shape)\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 3, 2])\n",
    "[[[135 178]\n",
    "  [26 116]\n",
    "  [29 224]]\n",
    "\n",
    " [[179 219]\n",
    "  [153 209]\n",
    "  [111 215]]\n",
    "\n",
    " [[39 7]\n",
    "  [138 129]\n",
    "  [59 205]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.expand_dims(s,axis=0) #在第0维插入长度为1的一个维度\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(1, 3, 3, 2), dtype=int32, numpy=\n",
    "array([[[[135, 178],\n",
    "         [ 26, 116],\n",
    "         [ 29, 224]],\n",
    "\n",
    "        [[179, 219],\n",
    "         [153, 209],\n",
    "         [111, 215]],\n",
    "\n",
    "        [[ 39,   7],\n",
    "         [138, 129],\n",
    "         [ 59, 205]]]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.transpose可以交换张量的维度，与tf.reshape不同，它会改变张量元素的存储顺序。\n",
    "\n",
    "tf.transpose常用于图片存储格式的变换上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch,Height,Width,Channel\n",
    "a = tf.random.uniform(shape=[100,600,600,4],minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "\n",
    "# 转换成 Channel,Height,Width,Batch\n",
    "s= tf.transpose(a,perm=[3,1,2,0])\n",
    "tf.print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([100, 600, 600, 4])\n",
    "TensorShape([4, 600, 600, 100])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，合并分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和numpy类似，可以用tf.concat和tf.stack方法对多个张量进行合并，可以用tf.split方法把一个张量分割成多个张量。\n",
    "\n",
    "tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
    "array([[ 1.,  2.],\n",
    "       [ 3.,  4.],\n",
    "       [ 5.,  6.],\n",
    "       [ 7.,  8.],\n",
    "       [ 9., 10.],\n",
    "       [11., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat([a,b,c],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
    "array([[ 1.,  2.,  5.,  6.,  9., 10.],\n",
    "       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
    "array([[[ 1.,  2.],\n",
    "        [ 3.,  4.]],\n",
    "\n",
    "       [[ 5.,  6.],\n",
    "        [ 7.,  8.]],\n",
    "\n",
    "       [[ 9., 10.],\n",
    "        [11., 12.]]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([a,b,c],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
    "array([[[ 1.,  2.],\n",
    "        [ 5.,  6.],\n",
    "        [ 9., 10.]],\n",
    "\n",
    "       [[ 3.,  4.],\n",
    "        [ 7.,  8.],\n",
    "        [11., 12.]]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "c = tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.split(value,num_or_size_splits,axis)\n",
    "tf.split(c,3,axis = 0)  #指定分割份数，平均分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[1., 2.],\n",
    "        [3., 4.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[5., 6.],\n",
    "        [7., 8.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[ 9., 10.],\n",
    "        [11., 12.]], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.split(c,[2,2,2],axis = 0) #指定每份的记录数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[1., 2.],\n",
    "        [3., 4.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[5., 6.],\n",
    "        [7., 8.]], dtype=float32)>,\n",
    " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    " array([[ 9., 10.],\n",
    "        [11., 12.]], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-2,张量的数学运算\n",
    "\n",
    "张量的操作主要包括张量的结构操作和张量的数学运算。\n",
    "\n",
    "张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。\n",
    "\n",
    "张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "本篇我们介绍张量的数学运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，标量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。\n",
    "\n",
    "加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。\n",
    "\n",
    "标量运算符的特点是对张量实施逐元素运算。\n",
    "\n",
    "有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。\n",
    "\n",
    "许多标量运算符都在 tf.math模块下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2],[-3,4.0]])\n",
    "b = tf.constant([[5.0,6],[7.0,8.0]])\n",
    "a+b  #运算符重载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 6.,  8.],\n",
    "       [ 4., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a-b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ -4.,  -4.],\n",
    "       [-10.,  -4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[  5.,  12.],\n",
    "       [-21.,  32.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.2       ,  0.33333334],\n",
    "       [-0.42857143,  0.5       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 1.,  4.],\n",
    "       [ 9., 16.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a%3 #mod的运算符重载，等价于m = tf.math.mod(a,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 0], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a//3  #地板除法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.,  0.],\n",
    "       [-1.,  1.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)&(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False, False]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)|(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[ True,  True],\n",
    "       [ True,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==5 #tf.equal(a,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False, False, False])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1.0,8.0])\n",
    "b = tf.constant([5.0,6.0])\n",
    "c = tf.constant([6.0,7.0])\n",
    "tf.add_n([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.maximum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[5 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.minimum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，向量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。\n",
    "许多向量运算符都以reduce开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量reduce\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.reduce_sum(a))\n",
    "tf.print(tf.reduce_mean(a))\n",
    "tf.print(tf.reduce_max(a))\n",
    "tf.print(tf.reduce_min(a))\n",
    "tf.print(tf.reduce_prod(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "45\n",
    "5\n",
    "9\n",
    "1\n",
    "362880\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#张量指定维度进行reduce\n",
    "b = tf.reshape(a,(3,3))\n",
    "tf.print(tf.reduce_sum(b, axis=1, keepdims=True))\n",
    "tf.print(tf.reduce_sum(b, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[6]\n",
    " [15]\n",
    " [24]]\n",
    "[[12 15 18]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool类型的reduce\n",
    "p = tf.constant([True,False,False])\n",
    "q = tf.constant([False,False,True])\n",
    "tf.print(tf.reduce_all(p))\n",
    "tf.print(tf.reduce_any(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用tf.foldr实现tf.reduce_sum\n",
    "s = tf.foldr(lambda a,b:a+b,tf.range(10)) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "45\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cum扫描累积\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.math.cumsum(a))\n",
    "tf.print(tf.math.cumprod(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 3 6 ... 28 36 45]\n",
    "[1 2 6 ... 5040 40320 362880]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg最大最小值索引\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.argmax(a))\n",
    "tf.print(tf.argmin(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "8\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.math.top_k可以用于对张量排序\n",
    "a = tf.constant([1,3,7,5,4,8])\n",
    "\n",
    "values,indices = tf.math.top_k(a,3,sorted=True)\n",
    "tf.print(values)\n",
    "tf.print(indices)\n",
    "\n",
    "#利用tf.math.top_k可以在TensorFlow中实现KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[8 7 5]\n",
    "[5 2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，矩阵运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。\n",
    "\n",
    "矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。\n",
    "\n",
    "除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵乘法\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[2,0],[0,2]])\n",
    "a@b  #等价于tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[2, 4],\n",
    "       [6, 8]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵转置\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.transpose(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1., 3.],\n",
    "       [2., 4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵逆，必须为tf.float32或tf.double类型\n",
    "a = tf.constant([[1.0,2],[3.0,4]],dtype = tf.float32)\n",
    "tf.linalg.inv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[-2.0000002 ,  1.0000001 ],\n",
    "       [ 1.5000001 , -0.50000006]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵求trace\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.trace(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵求范数\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=5.477226>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵行列式\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.det(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵特征值\n",
    "tf.linalg.eigvalsh(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵qr分解\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "q,r = tf.linalg.qr(a)\n",
    "tf.print(q)\n",
    "tf.print(r)\n",
    "tf.print(q@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-0.316227794 -0.948683321]\n",
    " [-0.948683321 0.316227734]]\n",
    "[[-3.1622777 -4.4271884]\n",
    " [0 -0.632455349]]\n",
    "[[1.00000012 1.99999976]\n",
    " [3 4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵svd分解\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "v,s,d = tf.linalg.svd(a)\n",
    "tf.matmul(tf.matmul(s,tf.linalg.diag(v)),d)\n",
    "\n",
    "#利用svd分解可以在TensorFlow中实现主成分分析降维\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[0.9999996, 1.9999996],\n",
    "       [2.9999998, 4.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow的广播规则和numpy是一样的:\n",
    "\n",
    "* 1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。\n",
    "* 2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。\n",
    "* 3、如果两个张量在所有维度上都是相容的，它们就能使用广播。\n",
    "* 4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。\n",
    "* 5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。\n",
    "\n",
    "tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])\n",
    "b + a  #等价于 b + tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [2, 3, 4],\n",
    "       [3, 4, 5]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [1, 2, 3],\n",
    "       [1, 2, 3]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算广播后计算结果的形状，静态形状，TensorShape类型参数\n",
    "tf.broadcast_static_shape(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算广播后计算结果的形状，动态形状，Tensor类型参数\n",
    "c = tf.constant([1,2,3])\n",
    "d = tf.constant([[1],[2],[3]])\n",
    "tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#广播效果\n",
    "c+d #等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[6.5760484, 7.8174157],\n",
    "       [6.8174157, 6.4239516]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-3,AutoGraph的使用规范\n",
    "\n",
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "我们将着重介绍Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "并介绍使用tf.Module来更好地构建Autograph。\n",
    "\n",
    "本篇我们介绍使用Autograph的编码规范。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，Autograph编码规范总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1，被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print，使用tf.range而不是range，使用tf.constant(True)而不是True.\n",
    "\n",
    "* 2，避免在@tf.function修饰的函数内部定义tf.Variable. \n",
    "\n",
    "* 3，被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，Autograph编码规范解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1，被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def np_random():\n",
    "    a = np.random.randn(3,3)\n",
    "    tf.print(a)\n",
    "    \n",
    "@tf.function\n",
    "def tf_random():\n",
    "    a = tf.random.normal((3,3))\n",
    "    tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_random每次执行都是一样的结果。\n",
    "np_random()\n",
    "np_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "array([[ 0.22619201, -0.4550123 , -0.42587565],\n",
    "       [ 0.05429906,  0.2312667 , -1.44819738],\n",
    "       [ 0.36571796,  1.45578986, -1.05348983]])\n",
    "array([[ 0.22619201, -0.4550123 , -0.42587565],\n",
    "       [ 0.05429906,  0.2312667 , -1.44819738],\n",
    "       [ 0.36571796,  1.45578986, -1.05348983]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_random每次执行都会有重新生成随机数。\n",
    "tf_random()\n",
    "tf_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-1.38956189 -0.394843668 0.420657277]\n",
    " [2.87235498 -1.33740318 -0.533843279]\n",
    " [0.918233037 0.118598573 -0.399486482]]\n",
    "[[-0.858178258 1.67509317 0.511889517]\n",
    " [-0.545829177 -2.20118237 -0.968222201]\n",
    " [0.733958483 -0.61904633 0.77440238]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，避免在@tf.function修饰的函数内部定义tf.Variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 避免在@tf.function修饰的函数内部定义tf.Variable.\n",
    "\n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "@tf.function\n",
    "def outer_var():\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "outer_var() \n",
    "outer_var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inner_var():\n",
    "    x = tf.Variable(1.0,dtype = tf.float32)\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "#执行将报错\n",
    "#inner_var()\n",
    "#inner_var()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-12-c95a7c3c1ddd> in <module>\n",
    "      7 \n",
    "      8 #执行将报错\n",
    "----> 9 inner_var()\n",
    "     10 inner_var()\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\n",
    "    566         xla_context.Exit()\n",
    "    567     else:\n",
    "--> 568       result = self._call(*args, **kwds)\n",
    "    569 \n",
    "    570     if tracing_count == self._get_tracing_count():\n",
    "......\n",
    "ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3,被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "\n",
    "#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor: shape=(), dtype=float32, numpy=5.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "\n",
    "@tf.function #加上这一行切换成Autograph结果将不符合预期！！！\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Tensor 'x:0' shape=() dtype=float32>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-4,AutoGraph的机制原理\n",
    "\n",
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "我们会介绍Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "并介绍使用tf.Module来更好地构建Autograph。\n",
    "\n",
    "上篇我们介绍了Autograph的编码规范，本篇我们介绍Autograph的机制原理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，Autograph的机制原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当我们使用@tf.function装饰一个函数的时候，后面到底发生了什么呢？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "@tf.function(autograph=True)\n",
    "def myadd(a,b):\n",
    "    for i in tf.range(3):\n",
    "        tf.print(i)\n",
    "    c = a+b\n",
    "    print(\"tracing\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面什么都没有发生。仅仅是在Python堆栈中记录了这样一个函数的签名。\n",
    "\n",
    "**当我们第一次调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(\"hello\"),tf.constant(\"world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发生了2件事情，\n",
    "\n",
    "第一件事情是创建计算图。\n",
    "\n",
    "即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。\n",
    "在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。\n",
    "主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加\n",
    "tf.control_dependencies指定执行顺序依赖关系。\n",
    "\n",
    "相当于在 tensorflow1.0执行了类似下面的语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    b = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    cond = lambda i: i<tf.constant(3)\n",
    "    def body(i):\n",
    "        tf.print(i)\n",
    "        return(i+1)\n",
    "    loop = tf.while_loop(cond,body,loop_vars=[0])\n",
    "    loop\n",
    "    with tf.control_dependencies(loop):\n",
    "        c = tf.strings.join([a,b])\n",
    "    print(\"tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二件事情是执行计算图。\n",
    "\n",
    "相当于在 tensorflow1.0中执行了下面的语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(c,feed_dict={a:tf.constant(\"hello\"),b:tf.constant(\"world\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们先看到的是第一个步骤的结果：即Python调用标准输出流打印\"tracing\"语句。\n",
    "\n",
    "然后看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(\"good\"),tf.constant(\"morning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只会发生一件事情，那就是上面步骤的第二步，执行计算图。\n",
    "\n",
    "所以这一次我们没有看到打印\"tracing\"的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当我们再次用不同的的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(tf.constant(1),tf.constant(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。\n",
    "\n",
    "需要重新做2件事情：创建新的计算图、执行计算图。\n",
    "\n",
    "所以我们又会先看到的是第一个步骤的结果：即Python调用标准输出流打印\"tracing\"语句。\n",
    "\n",
    "然后再看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**需要注意的是，如果调用被@tf.function装饰的函数时输入的参数不是Tensor类型，则每次都会重新创建计算图。**\n",
    "\n",
    "例如我们写下如下代码。两次都会重新创建计算图。因此，一般建议调用@tf.function时应传入Tensor类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myadd(\"hello\",\"world\")\n",
    "myadd(\"good\",\"morning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "tracing\n",
    "0\n",
    "1\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，重新理解Autograph的编码规范"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解了以上Autograph的机制原理，我们也就能够理解Autograph编码规范的3条建议了。\n",
    "\n",
    "1，被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print.\n",
    "\n",
    "解释：Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用，普通Python函数是无法嵌入到静态计算图中的，所以\n",
    "在计算图构建好之后再次调用的时候，这些Python函数并没有被计算，而TensorFlow中的函数则可以嵌入到计算图中。使用普通的Python函数会导致\n",
    "被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。\n",
    "\n",
    "2，避免在@tf.function修饰的函数内部定义tf.Variable. \n",
    "\n",
    "解释：如果函数内部定义了tf.Variable,那么在【eager执行】时，这种创建tf.Variable的行为在每次函数调用时候都会发生。但是在【静态图执行】时，这种创建tf.Variable的行为只会发生在第一步跟踪Python代码逻辑创建计算图时，这会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。实际上，TensorFlow在这种情况下一般会报错。\n",
    "\n",
    "3，被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。\n",
    "\n",
    "解释：静态计算图是被编译成C++代码在TensorFlow内核中执行的。Python中的列表和字典等数据结构变量是无法嵌入到计算图中，它们仅仅能够在创建计算图时被读取，在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-5,AutoGraph和tf.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "前面我们介绍了Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "本篇我们介绍使用tf.Module来更好地构建Autograph。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，Autograph和tf.Module概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在@tf.function修饰的函数内部定义tf.Variable. \n",
    "\n",
    "但是如果在函数外部定义tf.Variable的话，又会显得这个函数有外部变量依赖，封装不够完美。\n",
    "\n",
    "一种简单的思路是定义一个类，并将相关的tf.Variable创建放在类的初始化方法中。而将函数的逻辑放在其他方法中。\n",
    "\n",
    "这样一顿猛如虎的操作之后，我们会觉得一切都如同人法地地法天天法道道法自然般的自然。\n",
    "\n",
    "惊喜的是，TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。\n",
    "\n",
    "实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。\n",
    "\n",
    "**因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用。**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，应用tf.Module封装Autograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个简单的function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "\n",
    "#在tf.function中用input_signature限定输入张量的签名类型：shape和dtype\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    \n",
    "def add_print(a):\n",
    "    x.assign_add(a)\n",
    "    tf.print(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_print(tf.constant(3.0))\n",
    "#add_print(tf.constant(3)) #输入不符合张量签名的参数将报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面利用tf.Module的子类化将其封装一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoModule(tf.Module):\n",
    "    def __init__(self,init_value = tf.constant(0.0),name=None):\n",
    "        super(DemoModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=True)\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "    def addprint(self,a):\n",
    "        with self.name_scope:\n",
    "            self.x.assign_add(a)\n",
    "            tf.print(self.x)\n",
    "            return(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#执行\n",
    "demo = DemoModule(init_value = tf.constant(1.0))\n",
    "result = demo.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看模块中的全部变量和全部可训练变量\n",
    "print(demo.variables)\n",
    "print(demo.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n",
    "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看模块中的全部子模块\n",
    "demo.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用tf.saved_model 保存模型，并指定需要跨平台部署的方法\n",
    "tf.saved_model.save(demo,\"./data/demo/1\",signatures = {\"serving_default\":demo.addprint})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "demo2 = tf.saved_model.load(\"./data/demo/1\")\n",
    "demo2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型文件相关信息，红框标出来的输出信息在模型部署和跨平台使用时有可能会用到\n",
    "!saved_model_cli show --dir ./data/demo/1 --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/查看模型文件信息.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在tensorboard中查看计算图，模块会被添加模块名demo_module,方便层次化呈现计算图结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 创建日志\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/demomodule/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "#开启autograph跟踪\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "#执行autograph\n",
    "demo = DemoModule(init_value = tf.constant(0.0))\n",
    "result = demo.addprint(tf.constant(5.0))\n",
    "\n",
    "#将计算图信息写入日志\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"demomodule\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动 tensorboard在jupyter中的魔法命令\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.start(\"--logdir ./data/demomodule/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/demomodule的计算图结构.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了利用tf.Module的子类化实现封装，我们也可以通过给tf.Module添加属性的方法进行封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodule = tf.Module()\n",
    "mymodule.x = tf.Variable(0.0)\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "def addprint(a):\n",
    "    mymodule.x.assign_add(a)\n",
    "    tf.print(mymodule.x)\n",
    "    return (mymodule.x)\n",
    "\n",
    "mymodule.addprint = addprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodule.addprint(tf.constant(1.0)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mymodule.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用tf.saved_model 保存模型\n",
    "tf.saved_model.save(mymodule,\"./data/mymodule\",\n",
    "    signatures = {\"serving_default\":mymodule.addprint})\n",
    "\n",
    "#加载模型\n",
    "mymodule2 = tf.saved_model.load(\"./data/mymodule\")\n",
    "mymodule2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "INFO:tensorflow:Assets written to: ./data/mymodule/assets\n",
    "5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，tf.Module和tf.keras.Model，tf.keras.layers.Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras中的模型和层都是继承tf.Module实现的，也具有变量管理和子模块管理功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,losses,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issubclass(tf.keras.Model,tf.Module))\n",
    "print(issubclass(tf.keras.layers.Layer,tf.Module))\n",
    "print(issubclass(tf.keras.Model,tf.keras.layers.Layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "True\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(4,input_shape = (10,)))\n",
    "model.add(layers.Dense(2))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 4)                 44        \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 2)                 10        \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 3         \n",
    "=================================================================\n",
    "Total params: 57\n",
    "Trainable params: 57\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Variable 'dense/kernel:0' shape=(10, 4) dtype=float32, numpy=\n",
    " array([[-0.06741005,  0.45534766,  0.5190817 , -0.01806331],\n",
    "        [-0.14258742, -0.49711505,  0.26030976,  0.18607801],\n",
    "        [-0.62806034,  0.5327399 ,  0.42206633,  0.29201728],\n",
    "        [-0.16602087, -0.18901917,  0.55159235, -0.01091868],\n",
    "        [ 0.04533798,  0.326845  , -0.582667  ,  0.19431782],\n",
    "        [ 0.6494713 , -0.16174704,  0.4062966 ,  0.48760796],\n",
    "        [ 0.58400524, -0.6280886 , -0.11265379, -0.6438277 ],\n",
    "        [ 0.26642334,  0.49275804,  0.20793378, -0.43889117],\n",
    "        [ 0.4092741 ,  0.09871006, -0.2073121 ,  0.26047975],\n",
    "        [ 0.43910992,  0.00199282, -0.07711256, -0.27966842]],\n",
    "       dtype=float32)>,\n",
    " <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
    " array([[ 0.5022683 , -0.0507431 ],\n",
    "        [-0.61540484,  0.9369011 ],\n",
    "        [-0.14412141, -0.54607415],\n",
    "        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
    " array([[-0.244825 ],\n",
    "        [-1.2101456]], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False #冻结第0层的变量,使其不可训练\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
    " array([[ 0.5022683 , -0.0507431 ],\n",
    "        [-0.61540484,  0.9369011 ],\n",
    "        [-0.14412141, -0.54607415],\n",
    "        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,\n",
    " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
    " array([[-0.244825 ],\n",
    "        [-1.2101456]], dtype=float32)>,\n",
    " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.submodules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x144d8c080>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144daada0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<tensorflow.python.keras.layers.core.Dense at 0x144daada0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,\n",
    " <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name)\n",
    "print(model.name_scope())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sequential\n",
    "sequential\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、TensorFlow的中阶API\n",
    "\n",
    "TensorFlow的中阶API主要包括: \n",
    "\n",
    "* 数据管道(tf.data)\n",
    "\n",
    "* 特征列(tf.feature_column)\n",
    "\n",
    "* 激活函数(tf.nn)\n",
    "\n",
    "* 模型层(tf.keras.layers)\n",
    "\n",
    "* 损失函数(tf.keras.losses)\n",
    "\n",
    "* 评估函数(tf.keras.metrics)\n",
    "\n",
    "* 优化器(tf.keras.optimizers)\n",
    "\n",
    "* 回调函数(tf.keras.callbacks)\n",
    "\n",
    "如果把模型比作一个房子，那么中阶API就是【模型之墙】。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-1,数据管道Dataset\n",
    "\n",
    "如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。\n",
    "\n",
    "但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。\n",
    "\n",
    "使用 tf.data API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，构建数据管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。\n",
    "\n",
    "其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。\n",
    "\n",
    "通过tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecoreds文件，读取后再解析成tf.Example。\n",
    "\n",
    "但tfrecoreds文件的优点是压缩后文件较小，便于网络传播，加载速度较快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1,从Numpy array构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Numpy array构建数据管道\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "ds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"],iris[\"target\"]))\n",
    "for features,label in ds1.take(5):\n",
    "    print(features,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2,从 Pandas DataFrame构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 Pandas DataFrame构建数据管道\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets \n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "dfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\n",
    "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n",
    "\n",
    "for features,label in ds2.take(3):\n",
    "    print(features,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3,从Python generator构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Python generator构建数据管道\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 定义一个从文件中读取图片的generator\n",
    "image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
    "                    \"./data/cifar2/test/\",\n",
    "                    target_size=(32, 32),\n",
    "                    batch_size=20,\n",
    "                    class_mode='binary')\n",
    "\n",
    "classdict = image_generator.class_indices\n",
    "print(classdict)\n",
    "\n",
    "def generator():\n",
    "    for features,label in image_generator:\n",
    "        yield (features,label)\n",
    "\n",
    "ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds3.unbatch().take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-cifar2预览.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4,从csv文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv文件构建数据管道\n",
    "ds4 = tf.data.experimental.make_csv_dataset(\n",
    "      file_pattern = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"],\n",
    "      batch_size=3, \n",
    "      label_name=\"Survived\",\n",
    "      na_value=\"\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)\n",
    "\n",
    "for data,label in ds4.take(2):\n",
    "    print(data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([540,  58, 764], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 1], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Frolicher, Miss. Hedwig Margaritha', b'Novel, Mr. Mansouer',\n",
    "       b'Carter, Mrs. William Ernest (Lucile Polk)'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'female', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([22. , 28.5, 36. ], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 2], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'13568', b'2697', b'113760'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 49.5   ,   7.2292, 120.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'B39', b'', b'B96 B98'], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'C', b'S'], dtype=object)>)]) tf.Tensor([1 0 1], shape=(3,), dtype=int32)\n",
    "OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([845,  66, 390], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 2], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
    "array([b'Culumovic, Mr. Jeso', b'Moubarek, Master. Gerios',\n",
    "       b'Lehmann, Miss. Bertha'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'male', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([17.,  0., 17.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'315090', b'2661', b'SC 1748'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 8.6625, 15.2458, 12.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'S', b'C', b'C'], dtype=object)>)]) tf.Tensor([0 1 1], shape=(3,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5,从文本文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件构建数据管道\n",
    "\n",
    "ds5 = tf.data.TextLineDataset(\n",
    "    filenames = [\"./data/titanic/train.csv\",\"./data/titanic/test.csv\"]\n",
    "    ).skip(1) #略去第一行header\n",
    "\n",
    "for line in ds5.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'687,0,3,\"Panula, Mr. Jaako Arnold\",male,14.0,4,1,3101295,39.6875,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6,从文件路径构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6 = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "for file in ds6.take(5):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/1263.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/2837.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/airplane/4264.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/4241.jpg', shape=(), dtype=string)\n",
    "tf.Tensor(b'./data/cifar2/train/automobile/192.jpg', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "for i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n",
    "    plt.figure(i)\n",
    "    plt.imshow((img/255.0).numpy())\n",
    "    plt.title(\"label = %d\"%label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7,从tfrecords文件构建数据管道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# inpath：原始数据路径 outpath:TFRecord文件输出路径\n",
    "def create_tfrecords(inpath,outpath): \n",
    "    writer = tf.io.TFRecordWriter(outpath)\n",
    "    dirs = os.listdir(inpath)\n",
    "    for index, name in enumerate(dirs):\n",
    "        class_path = inpath +\"/\"+ name+\"/\"\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = class_path + img_name\n",
    "            img = tf.io.read_file(img_path)\n",
    "            #img = tf.image.decode_image(img)\n",
    "            #img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩\n",
    "            example = tf.train.Example(\n",
    "               features=tf.train.Features(feature={\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n",
    "               }))\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "create_tfrecords(\"./data/cifar2/test/\",\"./data/cifar2_test.tfrecords/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def parse_example(proto):\n",
    "    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n",
    "                   'label': tf.io.FixedLenFeature([], tf.int64)} \n",
    "    example = tf.io.parse_single_example(proto, description)\n",
    "    img = tf.image.decode_jpeg(example[\"img_raw\"])   #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img, (32,32))\n",
    "    label = example[\"label\"]\n",
    "    return(img,label)\n",
    "\n",
    "ds7 = tf.data.TFRecordDataset(\"./data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.figure(figsize=(6,6)) \n",
    "for i,(img,label) in enumerate(ds7.take(9)):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow((img/255.0).numpy())\n",
    "    ax.set_title(\"label = %d\"%label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-1-car9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，应用数据转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。\n",
    "\n",
    "Dataset包含了非常丰富的数据转换功能。\n",
    "\n",
    "* map: 将转换函数映射到数据集每一个元素。\n",
    "\n",
    "* flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n",
    "\n",
    "* interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n",
    "\n",
    "* filter: 过滤掉某些元素。\n",
    "\n",
    "* zip: 将两个长度相同的Dataset横向铰合。\n",
    "\n",
    "* concatenate: 将两个Dataset纵向连接。\n",
    "\n",
    "* reduce: 执行归并操作。\n",
    "\n",
    "* batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。\n",
    "\n",
    "* padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。\n",
    "\n",
    "* window :构建滑动窗口，返回Dataset of Dataset.\n",
    "\n",
    "* shuffle: 数据顺序洗牌。\n",
    "\n",
    "* repeat: 重复数据若干次，不带参数时，重复无数次。\n",
    "\n",
    "* shard: 采样，从某个位置开始隔固定距离采样一个元素。\n",
    "\n",
    "* take: 采样，从开始位置取前几个元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map:将转换函数映射到数据集每一个元素\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_map = ds.map(lambda x:tf.strings.split(x,\" \"))\n",
    "for x in ds_map:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)\n",
    "tf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_flatmap = ds.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_flatmap:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "ds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
    "for x in ds_interleave:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello', shape=(), dtype=string)\n",
    "tf.Tensor(b'world', shape=(), dtype=string)\n",
    "tf.Tensor(b'China', shape=(), dtype=string)\n",
    "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter:过滤掉某些元素。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
    "#找出含有字母a或B的元素\n",
    "ds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, \".*[a|B].*\"))\n",
    "for x in ds_filter:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'hello China', shape=(), dtype=string)\n",
    "tf.Tensor(b'hello Beijing', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip:将两个长度相同的Dataset横向铰合。\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds3 = tf.data.Dataset.range(6,9)\n",
    "ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))\n",
    "for x,y,z in ds_zip:\n",
    "    print(x.numpy(),y.numpy(),z.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 3 6\n",
    "1 4 7\n",
    "2 5 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condatenate:将两个Dataset纵向连接。\n",
    "\n",
    "ds1 = tf.data.Dataset.range(0,3)\n",
    "ds2 = tf.data.Dataset.range(3,6)\n",
    "ds_concat = tf.data.Dataset.concatenate(ds1,ds2)\n",
    "for x in ds_concat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce:执行归并操作。\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5.0])\n",
    "result = ds.reduce(0.0,lambda x,y:tf.add(x,y))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 \n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_batch = ds.batch(4)\n",
    "for x in ds_batch:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
    "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。\n",
    "\n",
    "elements = [[1, 2],[3, 4, 5],[6, 7],[8]]\n",
    "ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n",
    "\n",
    "ds_padded_batch = ds.padded_batch(2,padded_shapes = [4,])\n",
    "for x in ds_padded_batch:\n",
    "    print(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[1 2 0 0]\n",
    " [3 4 5 0]], shape=(2, 4), dtype=int32)\n",
    "tf.Tensor(\n",
    "[[6 7 0 0]\n",
    " [8 0 0 0]], shape=(2, 4), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window:构建滑动窗口，返回Dataset of Dataset.\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "#window返回的是Dataset of Dataset,可以用flat_map压平\n",
    "ds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3,drop_remainder=True)) \n",
    "for x in ds_window:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
    "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
    "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
    "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
    "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n",
    "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle:数据顺序洗牌。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shuffle = ds.shuffle(buffer_size = 5)\n",
    "for x in ds_shuffle:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(6, shape=(), dtype=int64)\n",
    "tf.Tensor(5, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(11, shape=(), dtype=int64)\n",
    "tf.Tensor(3, shape=(), dtype=int64)\n",
    "tf.Tensor(9, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "tf.Tensor(8, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat:重复数据若干次，不带参数时，重复无数次。\n",
    "\n",
    "ds = tf.data.Dataset.range(3)\n",
    "ds_repeat = ds.repeat(3)\n",
    "for x in ds_repeat:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shard:采样，从某个位置开始隔固定距离采样一个元素。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_shard = ds.shard(3,index = 1)\n",
    "\n",
    "for x in ds_shard:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(1, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)\n",
    "tf.Tensor(7, shape=(), dtype=int64)\n",
    "tf.Tensor(10, shape=(), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take:采样，从开始位置取前几个元素。\n",
    "\n",
    "ds = tf.data.Dataset.range(12)\n",
    "ds_take = ds.take(3)\n",
    "\n",
    "list(ds_take.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 1, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，提升管道性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练深度学习模型常常会非常耗时。\n",
    "\n",
    "模型训练的耗时主要来自于两个部分，一部分来自**数据准备**，另一部分来自**参数迭代**。\n",
    "\n",
    "参数迭代过程的耗时通常依赖于GPU来提升。\n",
    "\n",
    "而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。\n",
    "\n",
    "以下是一些构建高效数据管道的建议。\n",
    "\n",
    "* 1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n",
    "\n",
    "* 2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。\n",
    "\n",
    "* 3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。\n",
    "\n",
    "* 4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\n",
    "\n",
    "* 5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 数据准备和参数迭代两个过程默认情况下是串行的。\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(10):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要1s\n",
    "    time.sleep(1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程预计耗时 10*2+10*1+ = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for x in ds:\n",
    "    train_step()  \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n",
    "\n",
    "# 训练过程预计耗时 max(10*2,10*1) = 20s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training with prefetch...\"))\n",
    "\n",
    "# tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数\n",
    "for x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):\n",
    "    train_step()  \n",
    "    \n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(4):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_files = tf.data.Dataset.list_files(\"./data/titanic/*.csv\")\n",
    "ds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
    "for line in ds.take(8):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(b'181,0,3,\"Sage, Miss. Constance Gladys\",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'405,0,3,\"Oreskovic, Miss. Marija\",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'635,0,3,\"Skoog, Miss. Mabel\",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\n",
    "tf.Tensor(b'701,1,1,\"Astor, Mrs. John Jacob (Madeleine Talmadge Force)\",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)\n",
    "tf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\")\n",
    "def load_image(img_path,size = (32,32)):\n",
    "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n",
    "    img = tf.image.resize(img,size)\n",
    "    return(img,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单进程转换\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start transformation...\"))\n",
    "\n",
    "ds_map = ds.map(load_image)\n",
    "for _ in ds_map:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多进程转换\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start parallel transformation...\"))\n",
    "\n",
    "ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "for _ in ds_map_parallel:\n",
    "    pass\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end parallel transformation...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要0s\n",
    "    pass\n",
    "\n",
    "# 训练过程预计耗时 (5*2+5*0)*3 = 30s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 模拟数据准备\n",
    "def generator():\n",
    "    for i in range(5):\n",
    "        #假设每次准备数据需要2s\n",
    "        time.sleep(2) \n",
    "        yield i \n",
    "\n",
    "# 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\n",
    "ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()\n",
    "\n",
    "# 模拟参数迭代\n",
    "def train_step():\n",
    "    #假设每一步训练需要0s\n",
    "    time.sleep(0) \n",
    "\n",
    "# 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start training...\"))\n",
    "for epoch in tf.range(3):\n",
    "    for x in ds:\n",
    "        train_step()  \n",
    "    printbar()\n",
    "    tf.print(\"epoch =\",epoch,\" ended\")\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end training...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先map后batch\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_map_batch = ds.map(lambda x:x**2).batch(20)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start scalar transformation...\"))\n",
    "for x in ds_map_batch:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end scalar transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先batch后map\n",
    "ds = tf.data.Dataset.range(100000)\n",
    "ds_batch_map = ds.batch(20).map(lambda x:x**2)\n",
    "\n",
    "printbar()\n",
    "tf.print(tf.constant(\"start vector transformation...\"))\n",
    "for x in ds_batch_map:\n",
    "    pass\n",
    "printbar()\n",
    "tf.print(tf.constant(\"end vector transformation...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-2,特征列feature_column\n",
    "\n",
    "特征列 通常用于对结构化数据实施特征工程时候使用，图像或者文本数据一般不会用到特征列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，特征列用法概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用特征列可以将类别特征转换为one-hot编码特征，将连续特征构建分桶特征，以及对多个特征生成交叉特征等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要创建特征列，请调用 tf.feature_column 模块的函数。该模块中常用的九个函数如下图所示，所有九个函数都会返回一个 Categorical-Column 或一个 \n",
    "Dense-Column 对象，但却不会返回 bucketized_column，后者继承自这两个类。\n",
    "\n",
    "注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/特征列9种.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numeric_column 数值列，最常用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bucketized_column 分桶列，由数值列生成，可以由一个数值列出多个特征，one-hot编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_identity 分类标识列，one-hot编码，相当于分桶列每个桶为1个整数的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_vocabulary_list 分类词汇列，one-hot编码，由list指定词典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_vocabulary_file 分类词汇列，由文件file指定词典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorical_column_with_hash_bucket 哈希列，整数或词典较大时采用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* indicator_column 指标列，由Categorical Column生成，one-hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embedding_column 嵌入列，由Categorical Column生成，嵌入矢量分布参数需要学习。嵌入矢量维数建议取类别数量的 4 次方根。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crossed_column 交叉列，可以由除categorical_column_with_hash_bucket的任意分类列构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，特征列使用范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一个使用特征列解决Titanic生存问题的完整范例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "\n",
    "#打印日志\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(info+'...\\n\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 一，构建数据管道\n",
    "#================================================================================\n",
    "printlog(\"step1: prepare dataset...\")\n",
    "\n",
    "\n",
    "dftrain_raw = pd.read_csv(\"./data/titanic/train.csv\")\n",
    "dftest_raw = pd.read_csv(\"./data/titanic/test.csv\")\n",
    "\n",
    "dfraw = pd.concat([dftrain_raw,dftest_raw])\n",
    "\n",
    "def prepare_dfdata(dfraw):\n",
    "    dfdata = dfraw.copy()\n",
    "    dfdata.columns = [x.lower() for x in dfdata.columns]\n",
    "    dfdata = dfdata.rename(columns={'survived':'label'})\n",
    "    dfdata = dfdata.drop(['passengerid','name'],axis = 1)\n",
    "    for col,dtype in dict(dfdata.dtypes).items():\n",
    "        # 判断是否包含缺失值\n",
    "        if dfdata[col].hasnans:\n",
    "            # 添加标识是否缺失列\n",
    "            dfdata[col + '_nan'] = pd.isna(dfdata[col]).astype('int32')\n",
    "            # 填充\n",
    "            if dtype not in [np.object,np.str,np.unicode]:\n",
    "                dfdata[col].fillna(dfdata[col].mean(),inplace = True)\n",
    "            else:\n",
    "                dfdata[col].fillna('',inplace = True)\n",
    "    return(dfdata)\n",
    "\n",
    "dfdata = prepare_dfdata(dfraw)\n",
    "dftrain = dfdata.iloc[0:len(dftrain_raw),:]\n",
    "dftest = dfdata.iloc[len(dftrain_raw):,:]\n",
    "\n",
    "\n",
    "\n",
    "# 从 dataframe 导入数据 \n",
    "def df_to_dataset(df, shuffle=True, batch_size=32):\n",
    "    dfdata = df.copy()\n",
    "    if 'label' not in dfdata.columns:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = 'list'))\n",
    "    else: \n",
    "        labels = dfdata.pop('label')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = 'list'), labels))  \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dfdata))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "ds_train = df_to_dataset(dftrain)\n",
    "ds_test = df_to_dataset(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 二，定义特征列\n",
    "#================================================================================\n",
    "printlog(\"step2: make feature columns...\")\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# 数值列\n",
    "for col in ['age','fare','parch','sibsp'] + [\n",
    "    c for c in dfdata.columns if c.endswith('_nan')]:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))\n",
    "\n",
    "# 分桶列\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "age_buckets = tf.feature_column.bucketized_column(age, \n",
    "             boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# 类别列\n",
    "# 注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！！\n",
    "sex = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='sex',vocabulary_list=[\"male\", \"female\"]))\n",
    "feature_columns.append(sex)\n",
    "\n",
    "pclass = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='pclass',vocabulary_list=[1,2,3]))\n",
    "feature_columns.append(pclass)\n",
    "\n",
    "ticket = tf.feature_column.indicator_column(\n",
    "     tf.feature_column.categorical_column_with_hash_bucket('ticket',3))\n",
    "feature_columns.append(ticket)\n",
    "\n",
    "embarked = tf.feature_column.indicator_column(\n",
    "      tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='embarked',vocabulary_list=['S','C','B']))\n",
    "feature_columns.append(embarked)\n",
    "\n",
    "# 嵌入列\n",
    "cabin = tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('cabin',32),2)\n",
    "feature_columns.append(cabin)\n",
    "\n",
    "# 交叉列\n",
    "pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "          key='pclass',vocabulary_list=[1,2,3])\n",
    "\n",
    "crossed_feature = tf.feature_column.indicator_column(\n",
    "    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=15))\n",
    "\n",
    "feature_columns.append(crossed_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 三，定义模型\n",
    "#================================================================================\n",
    "printlog(\"step3: define model...\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "  layers.DenseFeatures(feature_columns), #将特征列放入到tf.keras.layers.DenseFeatures中!!!\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 四，训练模型\n",
    "#================================================================================\n",
    "printlog(\"step4: train model...\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(ds_train,\n",
    "          validation_data=ds_test,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 五，评估模型\n",
    "#================================================================================\n",
    "printlog(\"step5: eval model...\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(history,\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_features (DenseFeature multiple                  64        \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  3008      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              multiple                  4160      \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              multiple                  65        \n",
    "=================================================================\n",
    "Total params: 7,297\n",
    "Trainable params: 7,297\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/5-2-01-模型评估.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-3,激活函数activation\n",
    "\n",
    "激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。\n",
    "\n",
    "如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。\n",
    "\n",
    "目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，例如 swish、GELU 据称效果优于relu激活函数。\n",
    "\n",
    "激活函数的综述介绍可以参考下面两篇文章。\n",
    "\n",
    "[《一文概览深度学习中的激活函数》](https://zhuanlan.zhihu.com/p/98472075)\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/98472075\n",
    "\n",
    "[《从ReLU到GELU,一文概览神经网络中的激活函数》](https://zhuanlan.zhihu.com/p/98863801)\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/98863801\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，常用激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.sigmoid：将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。\n",
    "\n",
    "![](./data/sigmoid.png)\n",
    "\n",
    "* tf.nn.softmax：sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。\n",
    "\n",
    "![](./data/softmax说明.jpg)\n",
    "\n",
    "* tf.nn.tanh：将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。\n",
    "\n",
    "![](./data/tanh.png)\n",
    "\n",
    "* tf.nn.relu：修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。\n",
    "\n",
    "![](./data/relu.png)\n",
    "\n",
    "* tf.nn.leaky_relu：对修正线性单元的改进，解决了死亡relu问题。\n",
    "\n",
    "![](./data/leaky_relu.png)\n",
    "\n",
    "* tf.nn.elu：指数线性单元。对relu的改进，能够缓解死亡relu问题。\n",
    "\n",
    "![](./data/elu.png)\n",
    "\n",
    "* tf.nn.selu：扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。\n",
    "\n",
    "![](./data/selu.png)\n",
    "\n",
    "* tf.nn.swish：自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。\n",
    "\n",
    "![](./data/swish.png)\n",
    "\n",
    "* gelu：高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。\n",
    "\n",
    "![](./data/gelu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，在模型中使用激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在keras模型中使用激活函数一般有两种方式，一种是作为某些层的activation参数指定，另一种是显式添加layers.Activation激活层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) #通过activation参数指定\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Activation(tf.nn.softmax))  # 显式添加layers.Activation激活层\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-4,模型层layers\n",
    "\n",
    "深度学习模型一般由各种模型层组合而成。\n",
    "\n",
    "tf.keras.layers内置了非常丰富的各种功能的模型层。例如，\n",
    "\n",
    "layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout\n",
    "\n",
    "layers.Conv2D,layers.MaxPooling2D,layers.Conv1D\n",
    "\n",
    "layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional等等。\n",
    "\n",
    "如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。\n",
    "\n",
    "其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，内置模型层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些常用的内置模型层简单介绍如下。\n",
    "\n",
    "**基础层**\n",
    "\n",
    "* Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\n",
    "\n",
    "* Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。\n",
    "\n",
    "* Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。\n",
    "\n",
    "* BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。\n",
    "\n",
    "* SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。\n",
    "\n",
    "* Input：输入层。通常使用Functional API方式构建模型时作为第一层。\n",
    "\n",
    "* DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。\n",
    "\n",
    "* Flatten：压平层，用于将多维张量压成一维。\n",
    "\n",
    "* Reshape：形状重塑层，改变输入张量的形状。\n",
    "\n",
    "* Concatenate：拼接层，将多个张量在某个维度上拼接。\n",
    "\n",
    "* Add：加法层。\n",
    "\n",
    "* Subtract： 减法层。\n",
    "\n",
    "* Maximum：取最大值层。\n",
    "\n",
    "* Minimum：取最小值层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积网络相关层**\n",
    "\n",
    "* Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数\n",
    "\n",
    "* Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数\n",
    "\n",
    "* Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数\n",
    "\n",
    "* SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷即先操作区域，再用1乘1卷积跨通道组合即再操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。\n",
    "\n",
    "* DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。\n",
    "\n",
    "* Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。\n",
    "\n",
    "* LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。\n",
    "\n",
    "* MaxPooling2D: 二维最大池化层。也称作下采样层。池化层无参数，主要作用是降维。\n",
    "\n",
    "* AveragePooling2D: 二维平均池化层。\n",
    "\n",
    "* GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。\n",
    "\n",
    "* GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环网络相关层**\n",
    "\n",
    "* Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\n",
    "\n",
    "* LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。\n",
    "\n",
    "* GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\n",
    "\n",
    "* SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\n",
    "\n",
    "* ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。\n",
    "\n",
    "* Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。\n",
    "\n",
    "* RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。\n",
    "\n",
    "* LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。\n",
    "\n",
    "* GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。\n",
    "\n",
    "* Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，自定义模型层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。\n",
    "\n",
    "如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。\n",
    "\n",
    "Lamda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。\n",
    "\n",
    "Lamda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,regularizers\n",
    "\n",
    "mypower = layers.Lambda(lambda x:tf.math.pow(x,2))\n",
    "mypower(tf.range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    #build方法一般定义Layer需要被训练的参数。    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(Linear,self).build(input_shape) # 相当于设置self.built = True\n",
    "\n",
    "    #call方法一般定义正向传播运算逻辑，__call__方法调用了它。    \n",
    "    def call(self, inputs): \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    #如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。\n",
    "    def get_config(self):  \n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.built)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.compute_output_shape(input_shape = (None,16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "(None, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(units = 16)\n",
    "print(linear.built)\n",
    "#如果built = False，调用__call__时会先调用build方法, 再调用call方法。\n",
    "linear(tf.random.uniform((100,64))) \n",
    "print(linear.built)\n",
    "config = linear.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "False\n",
    "True\n",
    "{'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "#注意该处的input_shape会被模型加工，无需使用None代表样本数量维\n",
    "model.add(Linear(units = 16,input_shape = (64,)))  \n",
    "print(\"model.input_shape: \",model.input_shape)\n",
    "print(\"model.output_shape: \",model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.input_shape:  (None, 64)\n",
    "model.output_shape:  (None, 16)\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "linear (Linear)              (None, 16)                1040      \n",
    "=================================================================\n",
    "Total params: 1,040\n",
    "Trainable params: 1,040\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-5,损失函数losses\n",
    "\n",
    "一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）\n",
    "\n",
    "对于keras模型，目标函数中的正则化项一般在各层中指定，例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。\n",
    "\n",
    "损失函数在模型编译时候指定。对于回归模型，通常使用的损失函数是平方损失函数 mean_squared_error。\n",
    "\n",
    "对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。\n",
    "\n",
    "对于多分类模型，如果label是类别序号编码的，则使用类别交叉熵损失函数 categorical_crossentropy。如果label进行了one-hot编码，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。\n",
    "\n",
    "如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，损失函数和正则化项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01), \n",
    "                activity_regularizer=regularizers.l1(0.01),\n",
    "                kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) \n",
    "model.add(layers.Dense(10,\n",
    "        kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "        loss = \"sparse_categorical_crossentropy\",metrics = [\"AUC\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 4,810\n",
    "Trainable params: 4,810\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，内置损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "常用的一些内置损失函数说明如下。\n",
    "\n",
    "* mean_squared_error（平方差误差损失，用于回归，简写为 mse, 类实现形式为 MeanSquaredError 和 MSE）\n",
    "\n",
    "* mean_absolute_error (绝对值误差损失，用于回归，简写为 mae, 类实现形式为 MeanAbsoluteError 和 MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类实现形式为 MeanAbsolutePercentageError 和 MAPE)\n",
    "\n",
    "* Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)\n",
    "\n",
    "* binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)\n",
    "\n",
    "* hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)\n",
    "\n",
    "* kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类实现形式为 KLDivergence 或 KLD)\n",
    "\n",
    "* cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。\n",
    "\n",
    "下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。\n",
    "\n",
    "在类别不平衡和存在难以训练样本的情形下相对于二元交叉熵能够取得更好的效果。\n",
    "\n",
    "详见《如何评价Kaiming的Focal Loss for Dense Object Detection？》\n",
    "\n",
    "https://www.zhihu.com/question/63581984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(losses.Loss):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.25):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        \n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(self.alpha * tf.pow(1. - pt_1, self.gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-self.alpha) * tf.pow( pt_0, self.gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-6,评估指标metrics\n",
    "\n",
    "损失函数除了作为模型训练时候的优化目标，也能够作为模型好坏的一种评价指标。但通常人们还会从其它角度评估模型的好坏。\n",
    "\n",
    "这就是评估指标。通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。\n",
    "\n",
    "但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。\n",
    "\n",
    "编译模型时，可以通过列表形式指定多个评估指标。\n",
    "\n",
    "如果有需要，也可以自定义评估指标。\n",
    "\n",
    "自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。\n",
    "\n",
    "也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。\n",
    "\n",
    "由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。\n",
    "\n",
    "如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离拿整个epoch数据一次计算的结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，常用的内置评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MeanSquaredError（平方差误差，用于回归，可以简写为MSE，函数形式为mse）\n",
    "\n",
    "* MeanAbsoluteError (绝对值误差，用于回归，可以简写为MAE，函数形式为mae)\n",
    "\n",
    "* MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)\n",
    "\n",
    "* RootMeanSquaredError (均方根误差，用于回归)\n",
    "\n",
    "* Accuracy (准确率，用于分类，可以用字符串\"Accuracy\"表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)\n",
    "\n",
    "* Precision (精确率，用于二分类，Precision = TP/(TP+FP))\n",
    "\n",
    "* Recall (召回率，用于二分类，Recall = TP/(TP+FN))\n",
    "\n",
    "* TruePositives (真正例，用于二分类)\n",
    "\n",
    "* TrueNegatives (真负例，用于二分类)\n",
    "\n",
    "* FalsePositives (假正例，用于二分类)\n",
    "\n",
    "* FalseNegatives (假负例，用于二分类)\n",
    "\n",
    "* AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)\n",
    "\n",
    "* CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）\n",
    "\n",
    "* SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* MeanIoU (Intersection-Over-Union，常用于图像分割)\n",
    "\n",
    "* TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)\n",
    "\n",
    "* SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* Mean (平均值)\n",
    "\n",
    "* Sum (求和)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二， 自定义评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以金融风控领域常用的KS指标为例，示范自定义评估指标。\n",
    "\n",
    "KS指标适合二分类问题，其计算方式为 KS=max(TPR-FPR).\n",
    "\n",
    "其中TPR=TP/(TP+FN) , FPR = FP/(FP+TN) \n",
    "\n",
    "TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF)。\n",
    "\n",
    "KS指标就是正样本和负样本累积分布曲线差值的最大值。\n",
    "\n",
    "![](./data/KS_curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,metrics\n",
    "\n",
    "#函数形式的自定义评估指标\n",
    "@tf.function\n",
    "def ks(y_true,y_pred):\n",
    "    y_true = tf.reshape(y_true,(-1,))\n",
    "    y_pred = tf.reshape(y_pred,(-1,))\n",
    "    length = tf.shape(y_true)[0]\n",
    "    t = tf.math.top_k(y_pred,k = length,sorted = False)\n",
    "    y_pred_sorted = tf.gather(y_pred,t.indices)\n",
    "    y_true_sorted = tf.gather(y_true,t.indices)\n",
    "    cum_positive_ratio = tf.truediv(\n",
    "        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))\n",
    "    cum_negative_ratio = tf.truediv(\n",
    "        tf.cumsum(1 - y_true_sorted),tf.reduce_sum(1 - y_true_sorted))\n",
    "    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "    return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],[0.7],\n",
    "                      [0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "tf.print(ks(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.625\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#类形式的自定义评估指标\n",
    "class KS(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name = \"ks\", **kwargs):\n",
    "        super(KS,self).__init__(name=name,**kwargs)\n",
    "        self.true_positives = self.add_weight(\n",
    "            name = \"tp\",shape = (101,), initializer = \"zeros\")\n",
    "        self.false_positives = self.add_weight(\n",
    "            name = \"fp\",shape = (101,), initializer = \"zeros\")\n",
    "   \n",
    "    @tf.function\n",
    "    def update_state(self,y_true,y_pred):\n",
    "        y_true = tf.cast(tf.reshape(y_true,(-1,)),tf.bool)\n",
    "        y_pred = tf.cast(100*tf.reshape(y_pred,(-1,)),tf.int32)\n",
    "        \n",
    "        for i in tf.range(0,tf.shape(y_true)[0]):\n",
    "            if y_true[i]:\n",
    "                self.true_positives[y_pred[i]].assign(\n",
    "                    self.true_positives[y_pred[i]]+1.0)\n",
    "            else:\n",
    "                self.false_positives[y_pred[i]].assign(\n",
    "                    self.false_positives[y_pred[i]]+1.0)\n",
    "        return (self.true_positives,self.false_positives)\n",
    "    \n",
    "    @tf.function\n",
    "    def result(self):\n",
    "        cum_positive_ratio = tf.truediv(\n",
    "            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))\n",
    "        cum_negative_ratio = tf.truediv(\n",
    "            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))\n",
    "        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "        return ks_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],\n",
    "                      [0.7],[0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "\n",
    "myks = KS()\n",
    "myks.update_state(y_true,y_pred)\n",
    "tf.print(myks.result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.625\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "# 5-7,优化器optimizers\n",
    "\n",
    "机器学习界有一群炼丹师，他们每天的日常是：\n",
    "\n",
    "拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。\n",
    "\n",
    "不过，当过厨子的都知道，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。\n",
    "\n",
    "机器学习也是一样，模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。\n",
    "\n",
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "详见《一个框架看懂优化算法之异同 SGD/AdaGrad/Adam》\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/32230623\n",
    "\n",
    "对于一般新手炼丹师，优化器直接使用Adam，并使用其默认参数就OK了。\n",
    "\n",
    "一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。\n",
    "\n",
    "此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，优化器的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。\n",
    "\n",
    "当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。\n",
    "\n",
    "初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    while tf.constant(True): \n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "        #迭代终止条件\n",
    "        if tf.abs(dy_dx)<tf.constant(0.00001):\n",
    "            break\n",
    "            \n",
    "        if tf.math.mod(optimizer.iterations,100)==0:\n",
    "            printbar()\n",
    "            tf.print(\"step = \",optimizer.iterations)\n",
    "            tf.print(\"x = \", x)\n",
    "            tf.print(\"\")\n",
    "                \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(\"y =\",minimizef())\n",
    "tf.print(\"x =\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "# 使用optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch = 1000):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    tf.print(\"epoch = \",optimizer.iterations)\n",
    "    return(f())\n",
    "\n",
    "train(1000)\n",
    "tf.print(\"y = \",f())\n",
    "tf.print(\"x = \",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用model.fit\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class FakeModel(tf.keras.models.Model):\n",
    "    def __init__(self,a,b,c):\n",
    "        super(FakeModel,self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def build(self):\n",
    "        self.x = tf.Variable(0.0,name = \"x\")\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self,features):\n",
    "        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\n",
    "        return(tf.ones_like(features)*loss)\n",
    "    \n",
    "def myloss(y_true,y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = \n",
    "              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\n",
    "history = model.fit(tf.zeros((100,2)),\n",
    "                    tf.ones(100),batch_size = 1,epochs = 10)  #迭代1000次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(\"x=\",model.x)\n",
    "tf.print(\"loss=\",model(tf.constant(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，内置优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "在keras.optimizers子模块中，它们基本上都有对应的类的实现。\n",
    "\n",
    "* SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Acceleration Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。\n",
    "\n",
    "* Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。\n",
    "\n",
    "* RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。\n",
    "\n",
    "* Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。\n",
    "\n",
    "* Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了Momentum。\n",
    "\n",
    "* Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-8,回调函数callbacks\n",
    "\n",
    "tf.keras的回调函数实际上是一个类，一般是在model.fit时作为参数指定，用于控制在训练过程开始或者在训练过程结束，在每个epoch训练开始或者训练结束，在每个batch训练开始或者训练结束时执行一些操作，例如收集一些日志信息，改变学习率等超参数，提前终止训练过程等等。\n",
    "\n",
    "同样地，针对model.evaluate或者model.predict也可以指定callbacks参数，用于控制在评估或预测开始或者结束时，在每个batch开始或者结束时执行一些操作，但这种用法相对少见。\n",
    "\n",
    "大部分时候，keras.callbacks子模块中定义的回调函数类已经足够使用了，如果有特定的需要，我们也可以通过对keras.callbacks.Callbacks实施子类化构造自定义的回调函数。\n",
    "\n",
    "所有回调函数都继承至 keras.callbacks.Callbacks基类，拥有params和model这两个属性。\n",
    "\n",
    "其中params 是一个dict，记录了 training parameters (eg. verbosity, batch size, number of epochs...).\n",
    "\n",
    "model即当前关联的模型的引用。\n",
    "\n",
    "此外，对于回调类中的一些方法如on_epoch_begin,on_batch_end，还会有一个输入参数logs, 提供有关当前epoch或者batch的一些信息，并能够记录计算结果，如果model.fit指定了多个回调函数类，这些logs变量将在这些回调函数类的同名函数间依顺序传递。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，内置回调函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BaseLogger： 收集每个epoch上metrics在各个batch上的平均值，对stateful_metrics参数中的带中间状态的指标直接拿最终值无需对各个batch平均，指标均值结果将添加到logs变量中。该回调函数被所有模型默认添加，且是第一个被添加的。\n",
    "\n",
    "* History： 将BaseLogger计算的各个epoch的metrics结果记录到history这个dict变量中，并作为model.fit的返回值。该回调函数被所有模型默认添加，在BaseLogger之后被添加。\n",
    "\n",
    "* EarlyStopping： 当被监控指标在设定的若干个epoch后没有提升，则提前终止训练。\n",
    "\n",
    "* TensorBoard： 为Tensorboard可视化保存日志信息。支持评估指标，计算图，模型参数等的可视化。\n",
    "\n",
    "* ModelCheckpoint： 在每个epoch后保存模型。\n",
    "\n",
    "* ReduceLROnPlateau：如果监控指标在设定的若干个epoch后没有提升，则以一定的因子减少学习率。\n",
    "\n",
    "* TerminateOnNaN：如果遇到loss为NaN，提前终止训练。\n",
    "\n",
    "* LearningRateScheduler：学习率控制器。给定学习率lr和epoch的函数关系，根据该函数关系在每个epoch前调整学习率。\n",
    "\n",
    "* CSVLogger：将每个epoch后的logs结果记录到CSV文件中。\n",
    "\n",
    "* ProgbarLogger：将每个epoch后的logs结果打印到标准输出流中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，自定义回调函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用callbacks.LambdaCallback编写较为简单的回调函数，也可以通过对callbacks.Callback子类化编写更加复杂的回调函数逻辑。\n",
    "\n",
    "如果需要深入学习tf.Keras中的回调函数，不要犹豫阅读内置回调函数的源代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,metrics,callbacks\n",
    "import tensorflow.keras.backend as K \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示范使用LambdaCallback编写较为简单的回调函数\n",
    "\n",
    "import json\n",
    "json_log = open('./data/keras_log.json', mode='wt', buffering=1)\n",
    "json_logging_callback = callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps(dict(epoch = epoch,**logs)) + '\\n'),\n",
    "    on_train_end=lambda logs: json_log.close()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示范通过Callback子类化编写回调函数（LearningRateScheduler的源代码）\n",
    "\n",
    "class LearningRateScheduler(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, schedule, verbose=0):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        try:  \n",
    "            lr = float(K.get_value(self.model.optimizer.lr))\n",
    "            lr = self.schedule(epoch, lr)\n",
    "        except TypeError:  # Support for old API for backward compatibility\n",
    "            lr = self.schedule(epoch)\n",
    "        if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):\n",
    "            raise ValueError('The output of the \"schedule\" function '\n",
    "                             'should be float.')\n",
    "        if isinstance(lr, ops.Tensor) and not lr.dtype.is_floating:\n",
    "            raise ValueError('The dtype of Tensor should be float')\n",
    "        K.set_value(self.model.optimizer.lr, K.get_value(lr))\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n",
    "                 'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "# 六、TensorFlow的高阶API\n",
    "\n",
    "TensorFlow的高阶API主要是tensorflow.keras.models.\n",
    "\n",
    "本章我们主要详细介绍tensorflow.keras.models相关的以下内容。\n",
    "\n",
    "* 模型的构建（Sequential、functional API、Model子类化）\n",
    "\n",
    "* 模型的训练（内置fit方法、内置train_on_batch方法、自定义训练循环、单GPU训练模型、多GPU训练模型、TPU训练模型）\n",
    "\n",
    "* 模型的部署（tensorflow serving部署模型、使用spark(scala)调用tensorflow模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "# 6-1,构建模型的3种方法\n",
    "\n",
    "可以使用以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\n",
    "\n",
    "对于顺序结构的模型，优先使用Sequential方法构建。\n",
    "\n",
    "如果模型有多输入或者多输出，或者模型需要共享权重，或者模型具有残差连接等非顺序结构，推荐使用函数式API进行创建。\n",
    "\n",
    "如果无特定必要，尽可能避免使用Model子类化的方式构建模型，这种方式提供了极大的灵活性，但也有更大的概率出错。\n",
    "\n",
    "下面以IMDB电影评论的分类问题为例，演示3种创建模型的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import *\n",
    "\n",
    "\n",
    "train_token_path = \"./data/imdb/train_token.csv\"\n",
    "test_token_path = \"./data/imdb/test_token.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "MAX_LEN = 200  # We will cut reviews after 200 words\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# 构建管道\n",
    "def parse_line(line):\n",
    "    t = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.reshape(tf.cast(tf.strings.to_number(t[0]),tf.int32),(-1,))\n",
    "    features = tf.cast(tf.strings.to_number(tf.strings.split(t[1],\" \")),tf.int32)\n",
    "    return (features,label)\n",
    "\n",
    "ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，Sequential按层顺序创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/Sequential模型结构.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "baselogger = callbacks.BaseLogger(stateful_metrics=[\"AUC\"])\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "        epochs = 6,callbacks=[baselogger,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-fit模型.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，函数式API创建任意结构模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = layers.Input(shape=[MAX_LEN])\n",
    "x  = layers.Embedding(MAX_WORDS,7)(inputs)\n",
    "\n",
    "branch1 = layers.SeparableConv1D(64,3,activation=\"relu\")(x)\n",
    "branch1 = layers.MaxPool1D(3)(branch1)\n",
    "branch1 = layers.SeparableConv1D(32,3,activation=\"relu\")(branch1)\n",
    "branch1 = layers.GlobalMaxPool1D()(branch1)\n",
    "\n",
    "branch2 = layers.SeparableConv1D(64,5,activation=\"relu\")(x)\n",
    "branch2 = layers.MaxPool1D(5)(branch2)\n",
    "branch2 = layers.SeparableConv1D(32,5,activation=\"relu\")(branch2)\n",
    "branch2 = layers.GlobalMaxPool1D()(branch2)\n",
    "\n",
    "branch3 = layers.SeparableConv1D(64,7,activation=\"relu\")(x)\n",
    "branch3 = layers.MaxPool1D(7)(branch3)\n",
    "branch3 = layers.SeparableConv1D(32,7,activation=\"relu\")(branch3)\n",
    "branch3 = layers.GlobalMaxPool1D()(branch3)\n",
    "\n",
    "concat = layers.Concatenate()([branch1,branch2,branch3])\n",
    "outputs = layers.Dense(1,activation = \"sigmoid\")(concat)\n",
    "\n",
    "model = models.Model(inputs = inputs,outputs = outputs)\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"model\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding (Embedding)           (None, 200, 7)       70000       input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_2 (SeparableCo (None, 196, 64)      547         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_4 (SeparableCo (None, 194, 64)      561         embedding[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_conv1d[0][0]           \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1D)  (None, 39, 64)       0           separable_conv1d_2[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1D)  (None, 27, 64)       0           separable_conv1d_4[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_3 (SeparableCo (None, 35, 32)       2400        max_pooling1d_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "separable_conv1d_5 (SeparableCo (None, 21, 32)       2528        max_pooling1d_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d (GlobalMax (None, 32)           0           separable_conv1d_1[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_1 (GlobalM (None, 32)           0           separable_conv1d_3[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_2 (GlobalM (None, 32)           0           separable_conv1d_5[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "concatenate (Concatenate)       (None, 96)           0           global_max_pooling1d[0][0]       \n",
    "                                                                 global_max_pooling1d_1[0][0]     \n",
    "                                                                 global_max_pooling1d_2[0][0]     \n",
    "__________________________________________________________________________________________________\n",
    "dense (Dense)                   (None, 1)            97          concatenate[0][0]                \n",
    "==================================================================================================\n",
    "Total params: 78,938\n",
    "Trainable params: 78,938\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/FunctionalAPI模型结构.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 6,callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/6\n",
    "1000/1000 [==============================] - 32s 32ms/step - loss: 0.5527 - accuracy: 0.6758 - AUC: 0.7731 - val_loss: 0.3646 - val_accuracy: 0.8426 - val_AUC: 0.9192\n",
    "Epoch 2/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.3024 - accuracy: 0.8737 - AUC: 0.9444 - val_loss: 0.3281 - val_accuracy: 0.8644 - val_AUC: 0.9350\n",
    "Epoch 3/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.2158 - accuracy: 0.9159 - AUC: 0.9715 - val_loss: 0.3461 - val_accuracy: 0.8666 - val_AUC: 0.9363\n",
    "Epoch 4/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.1492 - accuracy: 0.9464 - AUC: 0.9859 - val_loss: 0.4017 - val_accuracy: 0.8568 - val_AUC: 0.9311\n",
    "Epoch 5/6\n",
    "1000/1000 [==============================] - 24s 24ms/step - loss: 0.0944 - accuracy: 0.9696 - AUC: 0.9939 - val_loss: 0.4998 - val_accuracy: 0.8550 - val_AUC: 0.9233\n",
    "Epoch 6/6\n",
    "1000/1000 [==============================] - 26s 26ms/step - loss: 0.0526 - accuracy: 0.9865 - AUC: 0.9977 - val_loss: 0.6463 - val_accuracy: 0.8462 - val_AUC: 0.9138\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-2-train.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，Model子类化创建自定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先自定义一个残差模块，为自定义Layer\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.conv1 = layers.Conv1D(filters=64,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv2 = layers.Conv1D(filters=32,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv3 = layers.Conv1D(filters=input_shape[-1],\n",
    "                                   kernel_size=self.kernel_size,activation = \"relu\",padding=\"same\")\n",
    "        self.maxpool = layers.MaxPool1D(2)\n",
    "        super(ResBlock,self).build(input_shape) # 相当于设置self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = layers.Add()([inputs,x])\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "    #如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。\n",
    "    def get_config(self):  \n",
    "        config = super(ResBlock, self).get_config()\n",
    "        config.update({'kernel_size': self.kernel_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试ResBlock\n",
    "resblock = ResBlock(kernel_size = 3)\n",
    "resblock.build(input_shape = (None,200,7))\n",
    "resblock.compute_output_shape(input_shape=(None,200,7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([None, 100, 7])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型，实际上也可以使用Sequential或者FunctionalAPI\n",
    "\n",
    "class ImdbModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ImdbModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7)\n",
    "        self.block1 = ResBlock(7)\n",
    "        self.block2 = ResBlock(5)\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(ImdbModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = ImdbModel()\n",
    "model.build(input_shape =(None,200))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"imdb_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        multiple                  70000     \n",
    "_________________________________________________________________\n",
    "res_block (ResBlock)         multiple                  19143     \n",
    "_________________________________________________________________\n",
    "res_block_1 (ResBlock)       multiple                  13703     \n",
    "_________________________________________________________________\n",
    "dense (Dense)                multiple                  351       \n",
    "=================================================================\n",
    "Total params: 103,197\n",
    "Trainable params: 103,197\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/Model子类化模型结构.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "logdir = \"./tflogs/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "                    epochs = 6,callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/6\n",
    "1000/1000 [==============================] - 47s 47ms/step - loss: 0.5629 - accuracy: 0.6618 - AUC: 0.7548 - val_loss: 0.3422 - val_accuracy: 0.8510 - val_AUC: 0.9286\n",
    "Epoch 2/6\n",
    "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2648 - accuracy: 0.8903 - AUC: 0.9576 - val_loss: 0.3276 - val_accuracy: 0.8650 - val_AUC: 0.9410\n",
    "Epoch 3/6\n",
    "1000/1000 [==============================] - 42s 42ms/step - loss: 0.1573 - accuracy: 0.9439 - AUC: 0.9846 - val_loss: 0.3861 - val_accuracy: 0.8682 - val_AUC: 0.9390\n",
    "Epoch 4/6\n",
    "1000/1000 [==============================] - 42s 42ms/step - loss: 0.0849 - accuracy: 0.9706 - AUC: 0.9950 - val_loss: 0.5324 - val_accuracy: 0.8616 - val_AUC: 0.9292\n",
    "Epoch 5/6\n",
    "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0393 - accuracy: 0.9876 - AUC: 0.9986 - val_loss: 0.7693 - val_accuracy: 0.8566 - val_AUC: 0.9132\n",
    "Epoch 6/6\n",
    "1000/1000 [==============================] - 44s 44ms/step - loss: 0.0222 - accuracy: 0.9926 - AUC: 0.9994 - val_loss: 0.9328 - val_accuracy: 0.8584 - val_AUC: 0.9052\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/6-1-3-fit模型.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-2,训练模型的3种方法\n",
    "\n",
    "模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。\n",
    "\n",
    "注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，内置fit方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。\n",
    "\n",
    "并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "281/281 [==============================] - 11s 37ms/step - loss: 2.0231 - sparse_categorical_accuracy: 0.4636 - sparse_top_k_categorical_accuracy: 0.7450 - val_loss: 1.7346 - val_sparse_categorical_accuracy: 0.5534 - val_sparse_top_k_categorical_accuracy: 0.7560\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 9s 31ms/step - loss: 1.5079 - sparse_categorical_accuracy: 0.6091 - sparse_top_k_categorical_accuracy: 0.7901 - val_loss: 1.5475 - val_sparse_categorical_accuracy: 0.6109 - val_sparse_top_k_categorical_accuracy: 0.7792\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 1.2204 - sparse_categorical_accuracy: 0.6823 - sparse_top_k_categorical_accuracy: 0.8448 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.6367 - val_sparse_top_k_categorical_accuracy: 0.8001\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 0.9382 - sparse_categorical_accuracy: 0.7543 - sparse_top_k_categorical_accuracy: 0.9075 - val_loss: 1.6780 - val_sparse_categorical_accuracy: 0.6398 - val_sparse_top_k_categorical_accuracy: 0.8032\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 10s 34ms/step - loss: 0.6791 - sparse_categorical_accuracy: 0.8255 - sparse_top_k_categorical_accuracy: 0.9513 - val_loss: 1.9426 - val_sparse_categorical_accuracy: 0.6376 - val_sparse_top_k_categorical_accuracy: 0.7956\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 9s 33ms/step - loss: 0.5063 - sparse_categorical_accuracy: 0.8762 - sparse_top_k_categorical_accuracy: 0.9716 - val_loss: 2.2141 - val_sparse_categorical_accuracy: 0.6291 - val_sparse_top_k_categorical_accuracy: 0.7947\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 10s 37ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.9050 - sparse_top_k_categorical_accuracy: 0.9817 - val_loss: 2.4126 - val_sparse_categorical_accuracy: 0.6264 - val_sparse_top_k_categorical_accuracy: 0.7947\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 10s 35ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.9205 - sparse_top_k_categorical_accuracy: 0.9881 - val_loss: 2.5366 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7974\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 10s 36ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.9299 - sparse_top_k_categorical_accuracy: 0.9909 - val_loss: 2.6564 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 9s 30ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.9334 - sparse_top_k_categorical_accuracy: 0.9947 - val_loss: 2.7365 - val_sparse_categorical_accuracy: 0.6220 - val_sparse_top_k_categorical_accuracy: 0.8005\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，内置train_on_batch方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:09:19\n",
    "epoch =  1\n",
    "train: {'loss': 0.82411176, 'sparse_categorical_accuracy': 0.77272725, 'sparse_top_k_categorical_accuracy': 0.8636364}\n",
    "valid: {'loss': 1.9265995, 'sparse_categorical_accuracy': 0.5743544, 'sparse_top_k_categorical_accuracy': 0.75779164}\n",
    "\n",
    "================================================================================13:09:27\n",
    "epoch =  2\n",
    "train: {'loss': 0.6006621, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}\n",
    "valid: {'loss': 1.844159, 'sparse_categorical_accuracy': 0.6126447, 'sparse_top_k_categorical_accuracy': 0.7920748}\n",
    "\n",
    "================================================================================13:09:35\n",
    "epoch =  3\n",
    "train: {'loss': 0.36935613, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}\n",
    "valid: {'loss': 2.163433, 'sparse_categorical_accuracy': 0.63312554, 'sparse_top_k_categorical_accuracy': 0.8045414}\n",
    "\n",
    "================================================================================13:09:42\n",
    "epoch =  4\n",
    "train: {'loss': 0.2304088, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 2.8911984, 'sparse_categorical_accuracy': 0.6344613, 'sparse_top_k_categorical_accuracy': 0.7978629}\n",
    "\n",
    "Lowering optimizer Learning Rate...\n",
    "\n",
    "\n",
    "================================================================================13:09:51\n",
    "epoch =  5\n",
    "train: {'loss': 0.111194365, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 3.6431572, 'sparse_categorical_accuracy': 0.6295637, 'sparse_top_k_categorical_accuracy': 0.7978629}\n",
    "\n",
    "================================================================================13:09:59\n",
    "epoch =  6\n",
    "train: {'loss': 0.07741702, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.074161, 'sparse_categorical_accuracy': 0.6255565, 'sparse_top_k_categorical_accuracy': 0.794301}\n",
    "\n",
    "================================================================================13:10:07\n",
    "epoch =  7\n",
    "train: {'loss': 0.056113098, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.4461513, 'sparse_categorical_accuracy': 0.6273375, 'sparse_top_k_categorical_accuracy': 0.79652715}\n",
    "\n",
    "================================================================================13:10:17\n",
    "epoch =  8\n",
    "train: {'loss': 0.043448802, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 4.7687583, 'sparse_categorical_accuracy': 0.6224399, 'sparse_top_k_categorical_accuracy': 0.79741764}\n",
    "\n",
    "================================================================================13:10:26\n",
    "epoch =  9\n",
    "train: {'loss': 0.035002146, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 5.130505, 'sparse_categorical_accuracy': 0.6175423, 'sparse_top_k_categorical_accuracy': 0.794301}\n",
    "\n",
    "================================================================================13:10:34\n",
    "epoch =  10\n",
    "train: {'loss': 0.028303564, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
    "valid: {'loss': 5.4559293, 'sparse_categorical_accuracy': 0.6148709, 'sparse_top_k_categorical_accuracy': 0.7947462}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，自定义训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================13:12:03\n",
    "Epoch=1,Loss:2.02051544,Accuracy:0.460253835,Valid Loss:1.75700927,Valid Accuracy:0.536954582\n",
    "\n",
    "================================================================================13:12:09\n",
    "Epoch=2,Loss:1.510795,Accuracy:0.610665798,Valid Loss:1.55349839,Valid Accuracy:0.616206586\n",
    "\n",
    "================================================================================13:12:17\n",
    "Epoch=3,Loss:1.19221532,Accuracy:0.696170092,Valid Loss:1.52315605,Valid Accuracy:0.651380241\n",
    "\n",
    "================================================================================13:12:23\n",
    "Epoch=4,Loss:0.90101546,Accuracy:0.766310394,Valid Loss:1.68327653,Valid Accuracy:0.648263574\n",
    "\n",
    "================================================================================13:12:30\n",
    "Epoch=5,Loss:0.655430496,Accuracy:0.831329346,Valid Loss:1.90872383,Valid Accuracy:0.641139805\n",
    "\n",
    "================================================================================13:12:37\n",
    "Epoch=6,Loss:0.492730737,Accuracy:0.877866864,Valid Loss:2.09966016,Valid Accuracy:0.63223511\n",
    "\n",
    "================================================================================13:12:44\n",
    "Epoch=7,Loss:0.391238362,Accuracy:0.904030263,Valid Loss:2.27431226,Valid Accuracy:0.625111282\n",
    "\n",
    "================================================================================13:12:51\n",
    "Epoch=8,Loss:0.327761739,Accuracy:0.922066331,Valid Loss:2.42568827,Valid Accuracy:0.617542326\n",
    "\n",
    "================================================================================13:12:58\n",
    "Epoch=9,Loss:0.285573095,Accuracy:0.930527747,Valid Loss:2.55942106,Valid Accuracy:0.612644672\n",
    "\n",
    "================================================================================13:13:05\n",
    "Epoch=10,Loss:0.255482465,Accuracy:0.936094403,Valid Loss:2.67789412,Valid Accuracy:0.612199485\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-3,使用单GPU训练模型\n",
    "\n",
    "深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。\n",
    "\n",
    "训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。\n",
    "\n",
    "当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。\n",
    "\n",
    "当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。\n",
    "\n",
    "详见《用GPU加速Keras模型——Colab免费GPU使用攻略》\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/68509398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。\n",
    "\n",
    "但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），我们通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Colab笔记本中：修改->笔记本设置->硬件加速器 中选择 GPU\n",
    "\n",
    "注：以下代码只能在Colab 上才能正确执行。\n",
    "\n",
    "可通过以下colab链接测试效果《tf_单GPU》：\n",
    "\n",
    "https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import * \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，GPU设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较GPU和CPU的计算速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printbar()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n",
    "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n",
    "    c = a@b\n",
    "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\n",
    "printbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:37:01\n",
    "2.24953778e+11\n",
    "================================================================================17:37:01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printbar()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n",
    "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n",
    "    c = a@b\n",
    "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\n",
    "printbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:37:34\n",
    "2.24953795e+11\n",
    "================================================================================17:37:40\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================17:13:26\n",
    "Epoch=1,Loss:1.96735072,Accuracy:0.489200622,Valid Loss:1.64124215,Valid Accuracy:0.582813919\n",
    "\n",
    "================================================================================17:13:28\n",
    "Epoch=2,Loss:1.4640888,Accuracy:0.624805152,Valid Loss:1.5559175,Valid Accuracy:0.607747078\n",
    "\n",
    "================================================================================17:13:30\n",
    "Epoch=3,Loss:1.20681274,Accuracy:0.68581605,Valid Loss:1.58494771,Valid Accuracy:0.622439921\n",
    "\n",
    "================================================================================17:13:31\n",
    "Epoch=4,Loss:0.937500894,Accuracy:0.75361836,Valid Loss:1.77466083,Valid Accuracy:0.621994674\n",
    "\n",
    "================================================================================17:13:33\n",
    "Epoch=5,Loss:0.693960547,Accuracy:0.822199941,Valid Loss:2.00267363,Valid Accuracy:0.6197685\n",
    "\n",
    "================================================================================17:13:35\n",
    "Epoch=6,Loss:0.519614,Accuracy:0.870296121,Valid Loss:2.23463202,Valid Accuracy:0.613980412\n",
    "\n",
    "================================================================================17:13:37\n",
    "Epoch=7,Loss:0.408562034,Accuracy:0.901246965,Valid Loss:2.46969271,Valid Accuracy:0.612199485\n",
    "\n",
    "================================================================================17:13:39\n",
    "Epoch=8,Loss:0.339028627,Accuracy:0.920062363,Valid Loss:2.68585229,Valid Accuracy:0.615316093\n",
    "\n",
    "================================================================================17:13:41\n",
    "Epoch=9,Loss:0.293798745,Accuracy:0.92930305,Valid Loss:2.88995624,Valid Accuracy:0.613535166\n",
    "\n",
    "================================================================================17:13:43\n",
    "Epoch=10,Loss:0.263130337,Accuracy:0.936651051,Valid Loss:3.09705234,Valid Accuracy:0.612644672\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-4,使用多GPU训练模型\n",
    "\n",
    "如果使用多GPU训练模型，推荐使用内置fit方法，较为方便，仅需添加2行代码。\n",
    "\n",
    "在Colab笔记本中：修改->笔记本设置->硬件加速器 中选择 GPU\n",
    "\n",
    "注：以下代码只能在Colab 上才能正确执行。\n",
    "\n",
    "可通过以下colab链接测试效果《tf_多GPU》：\n",
    "\n",
    "https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MirroredStrategy过程简介：\n",
    "\n",
    "* 训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；\n",
    "* 每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；\n",
    "* N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；\n",
    "* 使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；\n",
    "* 使用梯度求和的结果更新本地变量（镜像变量）；\n",
    "* 当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此处在colab上使用1个GPU模拟出两个逻辑GPU进行多GPU训练\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 设置两个逻辑GPU模拟多GPU训练\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加以下两行代码\n",
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "with strategy.scope(): \n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    \n",
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
    "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
    "281/281 [==============================] - 15s 53ms/step - loss: 2.0270 - sparse_categorical_accuracy: 0.4653 - sparse_top_k_categorical_accuracy: 0.7481 - val_loss: 1.7517 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7578\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 1.5206 - sparse_categorical_accuracy: 0.6045 - sparse_top_k_categorical_accuracy: 0.7938 - val_loss: 1.5715 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 1.2178 - sparse_categorical_accuracy: 0.6843 - sparse_top_k_categorical_accuracy: 0.8547 - val_loss: 1.5232 - val_sparse_categorical_accuracy: 0.6327 - val_sparse_top_k_categorical_accuracy: 0.8112\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 4s 13ms/step - loss: 0.9127 - sparse_categorical_accuracy: 0.7648 - sparse_top_k_categorical_accuracy: 0.9113 - val_loss: 1.6527 - val_sparse_categorical_accuracy: 0.6296 - val_sparse_top_k_categorical_accuracy: 0.8201\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.6606 - sparse_categorical_accuracy: 0.8321 - sparse_top_k_categorical_accuracy: 0.9525 - val_loss: 1.8791 - val_sparse_categorical_accuracy: 0.6158 - val_sparse_top_k_categorical_accuracy: 0.8219\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.4919 - sparse_categorical_accuracy: 0.8799 - sparse_top_k_categorical_accuracy: 0.9725 - val_loss: 2.1282 - val_sparse_categorical_accuracy: 0.6037 - val_sparse_top_k_categorical_accuracy: 0.8112\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.3947 - sparse_categorical_accuracy: 0.9051 - sparse_top_k_categorical_accuracy: 0.9814 - val_loss: 2.3033 - val_sparse_categorical_accuracy: 0.6046 - val_sparse_top_k_categorical_accuracy: 0.8094\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.3335 - sparse_categorical_accuracy: 0.9207 - sparse_top_k_categorical_accuracy: 0.9863 - val_loss: 2.4255 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.8099\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.9304 - sparse_top_k_categorical_accuracy: 0.9911 - val_loss: 2.5571 - val_sparse_categorical_accuracy: 0.6020 - val_sparse_top_k_categorical_accuracy: 0.8126\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 4s 14ms/step - loss: 0.2617 - sparse_categorical_accuracy: 0.9342 - sparse_top_k_categorical_accuracy: 0.9937 - val_loss: 2.6700 - val_sparse_categorical_accuracy: 0.6077 - val_sparse_top_k_categorical_accuracy: 0.8148\n",
    "CPU times: user 1min 2s, sys: 8.59 s, total: 1min 10s\n",
    "Wall time: 58.5 s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)\n",
    "\n",
    "# 6-5,使用TPU训练模型\n",
    "\n",
    "如果想尝试使用Google Colab上的TPU来训练模型，也是非常方便，仅需添加6行代码。\n",
    "\n",
    "在Colab笔记本中：修改->笔记本设置->硬件加速器 中选择 TPU\n",
    "\n",
    "注：以下代码只能在Colab 上才能正确执行。\n",
    "\n",
    "可通过以下colab链接测试效果《tf_TPU》：\n",
    "\n",
    "https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加以下6行代码\n",
    "import os\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
    "WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
    "INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470\n",
    "INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470\n",
    "INFO:tensorflow:Clearing out eager caches\n",
    "INFO:tensorflow:Clearing out eager caches\n",
    "INFO:tensorflow:Finished initializing TPU system.\n",
    "INFO:tensorflow:Finished initializing TPU system.\n",
    "INFO:tensorflow:Found TPU system:\n",
    "INFO:tensorflow:Found TPU system:\n",
    "INFO:tensorflow:*** Num TPU Cores: 8\n",
    "INFO:tensorflow:*** Num TPU Cores: 8\n",
    "INFO:tensorflow:*** Num TPU Workers: 1\n",
    "INFO:tensorflow:*** Num TPU Workers: 1\n",
    "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
    "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 300, 7)            216874    \n",
    "_________________________________________________________________\n",
    "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 2336)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 46)                107502    \n",
    "=================================================================\n",
    "Total params: 332,856\n",
    "Trainable params: 332,856\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train for 281 steps, validate for 71 steps\n",
    "Epoch 1/10\n",
    "281/281 [==============================] - 12s 43ms/step - loss: 3.4466 - sparse_categorical_accuracy: 0.4332 - sparse_top_k_categorical_accuracy: 0.7180 - val_loss: 3.3179 - val_sparse_categorical_accuracy: 0.5352 - val_sparse_top_k_categorical_accuracy: 0.7195\n",
    "Epoch 2/10\n",
    "281/281 [==============================] - 6s 20ms/step - loss: 3.3251 - sparse_categorical_accuracy: 0.5405 - sparse_top_k_categorical_accuracy: 0.7302 - val_loss: 3.3082 - val_sparse_categorical_accuracy: 0.5463 - val_sparse_top_k_categorical_accuracy: 0.7235\n",
    "Epoch 3/10\n",
    "281/281 [==============================] - 6s 20ms/step - loss: 3.2961 - sparse_categorical_accuracy: 0.5729 - sparse_top_k_categorical_accuracy: 0.7280 - val_loss: 3.3026 - val_sparse_categorical_accuracy: 0.5499 - val_sparse_top_k_categorical_accuracy: 0.7217\n",
    "Epoch 4/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2751 - sparse_categorical_accuracy: 0.5924 - sparse_top_k_categorical_accuracy: 0.7276 - val_loss: 3.2957 - val_sparse_categorical_accuracy: 0.5543 - val_sparse_top_k_categorical_accuracy: 0.7217\n",
    "Epoch 5/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2655 - sparse_categorical_accuracy: 0.6008 - sparse_top_k_categorical_accuracy: 0.7290 - val_loss: 3.3022 - val_sparse_categorical_accuracy: 0.5490 - val_sparse_top_k_categorical_accuracy: 0.7231\n",
    "Epoch 6/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2616 - sparse_categorical_accuracy: 0.6041 - sparse_top_k_categorical_accuracy: 0.7295 - val_loss: 3.3015 - val_sparse_categorical_accuracy: 0.5503 - val_sparse_top_k_categorical_accuracy: 0.7235\n",
    "Epoch 7/10\n",
    "281/281 [==============================] - 6s 21ms/step - loss: 3.2595 - sparse_categorical_accuracy: 0.6059 - sparse_top_k_categorical_accuracy: 0.7322 - val_loss: 3.3064 - val_sparse_categorical_accuracy: 0.5454 - val_sparse_top_k_categorical_accuracy: 0.7266\n",
    "Epoch 8/10\n",
    "281/281 [==============================] - 6s 21ms/step - loss: 3.2591 - sparse_categorical_accuracy: 0.6063 - sparse_top_k_categorical_accuracy: 0.7327 - val_loss: 3.3025 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7231\n",
    "Epoch 9/10\n",
    "281/281 [==============================] - 5s 19ms/step - loss: 3.2588 - sparse_categorical_accuracy: 0.6062 - sparse_top_k_categorical_accuracy: 0.7332 - val_loss: 3.2992 - val_sparse_categorical_accuracy: 0.5521 - val_sparse_top_k_categorical_accuracy: 0.7257\n",
    "Epoch 10/10\n",
    "281/281 [==============================] - 5s 18ms/step - loss: 3.2577 - sparse_categorical_accuracy: 0.6073 - sparse_top_k_categorical_accuracy: 0.7363 - val_loss: 3.2981 - val_sparse_categorical_accuracy: 0.5516 - val_sparse_top_k_categorical_accuracy: 0.7306\n",
    "CPU times: user 18.9 s, sys: 3.86 s, total: 22.7 s\n",
    "Wall time: 1min 1s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-6,使用tensorflow-serving部署模型\n",
    "\n",
    "TensorFlow训练好的模型以tensorflow原生方式保存成protobuf文件后可以用许多方式部署运行。\n",
    "\n",
    "例如：通过 tensorflow-js 可以用javascrip脚本加载模型并在浏览器中运行模型。\n",
    "\n",
    "通过 tensorflow-lite 可以在移动和嵌入式设备上加载并运行TensorFlow模型。\n",
    "\n",
    "通过 tensorflow-serving 可以加载模型后提供网络接口API服务，通过任意编程语言发送网络请求都可以获取模型预测结果。\n",
    "\n",
    "通过 tensorFlow for Java接口，可以在Java或者spark(scala)中调用tensorflow模型进行预测。\n",
    "\n",
    "我们主要介绍tensorflow serving部署模型、使用spark(scala)调用tensorflow模型的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 〇，tensorflow serving模型部署概述\n",
    "使用 tensorflow serving 部署模型要完成以下步骤。\n",
    "\n",
    "* (1) 准备protobuf模型文件。\n",
    "\n",
    "* (2) 安装tensorflow serving。\n",
    "\n",
    "* (3) 启动tensorflow serving 服务。\n",
    "\n",
    "* (4) 向API服务发送请求，获取预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可通过以下colab链接测试效果《tf_serving》：\n",
    "https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备protobuf模型文件\n",
    "\n",
    "我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "## 样本数量\n",
    "n = 800\n",
    "\n",
    "## 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],\n",
    "    mean = 0.0,stddev= 2.0) # @表示矩阵乘法,增加正态扰动\n",
    "\n",
    "## 建立模型\n",
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape = (2,),name =\"inputs\") #设置输入名字为inputs\n",
    "outputs = layers.Dense(1, name = \"outputs\")(inputs) #设置输出名字为outputs\n",
    "linear = models.Model(inputs = inputs,outputs = outputs)\n",
    "linear.summary()\n",
    "\n",
    "## 使用fit方法进行训练\n",
    "linear.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 8,epochs = 100)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[1].kernel)\n",
    "tf.print(\"b = \",linear.layers[1].bias)\n",
    "\n",
    "## 将模型保存成pb格式文件\n",
    "export_path = \"./data/linear_model/\"\n",
    "version = \"1\"       #后续可以通过版本号进行模型版本迭代与管理\n",
    "linear.save(export_path+version, save_format=\"tf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看保存的模型文件\n",
    "!ls {export_path+version}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "assets\tsaved_model.pb\tvariables\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型文件相关信息\n",
    "!saved_model_cli show --dir {export_path+str(version)} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['__saved_model_init_op']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['__saved_model_init_op'] tensor_info:\n",
    "        dtype: DT_INVALID\n",
    "        shape: unknown_rank\n",
    "        name: NoOp\n",
    "  Method name is: \n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['inputs'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 2)\n",
    "        name: serving_default_inputs:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['outputs'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 1)\n",
    "        name: StatefulPartitionedCall:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "If using Keras pass *_constraint arguments to layers.\n",
    "\n",
    "Defined Functions:\n",
    "  Function Name: '__call__'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "\n",
    "  Function Name: '_default_save_signature'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "\n",
    "  Function Name: 'call_and_return_all_conditional_losses'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，安装 tensorflow serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装 tensorflow serving 有2种主要方法：通过Docker镜像安装，通过apt安装。\n",
    "\n",
    "通过Docker镜像安装是最简单，最直接的方法，推荐采用。\n",
    "\n",
    "Docker可以理解成一种容器，其上面可以给各种不同的程序提供独立的运行环境。\n",
    "\n",
    "一般业务中用到tensorflow的企业都会有运维同学通过Docker 搭建 tensorflow serving.\n",
    "\n",
    "无需算法工程师同学动手安装，以下安装过程仅供参考。\n",
    "\n",
    "不同操作系统机器上安装Docker的方法可以参照以下链接。\n",
    "\n",
    "Windows: https://www.runoob.com/docker/windows-docker-install.html\n",
    "\n",
    "MacOs: https://www.runoob.com/docker/macos-docker-install.html\n",
    "\n",
    "CentOS: https://www.runoob.com/docker/centos-docker-install.html\n",
    "\n",
    "安装Docker成功后，使用如下命令加载 tensorflow/serving 镜像到Docker中\n",
    "\n",
    "docker pull tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，启动 tensorflow serving 服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"/Users/.../data/linear_model/\" \\\n",
    "    -e MODEL_NAME=linear_model \\\n",
    "    tensorflow/serving & >server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，向API服务发送请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用任何编程语言的http功能发送请求，下面示范linux的 curl 命令发送请求，以及Python的requests库发送请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n",
    "    -X POST http://localhost:8501/v1/models/linear_model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"predictions\": [[3.06546211], [5.01313448]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,requests\n",
    "\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": [[1.0, 2.0], [5.0,7.0]]})\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/linear_model:predict', \n",
    "        data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)[\"predictions\"]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[3.06546211], [6.02843142]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-7,使用spark-scala调用tensorflow2.0训练好的模型\n",
    "\n",
    "本篇文章介绍在spark中调用训练好的tensorflow模型进行预测的方法。\n",
    "\n",
    "本文内容的学习需要一定的spark和scala基础。\n",
    "\n",
    "如果使用pyspark的话会比较简单，只需要在每个excutor上用Python加载模型分别预测就可以了。\n",
    "\n",
    "但工程上为了性能考虑，通常使用的是scala版本的spark。\n",
    "\n",
    "本篇文章我们通过TensorFlow for Java 在spark中调用训练好的tensorflow模型。\n",
    "\n",
    "利用spark的分布式计算能力，从而可以让训练好的tensorflow模型在成百上千的机器上分布式并行执行模型推断。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 〇，spark-scala调用tensorflow模型概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在spark(scala)中调用tensorflow模型进行预测需要完成以下几个步骤。\n",
    "\n",
    "（1）准备protobuf模型文件\n",
    "\n",
    "（2）创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖\n",
    "\n",
    "（3）在spark(scala)项目中driver端加载tensorflow模型调试成功\n",
    "\n",
    "（4）在spark(scala)项目中通过RDD在excutor上加载tensorflow模型调试成功\n",
    "\n",
    "（5） 在spark(scala)项目中通过DataFrame在excutor上加载tensorflow模型调试成功\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备protobuf模型文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "## 样本数量\n",
    "n = 800\n",
    "\n",
    "## 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动\n",
    "\n",
    "## 建立模型\n",
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape = (2,),name =\"inputs\") #设置输入名字为inputs\n",
    "outputs = layers.Dense(1, name = \"outputs\")(inputs) #设置输出名字为outputs\n",
    "linear = models.Model(inputs = inputs,outputs = outputs)\n",
    "linear.summary()\n",
    "\n",
    "## 使用fit方法进行训练\n",
    "linear.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 8,epochs = 100)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[1].kernel)\n",
    "tf.print(\"b = \",linear.layers[1].bias)\n",
    "\n",
    "## 将模型保存成pb格式文件\n",
    "export_path = \"./data/linear_model/\"\n",
    "version = \"1\"       #后续可以通过版本号进行模型版本迭代与管理\n",
    "linear.save(export_path+version, save_format=\"tf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {export_path+version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型文件相关信息\n",
    "!saved_model_cli show --dir {export_path+str(version)} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型文件信息中这些标红的部分都是后面有可能会用到的。\n",
    "\n",
    "![](./data/模型文件信息.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用maven管理项目，需要添加如下 jar包依赖\n",
    "\n",
    "```\n",
    "<!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow -->\n",
    "<dependency>\n",
    "    <groupId>org.tensorflow</groupId>\n",
    "    <artifactId>tensorflow</artifactId>\n",
    "    <version>1.15.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "也可以从下面网址中直接下载 org.tensorflow.tensorflow的jar包\n",
    "\n",
    "以及其依赖的org.tensorflow.libtensorflow 和 org.tensorflowlibtensorflow_jni的jar包 放到项目中。\n",
    "\n",
    "https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三， 在spark(scala)项目中driver端加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的示范代码在jupyter notebook中进行演示，需要安装toree以支持spark(scala)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "//注：load函数的第二个参数一般都是“serve”，可以从模型文件相关信息中找到\n",
    "\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//注：在java版本的tensorflow中还是类似tensorflow1.0中静态计算图的模式，需要建立Session, 指定feed的数据和fetch的结果, 然后 run.\n",
    "//注：如果有多个数据需要喂入，可以连续用用多个feed方法\n",
    "//注：输入必须是float类型\n",
    "\n",
    "val sess = bundle.session()\n",
    "val x = tf.Tensor.create(Array(Array(1.0f,2.0f),Array(2.0f,3.0f)))\n",
    "val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "         .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "y.copyTo(result)\n",
    "\n",
    "if(x != null) x.close()\n",
    "if(y != null) y.close()\n",
    "if(sess != null) sess.close()\n",
    "if(bundle != null) bundle.close()  \n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出如下：\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9878292))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfDriver.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，在spark(scala)项目中通过RDD在excutor上加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们通过广播机制将Driver端加载的TensorFlow模型传递到各个excutor上，并在excutor上分布式地调用模型进行推断。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"TfRDD\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "//在Driver端加载模型\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//利用广播将模型发送到excutor上\n",
    "val broads = sc.broadcast(bundle)\n",
    "\n",
    "//构造数据集\n",
    "val rdd_data = sc.makeRDD(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(6.0f,7.0f),Array(8.0f,3.0f)))\n",
    "\n",
    "//通过mapPartitions调用模型进行批量推断\n",
    "val rdd_result = rdd_data.mapPartitions(iter => {\n",
    "    \n",
    "    val arr = iter.toArray\n",
    "    val model = broads.value\n",
    "    val sess = model.session()\n",
    "    val x = tf.Tensor.create(arr)\n",
    "    val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "             .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "    //将预测结果拷贝到相同shape的Float类型的Array中\n",
    "    val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "    y.copyTo(result)\n",
    "    result.iterator\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "rdd_result.take(5)\n",
    "bundle.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出如下：\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9264367), Array(7.8607616), Array(15.974984))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfRDD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五， 在spark(scala)项目中通过DataFrame在excutor上加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了可以在Spark的RDD数据上调用tensorflow模型进行分布式推断，\n",
    "\n",
    "我们也可以在DataFrame数据上调用tensorflow模型进行分布式推断。\n",
    "\n",
    "主要思路是将推断方法注册成为一个sparkSQL函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "object TfDataFrame extends Serializable{\n",
    "    \n",
    "    \n",
    "    def main(args:Array[String]):Unit = {\n",
    "        \n",
    "        val spark = SparkSession\n",
    "        .builder()\n",
    "        .appName(\"TfDataFrame\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "        val sc = spark.sparkContext\n",
    "        \n",
    "        \n",
    "        import spark.implicits._\n",
    "\n",
    "        val bundle = tf.SavedModelBundle \n",
    "           .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "        val broads = sc.broadcast(bundle)\n",
    "        \n",
    "        //构造预测函数，并将其注册成sparkSQL的udf\n",
    "        val tfpredict = (features:WrappedArray[Float])  => {\n",
    "            val bund = broads.value\n",
    "            val sess = bund.session()\n",
    "            val x = tf.Tensor.create(Array(features.toArray))\n",
    "            val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "                     .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "            val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "            y.copyTo(result)\n",
    "            val y_pred = result(0)(0)\n",
    "            y_pred\n",
    "        }\n",
    "        spark.udf.register(\"tfpredict\",tfpredict)\n",
    "        \n",
    "        //构造DataFrame数据集，将features放到一列中\n",
    "        val dfdata = sc.parallelize(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(7.0f,8.0f))).toDF(\"features\")\n",
    "        dfdata.show \n",
    "        \n",
    "        //调用sparkSQL预测函数，增加一个新的列作为y_preds\n",
    "        val dfresult = dfdata.selectExpr(\"features\",\"tfpredict(features) as y_preds\")\n",
    "        dfresult.show \n",
    "        bundle.close\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "TfDataFrame.main(Array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+----------+\n",
    "|  features|\n",
    "+----------+\n",
    "|[1.0, 2.0]|\n",
    "|[3.0, 5.0]|\n",
    "|[7.0, 8.0]|\n",
    "+----------+\n",
    "\n",
    "+----------+---------+\n",
    "|  features|  y_preds|\n",
    "+----------+---------+\n",
    "|[1.0, 2.0]| 3.019596|\n",
    "|[3.0, 5.0]|3.9264367|\n",
    "|[7.0, 8.0]| 8.828995|\n",
    "+----------+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上我们分别在spark 的RDD数据结构和DataFrame数据结构上实现了调用一个tf.keras实现的线性回归模型进行分布式模型推断。\n",
    "\n",
    "在本例基础上稍作修改则可以用spark调用训练好的各种复杂的神经网络模型进行分布式模型推断。\n",
    "\n",
    "但实际上tensorflow并不仅仅适合实现神经网络，其底层的计算图语言可以表达各种数值计算过程。\n",
    "\n",
    "利用其丰富的低阶API，我们可以在tensorflow2.0上实现任意机器学习模型，\n",
    "\n",
    "结合tf.Module提供的便捷的封装功能，我们可以将训练好的任意机器学习模型导出成模型文件并在spark上分布式调用执行。\n",
    "\n",
    "这无疑为我们的工程应用提供了巨大的想象空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
