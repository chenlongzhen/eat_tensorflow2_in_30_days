{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to eat TensorFlow2 in 30 days ?ğŸ”¥ğŸ”¥\n",
    "\n",
    "Click here for [Chinese Versionï¼ˆä¸­æ–‡ç‰ˆï¼‰](#30å¤©åƒæ‰é‚£åª-tensorflow2)\n",
    "\n",
    "ğŸš€ github repository: https://github.com/lyhue1991/eat_tensorflow2_in_30_days\n",
    "\n",
    "### 1. TensorFlow2 ğŸ or PytorchğŸ”¥\n",
    "\n",
    "Conclusion first: \n",
    "\n",
    "**For the engineers, priority goes to TensorFlow2.**\n",
    "\n",
    "**For the students and researchersï¼Œfirst choice should be Pytorch.**\n",
    "\n",
    "**The best way is to master both of them if having sufficient time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons:\n",
    "\n",
    "* 1. **Model implementation is the most important in the industry. Deployment supporting tensorflow models (not Pytorch) exclusively is the present situation in the majority of the Internet enterprises in China.** What's more, the industry prefers the models with higher availability; in most cases, they use well-validated modeling architectures with the minimized requirements of adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. **Fast iterative development and publication is the most important for the researchers since they need to test a lot of new models. Pytorch has advantages in accessing and debugging comparing with TensorFlow2.** Pytorch is most frequently used in academy since 2019 with a large amount of the cutting-edge results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3. Overall, TensorFlow2 and Pytorch are quite similar in programming nowadays, so mastering one helps learning the other. Mastering both framework provides you a lot more open-sourced models and helps you switching between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KerasğŸ and tf.keras ğŸ\n",
    "\n",
    "Conclusion first: \n",
    "\n",
    "**Keras will be discontinued in development after version 2.3.0, so use tf.keras.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high-level API for the deep learning frameworks. It help the users to define and training DL networks with a more intuitive way.\n",
    "\n",
    "The Keras libraries installed by pip implement this high-level API for the backends in tensorflow, theano, CNTK, etc.\n",
    "\n",
    "tf.keras is the high-level API just for Tensorflow, which is based on low-level APIs in Tensorflow.\n",
    "\n",
    "Most but not all of the functions in tf.keras are the same for those in Keras (which is compatible to many kinds of backend). tf.keras has a tighter combination to TensorFlow comparing to Keras.\n",
    "\n",
    "With the acquisition by Google, Keras will not update after version 2.3.0 , thus the users should use tf.keras from now on, instead of using Keras installed by pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What Should You Know Before Reading This Book ğŸ“–?\n",
    "\n",
    "**It is suggested that the readers have foundamental knowledges of machine/deep learning and experience of modeling using Keras or TensorFlow 1.0.**\n",
    "\n",
    "**For those who have zero experience of machine/deep learning, it is strongly suggested to refer to [\"Deep Learning with Python\"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_1?dchild=1&keywords=Deep+Learning+with+Python&qid=1586194568&sr=8-1) along with reading this book.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Deep Learning with Python\"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_1?dchild=1&keywords=Deep+Learning+with+Python&qid=1586194568&sr=8-1) is written by FranÃ§ois Chollet, the inventor of Keras. This book is based on Keras and has no machine learning related prerequisites to the reader.\n",
    "\n",
    "\"Deep Learning with Python\" is easy to understand as it uses various examples to demonstrate. **No mathematical equation is in this book since it focuses on cultivating the intuitive to the deep learning.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Writing Style ğŸ‰ of This Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a introduction reference book which is extremely friendly to human being. The lowest goal of the authors is to avoid giving up due to the difficulties, while \"Don't let the readers think\" is the highest target.**\n",
    "\n",
    "This book is mainly based on the official documents of TensorFlow together with its functions.\n",
    "\n",
    "However, the authors made a thorough restructuring and a lot optimizations on the demonstrations.\n",
    "\n",
    "It is different from the official documents, which is disordered and contains both tutorial and guidance with lack of systematic logic, that our book redesigns the content according to the difficulties, readers' searching habits, and the architecture of TensorFlow. We now make it progressive for TensorFlow studying with a clear path, and an easy access to the corresponding examples.\n",
    "\n",
    "In contrast to the verbose demonstrating code, the authors of this book try to minimize the length of the examples to make it easy for reading and implementation. What's more, most of the code cells can be used in your project instantaneously.\n",
    "\n",
    "**Given the level of difficulty as 9 for learning Tensorflow through official documents, it would be reduced to 3 if learning through this book.**\n",
    "\n",
    "This difference in difficulties could be demonstrated as the following figure:\n",
    "\n",
    "![](./data/30å¤©åƒæ‰é‚£ä¸ªTF2.0_en.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How to Learn With This Book â°\n",
    "\n",
    "**(1) Study Plan**\n",
    "\n",
    "The authors wrote this book using the spare time, especially the two-month unexpected \"holiday\" of COVID-19. Most readers should be able to completely master all the content within 30 days.\n",
    "\n",
    "Time required everyday would be between 30 minutes to 2 hours.\n",
    "\n",
    "This book could also be used as library examples to consult when implementing machine learning projects with TensorFlow2.\n",
    "\n",
    "**Click the blue captions to enter the corresponding chapter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date |Contents                                                       | Difficulties   | Est. Time | Update Status|\n",
    "|----:|:--------------------------------------------------------------|-----------:|----------:|-----:|\n",
    "|&nbsp;|[**Chapter 1: Modeling Procedure of TensorFlow**](./english/Chapter1.md)    |â­ï¸   |   0hour   |âœ…    |\n",
    "|Day 1 |  [1-1 Example: Modeling Procedure for Structured Data](./english/Chapter1-1.md)    | â­ï¸â­ï¸â­ï¸ |   1hour    |âœ…    |\n",
    "|Day 2 |[1-2 Example: Modeling Procedure for Images](./english/Chapter1-2.md)    | â­ï¸â­ï¸â­ï¸â­ï¸  |   2hours    |âœ…    |\n",
    "|Day 3 |  [1-3 Example: Modeling Procedure for Texts](./english/Chapter1-3.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hours    |âœ…    |\n",
    "|Day 4 |  [1-4 Example: Modeling Procedure for Temporal Sequences](./english/Chapter1-4.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hours    |âœ…    |\n",
    "|&nbsp;    |[**Chapter 2: Key Concepts of TensorFlow**](./english/Chapter2.md)  | â­ï¸  |  0hour |âœ…  |\n",
    "|Day 5 |  [2-1 Data Structure of Tensor](./english/Chapter2-1.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|Day 6 |  [2-2 Three Types of Graph](./english/Chapter2-2.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hours    |âœ…    |\n",
    "|Day 7 |  [2-3 Automatic Differentiate](./english/Chapter2-3.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|&nbsp; |[**Chapter 3: Hierarchy of TensorFlow**](./english/Chapter3.md) |   â­ï¸  |  0hour   |âœ…  |\n",
    "|Day 8 |  [3-1 Low-level API: Demonstration](./english/Chapter3-1.md)   | â­ï¸â­ï¸â­ï¸â­ï¸ |   1hour    |âœ…   |\n",
    "|Day 9 |  [3-2 Mid-level API: Demonstration](./english/Chapter3-2.md)   | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|Day 10 |  [3-3 High-level API: Demonstration](./english/Chapter3-3.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|&nbsp; |[**Chapter 4: Low-level API in TensorFlow**](./english/Chapter4.md) |â­ï¸    | 0hour|âœ…  |\n",
    "|Day 11|  [4-1 Structural Operations of the Tensor](./english/Chapter4-1.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hours    |âœ…   |\n",
    "|Day 12|  [4-2 Mathematical Operations of the Tensor](./english/Chapter4-2.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|Day 13|  [4-3 Rules of Using the AutoGraph](./english/Chapter4-3.md)| â­ï¸â­ï¸â­ï¸   |   0.5hour    | âœ…  |\n",
    "|Day 14|  [4-4 Mechanisms of the AutoGraph](./english/Chapter4-4.md)    | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hours    |âœ…  |\n",
    "|Day 15|  [4-5 AutoGraph and tf.Module](./english/Chapter4-5.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|&nbsp; |[**Chapter 5: Mid-level API in TensorFlow**](./english/Chapter5.md) |  â­ï¸  | 0hour|âœ… |\n",
    "|Day 16|  [5-1 Dataset](./english/Chapter5-1.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hours    |âœ…  |\n",
    "|Day 17|  [5-2 feature_column](./english/Chapter5-2.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|Day 18|  [5-3 activation](./english/Chapter5-3.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|Day 19|  [5-4 layers](./english/Chapter5-4.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|Day 20|  [5-5 losses](./english/Chapter5-5.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|Day 21|  [5-6 metrics](./english/Chapter5-6.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|Day 22|  [5-7 optimizers](./english/Chapter5-7.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|Day 23|  [5-8 callbacks](./english/Chapter5-8.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|&nbsp; |[**Chapter 6: High-level API in TensorFlow**](./english/Chapter6.md)|    â­ï¸ | 0hour|âœ…  |\n",
    "|Day 24|  [6-1 Three Ways of Modeling](./english/Chapter6-1.md)   | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ… |\n",
    "|Day 25|  [6-2 Three Ways of Training](./english/Chapter6-2.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|Day 26|  [6-3 Model Training Using Single GPU](./english/Chapter6-3.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|Day 27|  [6-4 Model Training Using Multiple GPUs](./english/Chapter6-4.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|Day 28|  [6-5 Model Training Using TPU](./english/Chapter6-5.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|Day 29| [6-6 Model Deploying Using tensorflow-serving](./english/Chapter6-6.md) | â­ï¸â­ï¸â­ï¸â­ï¸| 1hour |âœ…   |\n",
    "|Day 30| [6-7 Call Tensorflow Model Using spark-scala](./english/Chapter6-7.md) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸|2hours|âœ…  |\n",
    "|&nbsp;| [Epilogue: A Story Between a Foodie and Cuisine](./english/Epilogue.md) | â­ï¸|0hour|âœ…  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Software environment for studying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the source codes are tested in jupyter. It is suggested to clone the repository to local machine and run them in jupyter for an interactive learning experience.\n",
    "\n",
    "The authors would suggest to install jupytext that converts markdown files into ipynb, so the readers would be able to open markdown files in jupyter directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the readers in mainland China, using gitee will allow cloning with a faster speed\n",
    "#!git clone https://gitee.com/Python_Ai_Road/eat_tensorflow2_in_30_days\n",
    "\n",
    "#It is suggested to install jupytext that converts and run markdown files as ipynb.\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext\n",
    "    \n",
    "#It is also suggested to install the latest version of TensorFlow to test the demonstrating code in this book\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Note: all the codes are tested under TensorFlow 2.1\n",
    "tf.print(\"tensorflow version:\",tf.__version__)\n",
    "\n",
    "a = tf.constant(\"hello\")\n",
    "b = tf.constant(\"tensorflow2\")\n",
    "c = tf.strings.join([a,b],\" \")\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow version: 2.1.0\n",
    "hello tensorflow2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Contact and support the author ğŸˆğŸˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you find this book helpful and want to support the author, please give a star â­ï¸ to this repository and don't forget to share it to your friends ğŸ˜Š** \n",
    "\n",
    "Please leave comments in the WeChat official account \"Pythonä¸ç®—æ³•ä¹‹ç¾\" (Elegance of Python and Algorithms) if you want to communicate with the author about the content. The author will try best to reply given the limited time available.\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30å¤©åƒæ‰é‚£åª TensorFlow2\n",
    "\n",
    "ğŸ“š gitbookç”µå­ä¹¦åœ°å€ï¼š https://lyhue1991.github.io/eat_tensorflow2_in_30_days\n",
    "\n",
    "ğŸš€ githubé¡¹ç›®åœ°å€ï¼šhttps://github.com/lyhue1991/eat_tensorflow2_in_30_days\n",
    "\n",
    "ğŸ³ kesciä¸“æ åœ°å€ï¼šhttps://www.kesci.com/home/column/5d8ef3c3037db3002d3aa3a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒTensorFlow2 ğŸ or PytorchğŸ”¥\n",
    "\n",
    "å…ˆè¯´ç»“è®º:\n",
    "\n",
    "**å¦‚æœæ˜¯å·¥ç¨‹å¸ˆï¼Œåº”è¯¥ä¼˜å…ˆé€‰TensorFlow2.**\n",
    "\n",
    "**å¦‚æœæ˜¯å­¦ç”Ÿæˆ–è€…ç ”ç©¶äººå‘˜ï¼Œåº”è¯¥ä¼˜å…ˆé€‰æ‹©Pytorch.**\n",
    "\n",
    "**å¦‚æœæ—¶é—´è¶³å¤Ÿï¼Œæœ€å¥½TensorFlow2å’ŒPytorchéƒ½è¦å­¦ä¹ æŒæ¡ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç†ç”±å¦‚ä¸‹ï¼š\n",
    "\n",
    "* 1ï¼Œ**åœ¨å·¥ä¸šç•Œæœ€é‡è¦çš„æ˜¯æ¨¡å‹è½åœ°ï¼Œç›®å‰å›½å†…çš„å¤§éƒ¨åˆ†äº’è”ç½‘ä¼ä¸šåªæ”¯æŒTensorFlowæ¨¡å‹çš„åœ¨çº¿éƒ¨ç½²ï¼Œä¸æ”¯æŒPytorchã€‚** å¹¶ä¸”å·¥ä¸šç•Œæ›´åŠ æ³¨é‡çš„æ˜¯æ¨¡å‹çš„é«˜å¯ç”¨æ€§ï¼Œè®¸å¤šæ—¶å€™ä½¿ç”¨çš„éƒ½æ˜¯æˆç†Ÿçš„æ¨¡å‹æ¶æ„ï¼Œè°ƒè¯•éœ€æ±‚å¹¶ä¸å¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2ï¼Œ**ç ”ç©¶äººå‘˜æœ€é‡è¦çš„æ˜¯å¿«é€Ÿè¿­ä»£å‘è¡¨æ–‡ç« ï¼Œéœ€è¦å°è¯•ä¸€äº›è¾ƒæ–°çš„æ¨¡å‹æ¶æ„ã€‚è€ŒPytorchåœ¨æ˜“ç”¨æ€§ä¸Šç›¸æ¯”TensorFlow2æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œæ›´åŠ æ–¹ä¾¿è°ƒè¯•ã€‚** å¹¶ä¸”åœ¨2019å¹´ä»¥æ¥åœ¨å­¦æœ¯ç•Œå é¢†äº†å¤§åŠå£æ±Ÿå±±ï¼Œèƒ½å¤Ÿæ‰¾åˆ°çš„ç›¸åº”æœ€æ–°ç ”ç©¶æˆæœæ›´å¤šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3ï¼ŒTensorFlow2å’ŒPytorchå®é™…ä¸Šæ•´ä½“é£æ ¼å·²ç»éå¸¸ç›¸ä¼¼äº†ï¼Œå­¦ä¼šäº†å…¶ä¸­ä¸€ä¸ªï¼Œå­¦ä¹ å¦å¤–ä¸€ä¸ªå°†æ¯”è¾ƒå®¹æ˜“ã€‚ä¸¤ç§æ¡†æ¶éƒ½æŒæ¡çš„è¯ï¼Œèƒ½å¤Ÿå‚è€ƒçš„å¼€æºæ¨¡å‹æ¡ˆä¾‹æ›´å¤šï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒï¼ŒKerasğŸ and  tf.keras ğŸ\n",
    "\n",
    "å…ˆè¯´ç»“è®ºï¼š\n",
    "\n",
    "**Kerasåº“åœ¨2.3.0ç‰ˆæœ¬åå°†ä¸å†æ›´æ–°ï¼Œç”¨æˆ·åº”è¯¥ä½¿ç”¨tf.kerasã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keraså¯ä»¥çœ‹æˆæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜é˜¶æ¥å£è§„èŒƒï¼Œå®ƒå¸®åŠ©ç”¨æˆ·ä»¥æ›´ç®€æ´çš„å½¢å¼å®šä¹‰å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ ç½‘ç»œã€‚\n",
    "\n",
    "ä½¿ç”¨pipå®‰è£…çš„Kerasåº“åŒæ—¶åœ¨tensorflow,theano,CNTKç­‰åç«¯åŸºç¡€ä¸Šè¿›è¡Œäº†è¿™ç§é«˜é˜¶æ¥å£è§„èŒƒçš„å®ç°ã€‚\n",
    "\n",
    "è€Œtf.kerasæ˜¯åœ¨TensorFlowä¸­ä»¥TensorFlowä½é˜¶APIä¸ºåŸºç¡€å®ç°çš„è¿™ç§é«˜é˜¶æ¥å£ï¼Œå®ƒæ˜¯Tensorflowçš„ä¸€ä¸ªå­æ¨¡å—ã€‚\n",
    "\n",
    "tf.kerasç»å¤§éƒ¨åˆ†åŠŸèƒ½å’Œå…¼å®¹å¤šç§åç«¯çš„Kerasåº“ç”¨æ³•å®Œå…¨ä¸€æ ·ï¼Œä½†å¹¶éå…¨éƒ¨ï¼Œå®ƒå’ŒTensorFlowä¹‹é—´çš„ç»“åˆæ›´ä¸ºç´§å¯†ã€‚\n",
    "\n",
    "éšç€è°·æ­Œå¯¹Kerasçš„æ”¶è´­ï¼ŒKerasåº“2.3.0ç‰ˆæœ¬åä¹Ÿå°†ä¸å†è¿›è¡Œæ›´æ–°ï¼Œç”¨æˆ·åº”å½“ä½¿ç”¨tf.kerasè€Œä¸æ˜¯ä½¿ç”¨pipå®‰è£…çš„Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œæœ¬ä¹¦ğŸ“–é¢å‘è¯»è€… ğŸ‘¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æœ¬ä¹¦å‡å®šè¯»è€…æœ‰ä¸€å®šçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œä½¿ç”¨è¿‡Kerasæˆ–è€…Tensorflow1.0æˆ–è€…Pytorchæ­å»ºè®­ç»ƒè¿‡æ¨¡å‹ã€‚**\n",
    "\n",
    "**å¯¹äºæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦ï¼Œå»ºè®®åœ¨å­¦ä¹ æœ¬ä¹¦æ—¶åŒæ­¥å‚è€ƒå­¦ä¹ ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦ã€‚**\n",
    "\n",
    "ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹è¿™æœ¬ä¹¦æ˜¯Kerasä¹‹çˆ¶Francois Cholletæ‰€è‘—ï¼Œè¯¥ä¹¦å‡å®šè¯»è€…æ— ä»»ä½•æœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œä»¥Kerasä¸ºå·¥å…·ï¼Œ\n",
    "\n",
    "ä½¿ç”¨ä¸°å¯Œçš„èŒƒä¾‹ç¤ºèŒƒæ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œè¯¥ä¹¦é€šä¿—æ˜“æ‡‚ï¼Œ**å…¨ä¹¦æ²¡æœ‰ä¸€ä¸ªæ•°å­¦å…¬å¼ï¼Œæ³¨é‡åŸ¹å…»è¯»è€…çš„æ·±åº¦å­¦ä¹ ç›´è§‰ã€‚**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å››ï¼Œæœ¬ä¹¦å†™ä½œé£æ ¼ ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æœ¬ä¹¦æ˜¯ä¸€æœ¬å¯¹äººç±»ç”¨æˆ·æå…¶å‹å–„çš„TensorFlow2.0å…¥é—¨å·¥å…·ä¹¦ï¼Œä¸åˆ»æ„æ¶å¿ƒè¯»è€…æ˜¯æœ¬ä¹¦çš„åº•é™è¦æ±‚ï¼ŒDon't let me thinkæ˜¯æœ¬ä¹¦çš„æœ€é«˜è¿½æ±‚ã€‚**\n",
    "\n",
    "æœ¬ä¹¦ä¸»è¦æ˜¯åœ¨å‚è€ƒTensorFlowå®˜æ–¹æ–‡æ¡£å’Œå‡½æ•°docæ–‡æ¡£åŸºç¡€ä¸Šæ•´ç†å†™æˆçš„ã€‚\n",
    "\n",
    "ä½†æœ¬ä¹¦åœ¨ç¯‡ç« ç»“æ„å’ŒèŒƒä¾‹é€‰å–ä¸Šåšäº†å¤§é‡çš„ä¼˜åŒ–ã€‚\n",
    "\n",
    "ä¸åŒäºå®˜æ–¹æ–‡æ¡£æ··ä¹±çš„ç¯‡ç« ç»“æ„ï¼Œæ—¢æœ‰æ•™ç¨‹åˆæœ‰æŒ‡å—ï¼Œç¼ºå°‘æ•´ä½“çš„ç¼–æ’é€»è¾‘ã€‚\n",
    "\n",
    "æœ¬ä¹¦æŒ‰ç…§å†…å®¹éš¾æ˜“ç¨‹åº¦ã€è¯»è€…æ£€ç´¢ä¹ æƒ¯å’ŒTensorFlowè‡ªèº«çš„å±‚æ¬¡ç»“æ„è®¾è®¡å†…å®¹ï¼Œå¾ªåºæ¸è¿›ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œæ–¹ä¾¿æŒ‰ç…§åŠŸèƒ½æŸ¥æ‰¾ç›¸åº”èŒƒä¾‹ã€‚\n",
    "\n",
    "ä¸åŒäºå®˜æ–¹æ–‡æ¡£å†—é•¿çš„èŒƒä¾‹ä»£ç ï¼Œæœ¬ä¹¦åœ¨èŒƒä¾‹è®¾è®¡ä¸Šå°½å¯èƒ½ç®€çº¦åŒ–å’Œç»“æ„åŒ–ï¼Œå¢å¼ºèŒƒä¾‹æ˜“è¯»æ€§å’Œé€šç”¨æ€§ï¼Œå¤§éƒ¨åˆ†ä»£ç ç‰‡æ®µåœ¨å®è·µä¸­å¯å³å–å³ç”¨ã€‚\n",
    "\n",
    "**å¦‚æœè¯´é€šè¿‡å­¦ä¹ TensorFlowå®˜æ–¹æ–‡æ¡£æŒæ¡TensorFlow2.0çš„éš¾åº¦å¤§æ¦‚æ˜¯9çš„è¯ï¼Œé‚£ä¹ˆé€šè¿‡å­¦ä¹ æœ¬ä¹¦æŒæ¡TensorFlow2.0çš„éš¾åº¦åº”è¯¥å¤§æ¦‚æ˜¯3.**\n",
    "\n",
    "è°¨ä»¥ä¸‹å›¾å¯¹æ¯”ä¸€ä¸‹TensorFlowå®˜æ–¹æ•™ç¨‹ä¸æœ¬æ•™ç¨‹çš„å·®å¼‚ã€‚\n",
    "\n",
    "![](./data/30å¤©åƒæ‰é‚£ä¸ªTF2.0.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº”ï¼Œæœ¬ä¹¦å­¦ä¹ æ–¹æ¡ˆ â°\n",
    "\n",
    "**1ï¼Œå­¦ä¹ è®¡åˆ’**\n",
    "\n",
    "æœ¬ä¹¦æ˜¯ä½œè€…åˆ©ç”¨å·¥ä½œä¹‹ä½™å’Œç–«æƒ…æ”¾å‡æœŸé—´å¤§æ¦‚2ä¸ªæœˆå†™æˆçš„ï¼Œå¤§éƒ¨åˆ†è¯»è€…åº”è¯¥åœ¨30å¤©å¯ä»¥å®Œå…¨å­¦ä¼šã€‚\n",
    "\n",
    "é¢„è®¡æ¯å¤©èŠ±è´¹çš„å­¦ä¹ æ—¶é—´åœ¨30åˆ†é’Ÿåˆ°2ä¸ªå°æ—¶ä¹‹é—´ã€‚\n",
    "\n",
    "å½“ç„¶ï¼Œæœ¬ä¹¦ä¹Ÿéå¸¸é€‚åˆä½œä¸ºTensorFlowçš„å·¥å…·æ‰‹å†Œåœ¨å·¥ç¨‹è½åœ°æ—¶ä½œä¸ºèŒƒä¾‹åº“å‚è€ƒã€‚\n",
    "\n",
    "**ç‚¹å‡»å­¦ä¹ å†…å®¹è“è‰²æ ‡é¢˜å³å¯è¿›å…¥è¯¥ç« èŠ‚ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|æ—¥æœŸ | å­¦ä¹ å†…å®¹                                                       | å†…å®¹éš¾åº¦   | é¢„è®¡å­¦ä¹ æ—¶é—´ | æ›´æ–°çŠ¶æ€|\n",
    "|----:|:--------------------------------------------------------------|-----------:|----------:|-----:|\n",
    "|&nbsp;|[**ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹**](./ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹.md)    |â­ï¸   |   0hour   |âœ…    |\n",
    "|day1 |  [1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸ |   1hour    |âœ…    |\n",
    "|day2 |[1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|day3 |  [1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|day4 |  [1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |\n",
    "|&nbsp;    |[**äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ**](./äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ.md)  | â­ï¸  |  0hour |âœ…  |\n",
    "|day5 |  [2-1,å¼ é‡æ•°æ®ç»“æ„](./2-1,å¼ é‡æ•°æ®ç»“æ„.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|day6 |  [2-2,ä¸‰ç§è®¡ç®—å›¾](./2-2,ä¸‰ç§è®¡ç®—å›¾.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…    |\n",
    "|day7 |  [2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶](./2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |\n",
    "|&nbsp; |[**ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„**](./ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„.md) |   â­ï¸  |  0hour   |âœ…  |\n",
    "|day8 |  [3-1,ä½é˜¶APIç¤ºèŒƒ](./3-1,ä½é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|day9 |  [3-2,ä¸­é˜¶APIç¤ºèŒƒ](./3-2,ä¸­é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸â­ï¸   |  1hour    |âœ…  |\n",
    "|day10 |  [3-3,é«˜é˜¶APIç¤ºèŒƒ](./3-3,é«˜é˜¶APIç¤ºèŒƒ.md)  | â­ï¸â­ï¸â­ï¸  |   1hour    |âœ…  |\n",
    "|&nbsp; |[**å››ã€TensorFlowçš„ä½é˜¶API**](./å››ã€TensorFlowçš„ä½é˜¶API.md) |â­ï¸    | 0hour|âœ…  |\n",
    "|day11|  [4-1,å¼ é‡çš„ç»“æ„æ“ä½œ](./4-1,å¼ é‡çš„ç»“æ„æ“ä½œ.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…   |\n",
    "|day12|  [4-2,å¼ é‡çš„æ•°å­¦è¿ç®—](./4-2,å¼ é‡çš„æ•°å­¦è¿ç®—.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day13|  [4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ](./4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ.md)| â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day14|  [4-4,AutoGraphçš„æœºåˆ¶åŸç†](./4-4,AutoGraphçš„æœºåˆ¶åŸç†.md)    | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |\n",
    "|day15|  [4-5,AutoGraphå’Œtf.Module](./4-5,AutoGraphå’Œtf.Module.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|&nbsp; |[**äº”ã€TensorFlowçš„ä¸­é˜¶API**](./äº”ã€TensorFlowçš„ä¸­é˜¶API.md) |  â­ï¸  | 0hour|âœ… |\n",
    "|day16|  [5-1,æ•°æ®ç®¡é“Dataset](./5-1,æ•°æ®ç®¡é“Dataset.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |\n",
    "|day17|  [5-2,ç‰¹å¾åˆ—feature_column](./5-2,ç‰¹å¾åˆ—feature_column.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day18|  [5-3,æ¿€æ´»å‡½æ•°activation](./5-3,æ¿€æ´»å‡½æ•°activation.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day19|  [5-4,æ¨¡å‹å±‚layers](./5-4,æ¨¡å‹å±‚layers.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day20|  [5-5,æŸå¤±å‡½æ•°losses](./5-5,æŸå¤±å‡½æ•°losses.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |\n",
    "|day21|  [5-6,è¯„ä¼°æŒ‡æ ‡metrics](./5-6,è¯„ä¼°æŒ‡æ ‡metrics.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|day22|  [5-7,ä¼˜åŒ–å™¨optimizers](./5-7,ä¼˜åŒ–å™¨optimizers.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day23|  [5-8,å›è°ƒå‡½æ•°callbacks](./5-8,å›è°ƒå‡½æ•°callbacks.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|&nbsp; |[**å…­ã€TensorFlowçš„é«˜é˜¶API**](./å…­ã€TensorFlowçš„é«˜é˜¶API.md)|    â­ï¸ | 0hour|âœ…  |\n",
    "|day24|  [6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•](./6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•.md)   | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ… |\n",
    "|day25|  [6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•](./6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |\n",
    "|day26|  [6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹](./6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…   |\n",
    "|day27|  [6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹](./6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day28|  [6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹](./6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…  |\n",
    "|day29| [6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹](./6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸| 1hour |âœ…   |\n",
    "|day30| [6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹](./6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸|2hour|âœ…  |\n",
    "|&nbsp;| [åè®°ï¼šä¸€ä¸ªåƒè´§å’Œä¸€é“èœçš„æ•…äº‹](./åè®°ï¼šä¸€ä¸ªåƒè´§å’Œä¸€é“èœçš„æ•…äº‹.md) | â­ï¸|0hour|âœ…  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ï¼Œå­¦ä¹ ç¯å¢ƒ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ä¹¦å…¨éƒ¨æºç åœ¨jupyterä¸­ç¼–å†™æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®é€šè¿‡gitå…‹éš†åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨jupyterä¸­äº¤äº’å¼è¿è¡Œå­¦ä¹ ã€‚\n",
    "\n",
    "ä¸ºäº†ç›´æ¥èƒ½å¤Ÿåœ¨jupyterä¸­æ‰“å¼€markdownæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…jupytextï¼Œå°†markdownè½¬æ¢æˆipynbæ–‡ä»¶ã€‚\n",
    "\n",
    "**æ­¤å¤–ï¼Œæœ¬é¡¹ç›®ä¹Ÿä¸å’Œé²¸ç¤¾åŒºè¾¾æˆäº†åˆä½œï¼Œå¯ä»¥åœ¨å’Œé²¸ä¸“æ forkæœ¬é¡¹ç›®ï¼Œå¹¶ç›´æ¥åœ¨äº‘ç¬”è®°æœ¬ä¸Šè¿è¡Œä»£ç ï¼Œé¿å…ç¯å¢ƒé…ç½®ç—›è‹¦ã€‚** \n",
    "\n",
    "ğŸ³å’Œé²¸ä¸“æ åœ°å€ï¼šhttps://www.kesci.com/home/column/5d8ef3c3037db3002d3aa3a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…‹éš†æœ¬ä¹¦æºç åˆ°æœ¬åœ°,ä½¿ç”¨ç äº‘é•œåƒä»“åº“å›½å†…ä¸‹è½½é€Ÿåº¦æ›´å¿«\n",
    "#!git clone https://gitee.com/Python_Ai_Road/eat_tensorflow2_in_30_days\n",
    "\n",
    "#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…jupytextï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†æœ¬ä¹¦å„ç« èŠ‚markdownæ–‡ä»¶è§†ä½œipynbæ–‡ä»¶è¿è¡Œ\n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext\n",
    "    \n",
    "#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…æœ€æ–°ç‰ˆæœ¬tensorflow æµ‹è¯•æœ¬ä¹¦ä¸­çš„ä»£ç \n",
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#æ³¨ï¼šæœ¬ä¹¦å…¨éƒ¨ä»£ç åœ¨tensorflow 2.1ç‰ˆæœ¬æµ‹è¯•é€šè¿‡\n",
    "tf.print(\"tensorflow version:\",tf.__version__)\n",
    "\n",
    "a = tf.constant(\"hello\")\n",
    "b = tf.constant(\"tensorflow2\")\n",
    "c = tf.strings.join([a,b],\" \")\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow version: 2.1.0\n",
    "hello tensorflow2\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…­ï¼Œé¼“åŠ±å’Œè”ç³»ä½œè€… ğŸˆğŸˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"Pythonä¸ç®—æ³•ä¹‹ç¾\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚\n",
    "\n",
    "![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
